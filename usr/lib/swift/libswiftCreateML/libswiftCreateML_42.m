uint64_t specialized MLTrainingSession.train(job:)(uint64_t a1, uint64_t a2, char a3)
{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 128);
  v13 = *(v6 + 120);
  v12 = *(v6 + 112);
  v11 = *(v6 + 72);
  v9 = *(v6 + 88);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 128);
  v13 = *(v6 + 120);
  v12 = *(v6 + 112);
  v11 = *(v6 + 72);
  v9 = *(v6 + 88);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 128);
  v13 = *(v6 + 120);
  v12 = *(v6 + 112);
  v11 = *(v6 + 72);
  v9 = *(v6 + 88);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 128);
  v13 = *(v6 + 120);
  v12 = *(v6 + 112);
  v11 = *(v6 + 72);
  v9 = *(v6 + 88);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 128);
  v13 = *(v6 + 120);
  v12 = *(v6 + 112);
  v11 = *(v6 + 72);
  v9 = *(v6 + 88);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 128);
  v13 = *(v6 + 120);
  v12 = *(v6 + 112);
  v11 = *(v6 + 72);
  v9 = *(v6 + 88);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

{
  v5 = *(*v4 + 224);
  v6 = *v4;
  *(v6 + 232) = a1;
  *(v6 + 240) = a2;
  *(v6 + 258) = a3 & 1;
  *(v6 + 248) = v3;
  v5;
  if (!v3)
  {
    return swift_task_switch(specialized MLTrainingSession.train(job:), 0, 0);
  }

  v7 = *(v6 + 160);
  v8 = *(v6 + 152);
  v14 = *(v6 + 144);
  v13 = *(v6 + 120);
  v12 = *(v6 + 104);
  v11 = *(v6 + 88);
  v9 = *(v6 + 96);
  *(v6 + 168);
  v7;
  v8;
  v14;
  v13;
  v12;
  v9;
  v11;
  return (*(v6 + 8))();
}

uint64_t specialized MLTrainingSession.evaluate(job:)(uint64_t a1)
{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  *(v2 + 48) = v1;
  *(v2 + 40) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

uint64_t specialized MLTrainingSession.evaluate(job:)()
{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLActivityClassifier>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLActivityClassifier>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLHandPoseClassifier>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLHandPoseClassifier>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLRandomForestRegressor>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLRandomForestRegressor>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLStyleTransfer>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLStyleTransfer>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLLogisticRegressionClassifier>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLLogisticRegressionClassifier>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLDecisionTreeRegressor>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLDecisionTreeRegressor>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLActionClassifier>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLActionClassifier>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLHandActionClassifier>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLHandActionClassifier>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLRandomForestClassifier>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLRandomForestClassifier>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLBoostedTreeRegressor>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLBoostedTreeRegressor>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLObjectDetector>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLObjectDetector>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLDecisionTreeClassifier>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLDecisionTreeClassifier>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLSoundClassifier.DataSource>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLSoundClassifier.DataSource>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLSoundClassifier>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLSoundClassifier>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLBoostedTreeClassifier>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLBoostedTreeClassifier>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLLinearRegressor>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLLinearRegressor>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

{
  v28 = v0 | 0x1000000000000000;
  v27 = v1;
  v23 = *(v1 + 40);
  v2 = *(v1 + 48);
  v3 = direct field offset for MLTrainingSession.delegate;
  *(v1 + 56) = direct field offset for MLTrainingSession.delegate;
  v4 = *(v2 + v3 + 24);
  v24 = *(v2 + v3 + 32);
  __swift_project_boxed_opaque_existential_0Tm((v2 + v3), v4);
  v5 = *(*v2 + 112);
  *(v1 + 64) = v5;
  v6 = v5 + v2;
  swift_beginAccess(v6, v1 + 16, 1, 0);
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLTrainingSession<MLImageClassifier>.Metadata);
  *(v1 + 72) = v7;
  v26 = *(*(v7 + 28) + v6);
  v8 = (*(v24 + 32))(&v26, v4);
  LOBYTE(v4) = v9;
  *(v1 + 80) = v8;
  *(v1 + 120) = v9;
  v25 = *(*(v7 + 32) + v6);
  LODWORD(v6) = *(*(v7 + 28) + v6);
  v10 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
  *(v1 + 88) = v10;
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v10);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v25, v6, v8, v4 & 1, v11, specialized MLJob.currentPhase.setter);
  v11;
  if ([*(v23 + direct field offset for MLJob.progress) isCancelled])
  {
    return (*(v1 + 8))();
  }

  v13 = *(v1 + 72);
  v14 = *(v1 + 48);
  v15 = v14 + *(v1 + 64);
  v16 = (v14 + *(v1 + 56));
  v17 = v16[3];
  v18 = v16[4];
  __swift_project_boxed_opaque_existential_0Tm(v16, v17);
  v19 = *(*(v13 + 32) + v15);
  v20 = *(v18 + 64);
  v21 = (v20 + *v20);
  v22 = swift_task_alloc(v20[1]);
  *(v1 + 96) = v22;
  *v22 = v1;
  v22[1] = specialized MLTrainingSession.evaluate(job:);
  return v21(v19, v17, v18);
}

{
  v33 = v0 | 0x1000000000000000;
  v32 = v1;
  v2 = *(v1 + 72);
  v3 = *(v1 + 64) + *(v1 + 48);
  v4 = *(v2 + 32);
  v5 = *(v4 + v3);
  v6 = __OFADD__(*(v1 + 112), v5);
  v7 = *(v1 + 112) + v5;
  if (v6)
  {
    BUG();
  }

  v29 = *(v1 + 121);
  v8 = *(v1 + 88);
  v27 = *(v1 + 40);
  v28 = *(v1 + 80);
  v9 = *(v1 + 120) & 1;
  *(v3 + v4) = v7;
  v10 = *(v3 + *(v2 + 28));
  v11 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v8);
  specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(v7, v10, v28, v9, v11, specialized MLJob.currentPhase.setter);
  v11;
  if (v29 == 1)
  {
    v30 = *(v1 + 104);
    v12 = (*(v1 + 56) + *(v1 + 48));
    specialized MLTrainingSession.transition(to:)(4, &demangling cache variable for type metadata for MLTrainingSession<MLImageClassifier>.Metadata);
    v13 = v12[3];
    v14 = v12[4];
    v31 = 4;
    __swift_project_boxed_opaque_existential_0Tm(v12, v13);
    (*(v14 + 40))(&v31, v13, v14);
    if (v30)
    {
      v15 = *(v1 + 8);
      return v15();
    }

LABEL_6:
    v15 = *(v1 + 8);
    return v15();
  }

  if ([*(*(v1 + 40) + direct field offset for MLJob.progress) isCancelled])
  {
    goto LABEL_6;
  }

  v17 = *(v1 + 72);
  v18 = *(v1 + 48);
  v19 = v18 + *(v1 + 64);
  v20 = (v18 + *(v1 + 56));
  v21 = v20[3];
  v22 = v20[4];
  __swift_project_boxed_opaque_existential_0Tm(v20, v21);
  v23 = *(*(v17 + 32) + v19);
  v24 = *(v22 + 64);
  v25 = (v24 + *v24);
  v26 = swift_task_alloc(v24[1]);
  *(v1 + 96) = v26;
  *v26 = v1;
  v26[1] = specialized MLTrainingSession.evaluate(job:);
  return v25(v23, v21, v22);
}

uint64_t specialized MLTrainingSession.evaluate(job:)(uint64_t a1, char a2)
{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

{
  v5 = *(*v3 + 96);
  v6 = *v3;
  *(v6 + 121) = a2 & 1;
  *(v6 + 104) = v2;
  v5;
  if (v2)
  {
    return (*(v6 + 8))();
  }

  *(v6 + 112) = a1;
  return swift_task_switch(specialized MLTrainingSession.evaluate(job:), 0, 0);
}

id specialized MLJob.reportProgress(completedUnitCount:phase:phaseUnitCount:metrics:)(uint64_t a1, unsigned int a2, uint64_t a3, int a4, uint64_t a5, uint64_t a6)
{
  v58 = *&a6;
  v7 = v6;
  LODWORD(v61) = a4;
  v60 = a3;
  v56 = a1;
  v53 = type metadata accessor for Date(0);
  v54 = *(v53 - 8);
  v9 = *(v54 + 64);
  v10 = alloca(v9);
  v11 = alloca(v9);
  v55 = v51;
  v12 = *(v6 + 32);
  CurrentValueSubject.value.getter();
  v13 = v7;
  LODWORD(v59) = a2;
  v14 = a2;
  v15 = specialized == infix<A>(_:_:)(a2, LOBYTE(v51[0]));
  v57 = v7;
  if ((v15 & 1) == 0 || !*(v7 + direct field offset for MLJob.phaseProgress))
  {
    v16 = v59;
    v17 = 1;
    switch(v59)
    {
      case 0:
        break;
      case 1:
      case 3:
      case 4:
        goto LABEL_5;
      case 2:
        v17 = 7;
LABEL_5:
        v18 = v60;
        if (v61)
        {
          v18 = -1;
        }

        v19 = *(v57 + direct field offset for MLJob.progress);
        v20 = objc_opt_self(NSProgress);
        v21 = v18;
        v13 = v57;
        v22 = [v20 progressWithTotalUnitCount:v21 parent:v19 pendingUnitCount:v17];
        v23 = v22;
        v24 = *(v13 + direct field offset for MLJob.phaseProgress);
        *(v13 + direct field offset for MLJob.phaseProgress) = v23;

        break;
    }

    v14 = v16;
    (*&v58)(v16);
  }

  v25 = *(v13 + direct field offset for MLJob.progress);
  v26 = v55;
  Date.init()(v14);
  v58 = Date.timeIntervalSince(_:)(v13 + direct field offset for MLJob.startDate);
  (*(v54 + 8))(v26, v53);
  v27 = v58;
  v28.super.super.isa = Double._bridgeToObjectiveC()().super.super.isa;
  if (one-time initialization token for elapsedTimeKey != -1)
  {
    swift_once(&one-time initialization token for elapsedTimeKey, one-time initialization function for elapsedTimeKey);
  }

  [v25 setUserInfoObject:v28.super.super.isa forKey:{static MLProgress.elapsedTimeKey, v27}];

  LOBYTE(v51[0]) = v59;
  v29 = _bridgeAnythingNonVerbatimToObjectiveC<A>(_:)(v51, &type metadata for MLPhase);
  if (one-time initialization token for phaseKey != -1)
  {
    swift_once(&one-time initialization token for phaseKey, one-time initialization function for phaseKey);
  }

  [v25 setUserInfoObject:v29 forKey:static MLProgress.phaseKey];
  swift_unknownObjectRelease(v29);
  v30.super.super.isa = Int._bridgeToObjectiveC()().super.super.isa;
  if (one-time initialization token for itemCountKey != -1)
  {
    swift_once(&one-time initialization token for itemCountKey, one-time initialization function for itemCountKey);
  }

  [v25 setUserInfoObject:v30.super.super.isa forKey:static MLProgress.itemCountKey];

  if ((v61 & 1) == 0)
  {
    v31.super.super.isa = Int._bridgeToObjectiveC()().super.super.isa;
    if (one-time initialization token for totalItemCountKey != -1)
    {
      swift_once(&one-time initialization token for totalItemCountKey, one-time initialization function for totalItemCountKey);
    }

    [v25 setUserInfoObject:v31.super.super.isa forKey:static MLProgress.totalItemCountKey];
  }

  v59 = v25;
  v32 = 1 << *(a5 + 32);
  v33 = ~(-1 << v32);
  if (v32 >= 64)
  {
    v33 = -1;
  }

  v34 = *(a5 + 64) & v33;
  v61 = (v32 + 63) >> 6;
  a5;
  v35 = 0;
  while (1)
  {
    v36 = v35;
    if (v34)
    {
LABEL_23:
      v35 = v36;
      goto LABEL_38;
    }

    v37 = v35 + 1;
    if (__OFADD__(1, v35))
    {
      BUG();
    }

    if (v37 >= v61)
    {
      goto LABEL_80;
    }

    v34 = *(a5 + 8 * v37 + 64);
    if (v34)
    {
      ++v35;
      goto LABEL_38;
    }

    v35 += 2;
    if (v36 + 2 >= v61)
    {
      goto LABEL_80;
    }

    v34 = *(a5 + 8 * v37 + 72);
    if (!v34)
    {
      v35 = v36 + 3;
      if (v36 + 3 >= v61)
      {
        goto LABEL_80;
      }

      v34 = *(a5 + 8 * v37 + 80);
      if (!v34)
      {
        v35 = v36 + 4;
        if (v36 + 4 >= v61)
        {
          goto LABEL_80;
        }

        v34 = *(a5 + 8 * v37 + 88);
        if (!v34)
        {
          v35 = v36 + 5;
          if (v36 + 5 >= v61)
          {
            goto LABEL_80;
          }

          v34 = *(a5 + 8 * v37 + 96);
          if (!v34)
          {
            v35 = v36 + 6;
            if (v36 + 6 >= v61)
            {
              goto LABEL_80;
            }

            v34 = *(a5 + 8 * v37 + 104);
            if (!v34)
            {
              break;
            }
          }
        }
      }
    }

LABEL_38:
    _BitScanForward64(&v38, v34);
    v39 = *(*(a5 + 48) + (v38 | (v35 << 6)));
    if (*(a5 + 16))
    {
      v40 = specialized __RawDictionaryStorage.find<A>(_:)(*(*(a5 + 48) + (v38 | (v35 << 6))));
      if (v41)
      {
        v42 = *(a5 + 56) + 32 * v40;
        v60 = v35;
        outlined init with copy of Any(v42, v51);
        v43 = v52;
        v44 = __swift_project_boxed_opaque_existential_0Tm(v51, v52);
        v45 = _bridgeAnythingToObjectiveC<A>(_:)(v44, v43);
        v35 = v60;
        __swift_destroy_boxed_opaque_existential_1Tm(v51);
        switch(v39)
        {
          case 0:
            goto LABEL_42;
          case 1:
            goto LABEL_60;
          case 2:
            goto LABEL_51;
          case 3:
            goto LABEL_54;
          case 4:
            goto LABEL_45;
          case 5:
            goto LABEL_63;
          case 6:
            goto LABEL_66;
          case 7:
            goto LABEL_57;
          case 8:
            goto LABEL_72;
          case 9:
            goto LABEL_48;
          case 10:
            goto LABEL_69;
        }
      }
    }

    v45 = 0;
    switch(v39)
    {
      case 0:
LABEL_42:
        if (one-time initialization token for lossKey != -1)
        {
          swift_once(&one-time initialization token for lossKey, one-time initialization function for lossKey);
        }

        v46 = &static MLProgress.lossKey;
        break;
      case 1:
LABEL_60:
        if (one-time initialization token for contentLossKey != -1)
        {
          swift_once(&one-time initialization token for contentLossKey, one-time initialization function for contentLossKey);
        }

        v46 = &static MLProgress.contentLossKey;
        break;
      case 2:
LABEL_51:
        if (one-time initialization token for styleLossKey != -1)
        {
          swift_once(&one-time initialization token for styleLossKey, one-time initialization function for styleLossKey);
        }

        v46 = &static MLProgress.styleLossKey;
        break;
      case 3:
LABEL_54:
        if (one-time initialization token for accuracyKey != -1)
        {
          swift_once(&one-time initialization token for accuracyKey, one-time initialization function for accuracyKey);
        }

        v46 = &static MLProgress.accuracyKey;
        break;
      case 4:
LABEL_45:
        if (one-time initialization token for validationLossKey != -1)
        {
          swift_once(&one-time initialization token for validationLossKey, one-time initialization function for validationLossKey);
        }

        v46 = &static MLProgress.validationLossKey;
        break;
      case 5:
LABEL_63:
        if (one-time initialization token for validationAccuracyKey != -1)
        {
          swift_once(&one-time initialization token for validationAccuracyKey, one-time initialization function for validationAccuracyKey);
        }

        v46 = &static MLProgress.validationAccuracyKey;
        break;
      case 6:
LABEL_66:
        if (one-time initialization token for stylizedImageKey != -1)
        {
          swift_once(&one-time initialization token for stylizedImageKey, one-time initialization function for stylizedImageKey);
        }

        v46 = &static MLProgress.stylizedImageKey;
        break;
      case 7:
LABEL_57:
        if (one-time initialization token for rootMeanSquaredErrorKey != -1)
        {
          swift_once(&one-time initialization token for rootMeanSquaredErrorKey, one-time initialization function for rootMeanSquaredErrorKey);
        }

        v46 = &static MLProgress.rootMeanSquaredErrorKey;
        break;
      case 8:
LABEL_72:
        if (one-time initialization token for maximumErrorKey != -1)
        {
          swift_once(&one-time initialization token for maximumErrorKey, one-time initialization function for maximumErrorKey);
        }

        v46 = &static MLProgress.maximumErrorKey;
        break;
      case 9:
LABEL_48:
        if (one-time initialization token for validationRootMeanSquaredErrorKey != -1)
        {
          swift_once(&one-time initialization token for validationRootMeanSquaredErrorKey, one-time initialization function for validationRootMeanSquaredErrorKey);
        }

        v46 = &static MLProgress.validationRootMeanSquaredErrorKey;
        break;
      case 10:
LABEL_69:
        if (one-time initialization token for validationMaximumErrorKey != -1)
        {
          swift_once(&one-time initialization token for validationMaximumErrorKey, one-time initialization function for validationMaximumErrorKey);
        }

        v46 = &static MLProgress.validationMaximumErrorKey;
        break;
    }

    v34 &= v34 - 1;
    v47 = *v46;
    [v59 setUserInfoObject:v45 forKey:v47];
    swift_unknownObjectRelease(v45);
  }

  v48 = v36 + 7;
  while (v48 < v61)
  {
    v34 = *(a5 + 8 * v48++ + 64);
    if (v34)
    {
      v36 = v48 - 1;
      goto LABEL_23;
    }
  }

LABEL_80:

  result = direct field offset for MLJob.phaseProgress;
  v50 = *(v57 + direct field offset for MLJob.phaseProgress);
  if (v50)
  {
    return [v50 setCompletedUnitCount:v56];
  }

  return result;
}

{
  v56 = *&a6;
  v7 = v6;
  v52 = a5;
  LODWORD(v58) = a4;
  v54 = a3;
  v53 = a1;
  v50 = type metadata accessor for Date(0);
  v51 = *(v50 - 8);
  v8 = *(v51 + 64);
  v9 = alloca(v8);
  v10 = alloca(v8);
  v11 = *(v6 + 32);
  CurrentValueSubject.value.getter();
  v12 = v7;
  LODWORD(v57) = a2;
  v13 = a2;
  v55 = v7;
  if ((specialized == infix<A>(_:_:)(a2, LOBYTE(v48[0])) & 1) == 0 || !*(v7 + direct field offset for MLJob.phaseProgress))
  {
    v14 = v57;
    v15 = 1;
    switch(v57)
    {
      case 0:
        break;
      case 1:
      case 3:
      case 4:
        goto LABEL_5;
      case 2:
        v15 = 7;
LABEL_5:
        v16 = v54;
        if (v58)
        {
          v16 = -1;
        }

        v17 = *(v55 + direct field offset for MLJob.progress);
        v18 = objc_opt_self(NSProgress);
        v19 = v16;
        v12 = v55;
        v20 = [v18 progressWithTotalUnitCount:v19 parent:v17 pendingUnitCount:v15];
        v21 = v20;
        v22 = *(v55 + direct field offset for MLJob.phaseProgress);
        *(v55 + direct field offset for MLJob.phaseProgress) = v21;

        break;
    }

    v13 = v14;
    (*&v56)(v14);
  }

  v23 = *(v12 + direct field offset for MLJob.progress);
  Date.init()(v13);
  v56 = Date.timeIntervalSince(_:)(v12 + direct field offset for MLJob.startDate);
  (*(v51 + 8))(v48, v50);
  v24 = v56;
  v25.super.super.isa = Double._bridgeToObjectiveC()().super.super.isa;
  if (one-time initialization token for elapsedTimeKey != -1)
  {
    swift_once(&one-time initialization token for elapsedTimeKey, one-time initialization function for elapsedTimeKey);
  }

  [v23 setUserInfoObject:v25.super.super.isa forKey:{static MLProgress.elapsedTimeKey, v24}];

  LOBYTE(v48[0]) = v57;
  v26 = _bridgeAnythingNonVerbatimToObjectiveC<A>(_:)(v48, &type metadata for MLPhase);
  if (one-time initialization token for phaseKey != -1)
  {
    swift_once(&one-time initialization token for phaseKey, one-time initialization function for phaseKey);
  }

  [v23 setUserInfoObject:v26 forKey:static MLProgress.phaseKey];
  swift_unknownObjectRelease(v26);
  v27.super.super.isa = Int._bridgeToObjectiveC()().super.super.isa;
  if (one-time initialization token for itemCountKey != -1)
  {
    swift_once(&one-time initialization token for itemCountKey, one-time initialization function for itemCountKey);
  }

  [v23 setUserInfoObject:v27.super.super.isa forKey:static MLProgress.itemCountKey];

  if ((v58 & 1) == 0)
  {
    v28.super.super.isa = Int._bridgeToObjectiveC()().super.super.isa;
    if (one-time initialization token for totalItemCountKey != -1)
    {
      swift_once(&one-time initialization token for totalItemCountKey, one-time initialization function for totalItemCountKey);
    }

    [v23 setUserInfoObject:v28.super.super.isa forKey:static MLProgress.totalItemCountKey];
  }

  v57 = v23;
  v29 = v52;
  v30 = 1 << *(v52 + 32);
  v31 = ~(-1 << v30);
  if (v30 >= 64)
  {
    v31 = -1;
  }

  v32 = *(v52 + 64) & v31;
  v58 = (v30 + 63) >> 6;
  v52;
  v33 = 0;
  while (1)
  {
    v34 = v33;
    if (v32)
    {
LABEL_23:
      v33 = v34;
      goto LABEL_40;
    }

    v35 = v33 + 1;
    if (__OFADD__(1, v33))
    {
      BUG();
    }

    if (v35 >= v58)
    {
      goto LABEL_82;
    }

    v32 = v29[v35 + 8];
    if (v32)
    {
      ++v33;
      goto LABEL_40;
    }

    v33 += 2;
    if (v34 + 2 >= v58)
    {
      goto LABEL_82;
    }

    v32 = v29[v35 + 9];
    if (!v32)
    {
      v33 = v34 + 3;
      if (v34 + 3 >= v58)
      {
        goto LABEL_82;
      }

      v32 = v29[v35 + 10];
      if (!v32)
      {
        v33 = v34 + 4;
        if (v34 + 4 >= v58)
        {
          goto LABEL_82;
        }

        v32 = v29[v35 + 11];
        if (!v32)
        {
          v33 = v34 + 5;
          if (v34 + 5 >= v58)
          {
            goto LABEL_82;
          }

          v32 = v29[v35 + 12];
          if (!v32)
          {
            v33 = v34 + 6;
            if (v34 + 6 >= v58)
            {
              goto LABEL_82;
            }

            v32 = v29[v35 + 13];
            if (!v32)
            {
              v33 = v34 + 7;
              if (v34 + 7 >= v58)
              {
                goto LABEL_82;
              }

              v32 = v29[v35 + 14];
              if (!v32)
              {
                break;
              }
            }
          }
        }
      }
    }

LABEL_40:
    _BitScanForward64(&v36, v32);
    v37 = *(v29[6] + (v36 | (v33 << 6)));
    if (v29[2])
    {
      v38 = specialized __RawDictionaryStorage.find<A>(_:)(*(v29[6] + (v36 | (v33 << 6))));
      if (v39)
      {
        outlined init with copy of Any(v29[7] + 32 * v38, v48);
        v40 = v49;
        v41 = __swift_project_boxed_opaque_existential_0Tm(v48, v49);
        v42 = _bridgeAnythingToObjectiveC<A>(_:)(v41, v40);
        __swift_destroy_boxed_opaque_existential_1Tm(v48);
        switch(v37)
        {
          case 0:
            goto LABEL_44;
          case 1:
            goto LABEL_62;
          case 2:
            goto LABEL_53;
          case 3:
            goto LABEL_56;
          case 4:
            goto LABEL_47;
          case 5:
            goto LABEL_65;
          case 6:
            goto LABEL_68;
          case 7:
            goto LABEL_59;
          case 8:
            goto LABEL_74;
          case 9:
            goto LABEL_50;
          case 10:
            goto LABEL_71;
        }
      }
    }

    v42 = 0;
    switch(v37)
    {
      case 0:
LABEL_44:
        if (one-time initialization token for lossKey != -1)
        {
          swift_once(&one-time initialization token for lossKey, one-time initialization function for lossKey);
        }

        v43 = &static MLProgress.lossKey;
        break;
      case 1:
LABEL_62:
        if (one-time initialization token for contentLossKey != -1)
        {
          swift_once(&one-time initialization token for contentLossKey, one-time initialization function for contentLossKey);
        }

        v43 = &static MLProgress.contentLossKey;
        break;
      case 2:
LABEL_53:
        if (one-time initialization token for styleLossKey != -1)
        {
          swift_once(&one-time initialization token for styleLossKey, one-time initialization function for styleLossKey);
        }

        v43 = &static MLProgress.styleLossKey;
        break;
      case 3:
LABEL_56:
        if (one-time initialization token for accuracyKey != -1)
        {
          swift_once(&one-time initialization token for accuracyKey, one-time initialization function for accuracyKey);
        }

        v43 = &static MLProgress.accuracyKey;
        break;
      case 4:
LABEL_47:
        if (one-time initialization token for validationLossKey != -1)
        {
          swift_once(&one-time initialization token for validationLossKey, one-time initialization function for validationLossKey);
        }

        v43 = &static MLProgress.validationLossKey;
        break;
      case 5:
LABEL_65:
        if (one-time initialization token for validationAccuracyKey != -1)
        {
          swift_once(&one-time initialization token for validationAccuracyKey, one-time initialization function for validationAccuracyKey);
        }

        v43 = &static MLProgress.validationAccuracyKey;
        break;
      case 6:
LABEL_68:
        if (one-time initialization token for stylizedImageKey != -1)
        {
          swift_once(&one-time initialization token for stylizedImageKey, one-time initialization function for stylizedImageKey);
        }

        v43 = &static MLProgress.stylizedImageKey;
        break;
      case 7:
LABEL_59:
        if (one-time initialization token for rootMeanSquaredErrorKey != -1)
        {
          swift_once(&one-time initialization token for rootMeanSquaredErrorKey, one-time initialization function for rootMeanSquaredErrorKey);
        }

        v43 = &static MLProgress.rootMeanSquaredErrorKey;
        break;
      case 8:
LABEL_74:
        if (one-time initialization token for maximumErrorKey != -1)
        {
          swift_once(&one-time initialization token for maximumErrorKey, one-time initialization function for maximumErrorKey);
        }

        v43 = &static MLProgress.maximumErrorKey;
        break;
      case 9:
LABEL_50:
        if (one-time initialization token for validationRootMeanSquaredErrorKey != -1)
        {
          swift_once(&one-time initialization token for validationRootMeanSquaredErrorKey, one-time initialization function for validationRootMeanSquaredErrorKey);
        }

        v43 = &static MLProgress.validationRootMeanSquaredErrorKey;
        break;
      case 10:
LABEL_71:
        if (one-time initialization token for validationMaximumErrorKey != -1)
        {
          swift_once(&one-time initialization token for validationMaximumErrorKey, one-time initialization function for validationMaximumErrorKey);
        }

        v43 = &static MLProgress.validationMaximumErrorKey;
        break;
    }

    v32 &= v32 - 1;
    v44 = *v43;
    [v57 setUserInfoObject:v42 forKey:v44];
    swift_unknownObjectRelease(v42);

    v29 = v52;
  }

  v45 = v34 + 8;
  while (v45 < v58)
  {
    v32 = v29[v45++ + 8];
    if (v32)
    {
      v34 = v45 - 1;
      goto LABEL_23;
    }
  }

LABEL_82:

  result = direct field offset for MLJob.phaseProgress;
  v47 = *(v55 + direct field offset for MLJob.phaseProgress);
  if (v47)
  {
    return [v47 setCompletedUnitCount:v53];
  }

  return result;
}

NSURL *specialized MLTrainingSession.saveFeatureExtractionCheckpoint(to:)(uint64_t a1, uint64_t *a2, void (*a3)(void))
{
  v70 = a3;
  v84 = v4;
  v81 = a1;
  v75 = v3;
  v6 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLCheckpoint?) - 8) + 64);
  v7 = alloca(v6);
  v8 = alloca(v6);
  v69 = &v67;
  v73 = type metadata accessor for URL(0);
  v74 = *(v73 - 8);
  v9 = *(v74 + 64);
  v10 = alloca(v9);
  v11 = alloca(v9);
  v68 = &v67;
  v78 = type metadata accessor for MLCheckpoint(0);
  v79 = *(v78 - 8);
  v12 = *(v79 + 64);
  v13 = alloca(v12);
  v14 = alloca(v12);
  v80 = &v67;
  v15 = alloca(v12);
  v16 = alloca(v12);
  v85 = &v67;
  v17 = *(direct field offset for MLTrainingSession.delegate + v5 + 24);
  v82 = *(direct field offset for MLTrainingSession.delegate + v5 + 32);
  v77 = __swift_project_boxed_opaque_existential_0Tm((direct field offset for MLTrainingSession.delegate + v5), v17);
  v18 = *(*v5 + 112);
  v76 = v5;
  v19 = v5 + v18;
  swift_beginAccess(v19, v87, 1, 0);
  v20 = __swift_instantiateConcreteTypeFromMangledName(a2);
  LOBYTE(v86[0]) = *(*(v20 + 28) + v19);
  v21 = v84;
  v22 = (*(v82 + 72))(v81, v86, *(*(v20 + 32) + v19), v17);
  if (!v21)
  {
    v83 = v20;
    v84 = v19;
    if (v22)
    {
      v82 = 0;
      v71 = *(v74 + 16);
      v23 = v68;
      v24 = v73;
      v71(v68, v81);
      v25 = v84;
      LOBYTE(v81) = *(v84 + v83[7]);
      v77 = *(v84 + v83[8]);
      v26 = lazy protocol witness table accessor for type MLProgress.Metric and conformance MLProgress.Metric();
      v72 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, &type metadata for MLProgress.Metric, &type metadata for Any + 8, v26);
      v27 = v85;
      v28 = v85;
      (v71)(v85, v23, v24);
      v29 = v78;
      *(v27 + *(v78 + 20)) = v81;
      *(v27 + v29[6]) = v77;
      v30 = v29[7];
      Date.init()(v28);
      (*(v74 + 8))(v23, v73);
      *(v27 + v29[8]) = v72;
      v31 = v83;
      v32 = v69;
      specialized BidirectionalCollection.last.getter(*(v25 + v83[11]));
      if (__swift_getEnumTagSinglePayload(v32, 1, v29) == 1)
      {
        outlined destroy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v32, &demangling cache variable for type metadata for MLCheckpoint?);
LABEL_14:
        v43 = v31[11];
        v44 = v25;
        specialized Array._makeUniqueAndReserveCapacityIfNotUnique()();
        v45 = v43;
        v46 = *(*(v25 + v43) + 16);
        specialized Array._reserveCapacityAssumingUniqueBuffer(oldCount:)(v46);
        v47 = v45;
        v48 = *(v44 + v45);
        *(v48 + 16) = v46 + 1;
        outlined init with copy of MLTrainingSessionParameters(v85, v48 + ((*(v79 + 80) + 32) & ~*(v79 + 80)) + *(v79 + 72) * v46, type metadata accessor for MLCheckpoint);
        *(v44 + v47) = v48;
      }

      else
      {
        v36 = v32;
        v37 = 0xEB0000000064657ALL;
        v38 = v80;
        outlined init with take of MLClassifierMetrics(v36, v80, type metadata accessor for MLCheckpoint);
        v39 = v29[5];
        v40 = v38;
        switch(*(v38 + v39))
        {
          case 0:
            v41 = 0x696C616974696E69;
            goto LABEL_12;
          case 1:
            0xEA0000000000676ELL;
            goto LABEL_16;
          case 2:
            v41 = 0x676E696E69617274;
            v37 = 0xE800000000000000;
            goto LABEL_12;
          case 3:
            v41 = 0x697461756C617665;
            v37 = 0xEA0000000000676ELL;
            goto LABEL_12;
          case 4:
            v37 = 0xEB00000000676E69;
            v41 = 0x636E657265666E69;
LABEL_12:
            v42 = _stringCompareWithSmolCheck(_:_:expecting:)(v41, v37, 0x6974636172747865, 0xEA0000000000676ELL, 0);
            v37;
            if ((v42 & 1) == 0)
            {
              outlined destroy of MLActivityClassifier.ModelParameters(v40, type metadata accessor for MLCheckpoint);
              v31 = v83;
              goto LABEL_14;
            }

LABEL_16:
            v49 = objc_opt_self(NSFileManager);
            v50 = [v49 defaultManager];
            v51 = v50;
            URL._bridgeToObjectiveC()(v51);
            v53 = v52;
            v86[0] = 0;
            v54 = [(NSURL *)v51 removeItemAtURL:v52 error:v86];

            v55 = v86[0];
            if (v54)
            {
              v86[0];
            }

            else
            {
              v56 = v86[0];
              v57 = _convertNSErrorToError(_:)(v55);

              swift_willThrow(v56, "removeItemAtURL:error:");
              v82 = 0;
              v57;
            }

            v58 = v83;
            v59 = v84;
            v60 = *(v84 + v83[11]);
            v61 = v60[2];
            swift_beginAccess(v84, v86, 33, 0);
            v62 = v58[11];
            isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native(v60);
            *(v59 + v62) = v60;
            if (!isUniquelyReferenced_nonNull_native)
            {
              v60 = specialized _ArrayBuffer._consumeAndCreateNew()(v60);
              *(v84 + v62) = v60;
            }

            v64 = v80;
            if (!v61)
            {
              BUG();
            }

            if (v61 > v60[2])
            {
              BUG();
            }

            outlined assign with copy of MLCheckpoint(v85, v60 + ((*(v79 + 80) + 32) & ~*(v79 + 80)) + *(v79 + 72) * (v61 - 1));
            *(v84 + v62) = v60;
            swift_endAccess(v86);
            outlined destroy of MLActivityClassifier.ModelParameters(v64, type metadata accessor for MLCheckpoint);
            break;
        }
      }

      v65 = v82;
      v70();
      v35 = v78;
      if (v65)
      {
        outlined destroy of MLActivityClassifier.ModelParameters(v85, type metadata accessor for MLCheckpoint);
        return __stack_chk_guard;
      }

      v34 = v75;
      outlined init with take of MLClassifierMetrics(v85, v75, type metadata accessor for MLCheckpoint);
      v33 = 0;
    }

    else
    {
      v33 = 1;
      v34 = v75;
      v35 = v78;
    }

    __swift_storeEnumTagSinglePayload(v34, v33, 1, v35);
  }

  return __stack_chk_guard;
}

uint64_t MLJob.startDate.getter()
{
  v2 = v0;
  v3 = direct field offset for MLJob.startDate + v1;
  v4 = type metadata accessor for Date(0);
  return (*(*(v4 - 8) + 16))(v2, v3, v4);
}

uint64_t MLJob.checkpoints.getter()
{
  v4 = *(v0 + 16);
  v1 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  v2 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type PassthroughSubject<MLCheckpoint, Never> and conformance PassthroughSubject<A, B>, &demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>, &protocol conformance descriptor for PassthroughSubject<A, B>);
  return Publisher.eraseToAnyPublisher()(v1, v2);
}

uint64_t MLJob.result.getter()
{
  v15[1] = v0;
  v1 = *(*v0 + 80);
  v2 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for Error);
  v3 = type metadata accessor for PassthroughSubject(255, v1, v2, &protocol self-conformance witness table for Error);
  WitnessTable = swift_getWitnessTable(&protocol conformance descriptor for PassthroughSubject<A, B>, v3);
  Connectable = type metadata accessor for Publishers.MakeConnectable(0, v3, WitnessTable);
  v16 = *(Connectable - 8);
  v6 = *(v16 + 64);
  v7 = alloca(v6);
  v8 = alloca(v6);
  v15[0] = v0[3];

  Publishers.MakeConnectable.init(upstream:)(v15, v3, WitnessTable);
  v9 = swift_getWitnessTable(&protocol conformance descriptor for Publishers.MakeConnectable<A>, Connectable);
  v10 = ConnectablePublisher.autoconnect()(Connectable, v9);
  (*(v16 + 8))(v15, Connectable);
  v15[0] = v10;
  v11 = type metadata accessor for Publishers.Autoconnect(0, Connectable, v9);
  v12 = swift_getWitnessTable(&protocol conformance descriptor for Publishers.Autoconnect<A>, v11);
  v13 = Publisher.eraseToAnyPublisher()(v11, v12);

  return v13;
}

uint64_t MLJob.phase.getter()
{
  v4 = *(v0 + 32);
  v1 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  v2 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type CurrentValueSubject<MLPhase, Never> and conformance CurrentValueSubject<A, B>, &demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>, &protocol conformance descriptor for CurrentValueSubject<A, B>);
  return Publisher.eraseToAnyPublisher()(v1, v2);
}

uint64_t specialized MLJob.currentPhase.setter(char a1)
{
  v5[0] = HIBYTE(v1);
  v3 = *(v2 + 32);
  v5[0] = a1;
  return CurrentValueSubject.value.setter(v5);
}

{
  return specialized MLJob.currentPhase.setter(a1);
}

void *specialized MLJob.init(_:)(void *a1, uint64_t a2)
{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLActivityClassifier, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395E50, 40, 7);
  v10[2] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[3] = a1;
  v10[4] = a2;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLActivityClassifier.resume(_:), v10, &unk_395E78, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLHandPoseClassifier, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395E00, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLHandPoseClassifier.resume(_:), v10, &unk_395E28, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLRandomForestRegressor, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395DB0, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLRandomForestRegressor.resume(_:), v10, &unk_395DD8, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLStyleTransfer, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395D60, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLStyleTransfer.resume(_:), v10, &unk_395D88, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLLogisticRegressionClassifier, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395D10, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLLogisticRegressionClassifier.resume(_:), v10, &unk_395D38, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLDecisionTreeRegressor, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395CC0, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLDecisionTreeRegressor.resume(_:), v10, &unk_395CE8, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLActionClassifier, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395C70, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLActionClassifier.resume(_:), v10, &unk_395C98, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLHandActionClassifier, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395C20, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLHandActionClassifier.resume(_:), v10, &unk_395C48, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLRandomForestClassifier, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395BD0, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLRandomForestClassifier.resume(_:), v10, &unk_395BF8, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLBoostedTreeRegressor, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395B80, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLBoostedTreeRegressor.resume(_:), v10, &unk_395BA8, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLObjectDetector, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395B30, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLObjectDetector.resume(_:), v10, &unk_395B58, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLDecisionTreeClassifier, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395AE0, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLDecisionTreeClassifier.resume(_:), v10, &unk_395B08, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLSoundClassifier, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395A90, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLSoundClassifier.resume(_:), v10, &unk_395AB8, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLBoostedTreeClassifier, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_3959F0, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLBoostedTreeClassifier.resume(_:), v10, &unk_395A18, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLLinearRegressor, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_3959A0, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLLinearRegressor.resume(_:), v10, &unk_3959C8, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

{
  v12[0] = HIBYTE(v2);
  Date.init()(a1);
  v4 = direct field offset for MLJob.progress;
  v5 = objc_opt_self(NSProgress);
  v6 = [v5 progressWithTotalUnitCount:10];
  *(a1 + v4) = v6;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v7 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v7, *(v7 + 48), *(v7 + 52));
  a1[2] = PassthroughSubject.init()(v7);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLImageClassifier, Error>);
  swift_allocObject(v8, *(v8 + 48), *(v8 + 52));
  a1[3] = PassthroughSubject.init()(v8);
  v12[0] = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v12);
  v10 = swift_allocObject(&unk_395950, 40, 7);
  v10[2] = a2;
  v10[3] = partial apply for specialized closure #1 in MLJob.init(_:);
  v10[4] = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLImageClassifier.resume(_:), v10, &unk_395978, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

void *specialized MLJob.init(_:)(void *a1, uint64_t a2, uint64_t a3)
{
  v15[0] = HIBYTE(v3);
  Date.init()(a1);
  v6 = direct field offset for MLJob.progress;
  v7 = objc_opt_self(NSProgress);
  v8 = [v7 progressWithTotalUnitCount:10];
  *(a1 + v6) = v8;
  *(a1 + direct field offset for MLJob.phaseProgress) = 0;
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLCheckpoint, Never>);
  swift_allocObject(v9, *(v9 + 48), *(v9 + 52));
  a1[2] = PassthroughSubject.init()(v9);
  v10 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for PassthroughSubject<MLSoundClassifier.DataSource, Error>);
  swift_allocObject(v10, *(v10 + 48), *(v10 + 52));
  a1[3] = PassthroughSubject.init()(v10);
  v15[0] = 0;
  v11 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for CurrentValueSubject<MLPhase, Never>);
  swift_allocObject(v11, *(v11 + 48), *(v11 + 52));
  a1[4] = CurrentValueSubject.init(_:)(v15);
  v12 = swift_allocObject(&unk_395A40, 80, 7);
  *(v12 + 16) = a2;
  v13 = *(a3 + 16);
  *(v12 + 24) = *a3;
  *(v12 + 40) = v13;
  *(v12 + 56) = *(a3 + 32);
  *(v12 + 64) = partial apply for specialized closure #1 in MLJob.init(_:);
  *(v12 + 72) = a1;
  swift_retain_n(a1, 2);

  specialized MLTrainingSession.resume(job:completion:)(a1, partial apply for closure #1 in closure #1 in static MLSoundClassifier.extractFeatures(trainingData:parameters:sessionParameters:), v12, &unk_395A68, &async function pointer to partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:));

  return a1;
}

uint64_t specialized closure #1 in MLJob.init(_:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(void), uint64_t *a4)
{
  v19 = a2;
  v18 = a3;
  v5 = *(*(a3(0) - 8) + 64);
  v6 = alloca(v5);
  v7 = alloca(v5);
  v8 = __swift_instantiateConcreteTypeFromMangledName(a4);
  v9 = *(*(v8 - 8) + 64);
  v10 = alloca(v9);
  v11 = alloca(v9);
  outlined init with copy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(a1, &v17, a4);
  if (swift_getEnumCaseMultiPayload(&v17, v8) == 1)
  {
    v12 = v17;
    v13 = *(v19 + 24);
    swift_errorRetain(v17);
    PassthroughSubject.send(completion:)(&v17);
    v12;
    return v12;
  }

  else
  {
    v15 = v18;
    outlined init with take of MLClassifierMetrics(&v17, &v17, v18);
    v16 = *(v19 + 24);
    PassthroughSubject.send(_:)(&v17);
    v17 = 0;
    PassthroughSubject.send(completion:)(&v17);
    return outlined destroy of MLActivityClassifier.ModelParameters(&v17, v15);
  }
}

void *MLJob.deinit()
{
  v0[2];
  v0[3];
  v0[4];
  v1 = v0 + direct field offset for MLJob.startDate;
  v2 = type metadata accessor for Date(0);
  (*(*(v2 - 8) + 8))(v1, v2);

  return v0;
}

uint64_t type metadata completion function for MLJob(uint64_t a1)
{
  v3[0] = &value witness table for Builtin.NativeObject + 64;
  v3[1] = &value witness table for Builtin.NativeObject + 64;
  v3[2] = &value witness table for Builtin.NativeObject + 64;
  result = type metadata accessor for Date(319);
  if (v2 <= 0x3F)
  {
    v3[3] = *(result - 8) + 64;
    v3[4] = &value witness table for Builtin.UnknownObject + 64;
    v3[5] = "\b";
    result = swift_initClassMetadata2(a1, 0, 6, v3, a1 + 88);
    if (!result)
    {
      return 0;
    }
  }

  return result;
}

uint64_t partial apply for specialized closure #1 in MLJob.init(_:)(uint64_t a1)
{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLImageClassifier, &demangling cache variable for type metadata for Result<MLImageClassifier, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLLinearRegressor, &demangling cache variable for type metadata for Result<MLLinearRegressor, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLBoostedTreeClassifier, &demangling cache variable for type metadata for Result<MLBoostedTreeClassifier, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLSoundClassifier.DataSource, &demangling cache variable for type metadata for Result<MLSoundClassifier.DataSource, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLSoundClassifier, &demangling cache variable for type metadata for Result<MLSoundClassifier, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLDecisionTreeClassifier, &demangling cache variable for type metadata for Result<MLDecisionTreeClassifier, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLObjectDetector, &demangling cache variable for type metadata for Result<MLObjectDetector, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLBoostedTreeRegressor, &demangling cache variable for type metadata for Result<MLBoostedTreeRegressor, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLRandomForestClassifier, &demangling cache variable for type metadata for Result<MLRandomForestClassifier, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLHandActionClassifier, &demangling cache variable for type metadata for Result<MLHandActionClassifier, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLActionClassifier, &demangling cache variable for type metadata for Result<MLActionClassifier, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLDecisionTreeRegressor, &demangling cache variable for type metadata for Result<MLDecisionTreeRegressor, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLLogisticRegressionClassifier, &demangling cache variable for type metadata for Result<MLLogisticRegressionClassifier, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLStyleTransfer, &demangling cache variable for type metadata for Result<MLStyleTransfer, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLRandomForestRegressor, &demangling cache variable for type metadata for Result<MLRandomForestRegressor, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLHandPoseClassifier, &demangling cache variable for type metadata for Result<MLHandPoseClassifier, Error>);
}

{
  return specialized closure #1 in MLJob.init(_:)(a1, v1, type metadata accessor for MLActivityClassifier, &demangling cache variable for type metadata for Result<MLActivityClassifier, Error>);
}

uint64_t partial apply for specialized closure #1 in MLTrainingSession.resume(job:completion:)(uint64_t a1)
{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

{
  v3 = v1[2];
  v4 = v1[3];
  v8 = v1[4];
  v9 = v1[5];
  v10 = v1[6];
  v5 = v1[7];
  v6 = swift_task_alloc(48);
  *(v2 + 16) = v6;
  *v6 = v2;
  v6[1] = partial apply for closure #1 in MLActivityClassifier.init(trainingData:featureColumns:labelColumn:recordingFileColumn:parameters:);
  return specialized closure #1 in MLTrainingSession.resume(job:completion:)(a1, v3, v4, v8, v9, v10, v5);
}

uint64_t outlined assign with copy of MLCheckpoint(uint64_t a1, uint64_t a2)
{
  v2 = type metadata accessor for MLCheckpoint(0);
  (*(*(v2 - 8) + 24))(a2, a1, v2);
  return a2;
}

uint64_t sub_3026F4()
{
  *(v0 + 16);
  *(v0 + 72);
  return swift_deallocObject(v0, 80, 7);
}

uint64_t objectdestroyTm_8()
{
  *(v0 + 16);
  *(v0 + 32);
  return swift_deallocObject(v0, 40, 7);
}

uint64_t sub_3031AD()
{
  *(v0 + 24);
  *(v0 + 32);
  return swift_deallocObject(v0, 40, 7);
}

uint64_t partial apply for closure #1 in closure #1 in static MLActivityClassifier.resume(_:)(uint64_t a1, char a2)
{
  v3 = v2[3];
  v4 = v2[4];
  return closure #1 in closure #1 in static MLActivityClassifier.resume(_:)(a1, a2 & 1, v2[2]);
}

uint64_t objectdestroy_2Tm()
{
  swift_unknownObjectRelease(v0[2]);
  v0[4];
  v0[5];
  v0[7];
  return swift_deallocObject(v0, 64, 7);
}

Swift::Void __swiftcall __spoils<cf,zf,sf,of,pf,rax,rdx,rcx,rdi,rsi,r8,r9,r10,r11,r12,xmm0,xmm1,xmm2,xmm3,xmm4,xmm5,xmm6,xmm7> MLImageClassifier.ModelParameters.ModelAlgorithmType.validate()()
{
  v11 = v0;
  v2 = *(*(type metadata accessor for MLImageClassifier.FeatureExtractorType(0) - 8) + 64);
  v3 = alloca(v2);
  v4 = alloca(v2);
  v5 = *(*(type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType(0) - 8) + 64);
  v6 = alloca(v5);
  v7 = alloca(v5);
  outlined init with copy of MLImageClassifier.ModelParameters.ModelAlgorithmType(v1, &v10);
  v8 = *(&v10 + *(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType)) + 48));
  outlined init with take of MLImageClassifier.FeatureExtractorType(&v10, &v10);
  MLImageClassifier.FeatureExtractorType.validate()();
  if (!v9)
  {
    v10 = v8;
    MLImageClassifier.ModelParameters.ClassifierType.validate()();
  }

  outlined destroy of MLImageClassifier.FeatureExtractorType(&v10);
  v8;
}

uint64_t MLImageClassifier.ModelParameters.ModelAlgorithmType.description.getter()
{
  v20._countAndFlagsBits = 0xD000000000000012;
  v1 = *(*(type metadata accessor for MLImageClassifier.FeatureExtractorType(0) - 8) + 64);
  v2 = alloca(v1);
  v3 = alloca(v1);
  v4 = *(*(type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType(0) - 8) + 64);
  v5 = alloca(v4);
  v6 = alloca(v4);
  outlined init with copy of MLImageClassifier.ModelParameters.ModelAlgorithmType(v0, &v18);
  v7 = *(&v18 + *(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType)) + 48));
  outlined init with take of MLImageClassifier.FeatureExtractorType(&v18, &v18);
  v8 = MLImageClassifier.FeatureExtractorType.description.getter();
  v10 = v9;
  v18 = v8;
  v19 = v9;
  v9;
  v11._object = 0xE300000000000000;
  v11._countAndFlagsBits = 2108704;
  String.append(_:)(v11);
  v10;
  v12 = v18;
  v13 = v19;
  if (v7)
  {
    v7;
    v20._countAndFlagsBits += 3;
    v14 = "Feature Extractor: ";
  }

  else
  {
    v14 = "Multilayer Perceptron";
  }

  v18 = v12;
  v19 = v13;
  v13;
  v15._countAndFlagsBits = v20._countAndFlagsBits;
  v15._object = (v14 | 0x8000000000000000);
  String.append(_:)(v15);
  v13;
  v14 | 0x8000000000000000;
  v16 = v18;
  outlined destroy of MLImageClassifier.FeatureExtractorType(&v18);
  return v16;
}

uint64_t type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType(uint64_t a1)
{
  result = type metadata singleton initialization cache for MLImageClassifier.ModelParameters.ModelAlgorithmType;
  if (!type metadata singleton initialization cache for MLImageClassifier.ModelParameters.ModelAlgorithmType)
  {
    return swift_getSingletonMetadata(a1, &nominal type descriptor for MLImageClassifier.ModelParameters.ModelAlgorithmType);
  }

  return result;
}

uint64_t outlined init with copy of MLImageClassifier.ModelParameters.ModelAlgorithmType(uint64_t a1, uint64_t a2)
{
  v2 = type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType(0);
  (*(*(v2 - 8) + 16))(a2, a1, v2);
  return a2;
}

void *initializeBufferWithCopyOfBuffer for MLImageClassifier.ModelParameters.ModelAlgorithmType(char *__dst, char *__src, uint64_t a3)
{
  v3 = __dst;
  v4 = *(*(a3 - 8) + 80);
  if ((v4 & 0x20000) != 0)
  {
    v9 = *__src;
    *v3 = *__src;
    v3 = (v9 + ((v4 + 16) & ~v4));
  }

  else
  {
    v5 = type metadata accessor for MLImageClassifier.FeatureExtractorType(0);
    if (swift_getEnumCaseMultiPayload(__src, v5) == 1)
    {
      v6 = type metadata accessor for URL(0);
      (*(*(v6 - 8) + 16))(__dst, __src, v6);
      v7 = *(type metadata accessor for MLImageClassifier.CustomFeatureExtractor(0) + 20);
      *&__dst[v7] = *&__src[v7];
      v8 = *&__src[v7 + 8];
      *(v3 + v7 + 8) = v8;
      v8;
      swift_storeEnumTagMultiPayload(v3, v5, 1);
    }

    else
    {
      memcpy(__dst, __src, *(*(v5 - 8) + 64));
    }

    v10 = *(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType)) + 48);
    v11 = *&__src[v10];
    *(v3 + v10) = v11;
    v11;
  }

  return v3;
}

uint64_t destroy for MLImageClassifier.ModelParameters.ModelAlgorithmType(uint64_t a1)
{
  v1 = type metadata accessor for MLImageClassifier.FeatureExtractorType(0);
  if (swift_getEnumCaseMultiPayload(a1, v1) == 1)
  {
    v2 = type metadata accessor for URL(0);
    (*(*(v2 - 8) + 8))(a1, v2);
    v3 = type metadata accessor for MLImageClassifier.CustomFeatureExtractor(0);
    *(a1 + *(v3 + 20) + 8);
  }

  v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType));
  return *(a1 + *(v4 + 48));
}

char *initializeWithCopy for MLImageClassifier.ModelParameters.ModelAlgorithmType(char *__dst, char *__src)
{
  v3 = type metadata accessor for MLImageClassifier.FeatureExtractorType(0);
  if (swift_getEnumCaseMultiPayload(__src, v3) == 1)
  {
    v4 = type metadata accessor for URL(0);
    (*(*(v4 - 8) + 16))(__dst, __src, v4);
    v5 = *(type metadata accessor for MLImageClassifier.CustomFeatureExtractor(0) + 20);
    *&__dst[v5] = *&__src[v5];
    v6 = *&__src[v5 + 8];
    *&__dst[v5 + 8] = v6;
    v6;
    swift_storeEnumTagMultiPayload(__dst, v3, 1);
  }

  else
  {
    memcpy(__dst, __src, *(*(v3 - 8) + 64));
  }

  v7 = *(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType)) + 48);
  v8 = *&__src[v7];
  *&__dst[v7] = v8;
  v8;
  return __dst;
}

char *assignWithCopy for MLImageClassifier.ModelParameters.ModelAlgorithmType(char *__dst, char *__src)
{
  if (__dst != __src)
  {
    outlined destroy of MLImageClassifier.FeatureExtractorType(__dst);
    v3 = type metadata accessor for MLImageClassifier.FeatureExtractorType(0);
    if (swift_getEnumCaseMultiPayload(__src, v3) == 1)
    {
      v4 = type metadata accessor for URL(0);
      (*(*(v4 - 8) + 16))(__dst, __src, v4);
      v5 = *(type metadata accessor for MLImageClassifier.CustomFeatureExtractor(0) + 20);
      *&__dst[v5] = *&__src[v5];
      v6 = *&__src[v5 + 8];
      *&__dst[v5 + 8] = v6;
      v6;
      swift_storeEnumTagMultiPayload(__dst, v3, 1);
    }

    else
    {
      memcpy(__dst, __src, *(*(v3 - 8) + 64));
    }
  }

  v7 = *(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType)) + 48);
  v8 = *&__src[v7];
  v9 = *&__dst[v7];
  *&__dst[v7] = v8;
  v8;
  v9;
  return __dst;
}

char *initializeWithTake for MLImageClassifier.ModelParameters.ModelAlgorithmType(char *__dst, char *__src)
{
  v2 = type metadata accessor for MLImageClassifier.FeatureExtractorType(0);
  if (swift_getEnumCaseMultiPayload(__src, v2) == 1)
  {
    v3 = type metadata accessor for URL(0);
    (*(*(v3 - 8) + 32))(__dst, __src, v3);
    v4 = type metadata accessor for MLImageClassifier.CustomFeatureExtractor(0);
    *&__dst[*(v4 + 20)] = *&__src[*(v4 + 20)];
    swift_storeEnumTagMultiPayload(__dst, v2, 1);
  }

  else
  {
    memcpy(__dst, __src, *(*(v2 - 8) + 64));
  }

  v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType));
  *&__dst[*(v5 + 48)] = *&__src[*(v5 + 48)];
  return __dst;
}

char *assignWithTake for MLImageClassifier.ModelParameters.ModelAlgorithmType(char *__dst, char *__src)
{
  if (__dst != __src)
  {
    outlined destroy of MLImageClassifier.FeatureExtractorType(__dst);
    v3 = type metadata accessor for MLImageClassifier.FeatureExtractorType(0);
    if (swift_getEnumCaseMultiPayload(__src, v3) == 1)
    {
      v4 = type metadata accessor for URL(0);
      (*(*(v4 - 8) + 32))(__dst, __src, v4);
      v5 = type metadata accessor for MLImageClassifier.CustomFeatureExtractor(0);
      *&__dst[*(v5 + 20)] = *&__src[*(v5 + 20)];
      swift_storeEnumTagMultiPayload(__dst, v3, 1);
    }

    else
    {
      memcpy(__dst, __src, *(*(v3 - 8) + 64));
    }
  }

  v6 = *(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType)) + 48);
  v7 = *&__dst[v6];
  *&__dst[v6] = *&__src[v6];
  v7;
  return __dst;
}

uint64_t type metadata completion function for MLImageClassifier.ModelParameters.ModelAlgorithmType(uint64_t a1)
{
  result = type metadata accessor for MLImageClassifier.FeatureExtractorType(319);
  if (v2 <= 0x3F)
  {
    swift_getTupleTypeLayout2(v3, *(result - 8) + 64);
    swift_initEnumMetadataSingleCase(a1, 256, v3);
    *(*(a1 - 8) + 84) = v4;
    return 0;
  }

  return result;
}

uint64_t MLWordTaggerMetrics.isValid.getter()
{
  v7[0] = v0;
  v2 = type metadata accessor for MLClassifierMetrics.Contents(0);
  v3 = *(*(v2 - 8) + 64);
  v4 = alloca(v3);
  v5 = alloca(v3);
  outlined init with copy of MLClassifierMetrics.Contents(v1, v7);
  LOBYTE(v2) = swift_getEnumCaseMultiPayload(v7, v2) < 2;
  outlined destroy of MLActivityClassifier.ModelParameters(v7, type metadata accessor for MLClassifierMetrics.Contents);
  return v2;
}

uint64_t outlined init with copy of MLClassifierMetrics.Contents(uint64_t a1, uint64_t a2)
{
  v2 = type metadata accessor for MLClassifierMetrics.Contents(0);
  (*(*(v2 - 8) + 16))(a2, a1, v2);
  return a2;
}

uint64_t MLWordTaggerMetrics.error.getter()
{
  v1 = type metadata accessor for MLClassifierMetrics.Contents(0);
  v2 = *(*(v1 - 8) + 64);
  v3 = alloca(v2);
  v4 = alloca(v2);
  outlined init with copy of MLClassifierMetrics.Contents(v0, v6);
  if (swift_getEnumCaseMultiPayload(v6, v1) == 2)
  {
    return v6[0];
  }

  outlined destroy of MLActivityClassifier.ModelParameters(v6, type metadata accessor for MLClassifierMetrics.Contents);
  return 0;
}

double MLWordTaggerMetrics.taggingError.getter()
{
  v16[0] = v0;
  v2 = *(*(type metadata accessor for MLClassifierMetrics.Precomputed(0) - 8) + 64);
  v3 = alloca(v2);
  v4 = alloca(v2);
  v5 = *(*(type metadata accessor for AnyClassificationMetrics(0) - 8) + 64);
  v6 = alloca(v5);
  v7 = alloca(v5);
  v8 = type metadata accessor for MLClassifierMetrics.Contents(0);
  v9 = *(*(v8 - 8) + 64);
  v10 = alloca(v9);
  v11 = alloca(v9);
  outlined init with copy of MLClassifierMetrics.Contents(v1, v16);
  EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(v16, v8);
  if (EnumCaseMultiPayload)
  {
    if (EnumCaseMultiPayload != 1)
    {
      outlined destroy of MLActivityClassifier.ModelParameters(v16, type metadata accessor for MLClassifierMetrics.Contents);
      v14 = 0.0;
      return 1.0 - v14;
    }

    outlined init with take of MLClassifierMetrics(v16, v16, type metadata accessor for MLClassifierMetrics.Precomputed);
    v16[0] = 1.0 - v16[0];
    v13 = type metadata accessor for MLClassifierMetrics.Precomputed;
  }

  else
  {
    outlined init with take of MLClassifierMetrics(v16, v16, type metadata accessor for AnyClassificationMetrics);
    v16[0] = AnyClassificationMetrics.accuracy.getter();
    v13 = type metadata accessor for AnyClassificationMetrics;
  }

  outlined destroy of MLActivityClassifier.ModelParameters(v16, v13);
  v14 = v16[0];
  return 1.0 - v14;
}

uint64_t MLWordTaggerMetrics.precisionRecall.getter(__m128 a1)
{
  v6 = v1;
  v2 = *(*(type metadata accessor for DataFrame(0) - 8) + 64);
  v3 = alloca(v2);
  v4 = alloca(v2);
  *a1.i64 = MLClassifierMetrics.precisionRecallDataFrame.getter(0);
  return MLDataTable.init(_:convertArraysToShapedArrays:)(&v6, 0, a1);
}

uint64_t MLWordTaggerMetrics.playgroundDescription.getter()
{
  v1 = v0;
  v2 = MLClassifierMetrics.accuracyDescription.getter();
  v4 = v3;
  objc_allocWithZone(NSAttributedString);
  v5 = @nonobjc NSAttributedString.init(string:attributes:)(v2, v4, 0);
  result = type metadata accessor for NSAttributedString();
  v1[3] = result;
  *v1 = v5;
  return result;
}

char *assignWithCopy for MLWordTaggerMetrics(char *a1, char *a2)
{
  v2 = a1;
  if (a1 != a2)
  {
    outlined destroy of MLActivityClassifier.ModelParameters(a1, type metadata accessor for MLClassifierMetrics.Contents);
    v3 = type metadata accessor for MLClassifierMetrics.Contents(0);
    EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(a2, v3);
    if (EnumCaseMultiPayload == 2)
    {
      v10 = *a2;
      swift_errorRetain(*a2);
      *a1 = v10;
      v8 = 2;
      v9 = v3;
    }

    else if (EnumCaseMultiPayload == 1)
    {
      *a1 = *a2;
      v19 = type metadata accessor for MLClassifierMetrics.Precomputed(0);
      v5 = *(v19 + 20);
      v6 = type metadata accessor for DataFrame(0);
      v18 = v3;
      v7 = *(*(v6 - 8) + 16);
      v7(&a1[v5], &a2[v5], v6);
      v7(&a1[*(v19 + 24)], &a2[*(v19 + 24)], v6);
      v8 = 1;
      v9 = v18;
    }

    else
    {
      v11 = v3;
      v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Either<ClassificationMetrics<String>, ClassificationMetrics<Int>>);
      v13 = swift_getEnumCaseMultiPayload(a2, v12);
      v14 = v13 == 1;
      v15 = &demangling cache variable for type metadata for ClassificationMetrics<String>;
      if (v13 == 1)
      {
        v15 = &demangling cache variable for type metadata for ClassificationMetrics<Int>;
      }

      v16 = __swift_instantiateConcreteTypeFromMangledName(v15);
      (*(*(v16 - 8) + 16))(v2, a2, v16);
      swift_storeEnumTagMultiPayload(v2, v12, v14);
      a1 = v2;
      v9 = v11;
      v8 = 0;
    }

    swift_storeEnumTagMultiPayload(a1, v9, v8);
  }

  return v2;
}

char *assignWithTake for MLWordTaggerMetrics(char *__dst, char *__src)
{
  v2 = __dst;
  if (__dst == __src)
  {
    return v2;
  }

  outlined destroy of MLActivityClassifier.ModelParameters(__dst, type metadata accessor for MLClassifierMetrics.Contents);
  v3 = type metadata accessor for MLClassifierMetrics.Contents(0);
  EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(__src, v3);
  if (EnumCaseMultiPayload == 1)
  {
    *__dst = *__src;
    v17 = type metadata accessor for MLClassifierMetrics.Precomputed(0);
    v12 = *(v17 + 20);
    v13 = type metadata accessor for DataFrame(0);
    v16 = v3;
    v14 = *(*(v13 - 8) + 32);
    v14(&__dst[v12], &__src[v12], v13);
    v14(&__dst[*(v17 + 24)], &__src[*(v17 + 24)], v13);
    v11 = 1;
    v10 = v16;
LABEL_8:
    swift_storeEnumTagMultiPayload(__dst, v10, v11);
    return v2;
  }

  if (!EnumCaseMultiPayload)
  {
    v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Either<ClassificationMetrics<String>, ClassificationMetrics<Int>>);
    v6 = swift_getEnumCaseMultiPayload(__src, v5);
    v7 = v6 == 1;
    v8 = &demangling cache variable for type metadata for ClassificationMetrics<String>;
    if (v6 == 1)
    {
      v8 = &demangling cache variable for type metadata for ClassificationMetrics<Int>;
    }

    v9 = __swift_instantiateConcreteTypeFromMangledName(v8);
    (*(*(v9 - 8) + 32))(v2, __src, v9);
    swift_storeEnumTagMultiPayload(v2, v5, v7);
    __dst = v2;
    v10 = v3;
    v11 = 0;
    goto LABEL_8;
  }

  return memcpy(__dst, __src, *(*(v3 - 8) + 64));
}

uint64_t type metadata accessor for MLWordTaggerMetrics(uint64_t a1)
{
  result = type metadata singleton initialization cache for MLWordTaggerMetrics;
  if (!type metadata singleton initialization cache for MLWordTaggerMetrics)
  {
    return swift_getSingletonMetadata(a1, &nominal type descriptor for MLWordTaggerMetrics);
  }

  return result;
}

uint64_t type metadata completion function for MLWordTaggerMetrics(uint64_t a1)
{
  v4 = v1;
  result = type metadata accessor for MLClassifierMetrics.Contents(319);
  if (v3 <= 0x3F)
  {
    v4 = *(result - 8) + 64;
    swift_initStructMetadata(a1, 256, 1, &v4, a1 + 16);
    return 0;
  }

  return result;
}

uint64_t specialized DefaultStringInterpolation.appendInterpolation<A>(_:)(char a1)
{
  switch(a1)
  {
    case 0:
      v1 = 0xE300000000000000;
      v2._countAndFlagsBits = 7630409;
      break;
    case 1:
      v2._countAndFlagsBits = 0x656C62756F44;
      goto LABEL_7;
    case 2:
      v2._countAndFlagsBits = 0x676E69727453;
LABEL_7:
      v1 = 0xE600000000000000;
      break;
    case 3:
      v1 = 0xE800000000000000;
      v2._countAndFlagsBits = 0x65636E6575716553;
      break;
    case 4:
      v1 = 0xEA00000000007972;
      v2._countAndFlagsBits = 0x616E6F6974636944;
      break;
    case 5:
      v2._countAndFlagsBits = 0x72724169746C754DLL;
      v1 = 0xEA00000000007961;
      break;
    case 6:
      v1 = 0xE700000000000000;
      v2._countAndFlagsBits = 0x676E697373694DLL;
      break;
  }

  v2._object = v1;
  String.append(_:)(v2);
  return v1;
}

uint64_t _ss6ResultOsRi_zrlE8catchingAByxq_Gxyq_YKXE_tcfC8CreateML10_DataTableC_s5Error_pTt1g503_s8c10ML11MLDataf74V4pack12columnsNamed2to4type7fillingACSSd_SSAC8PackTypeOAA0C5ValueOtFAA05_E10D0CyKXEfU_AgE11CMLSequenceCSSAE0jS0O0sP0OAMTf1c_n(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, char a5, void *a6, double a7, void *a8, char a9)
{
  closure #1 in MLDataTable.pack(columnsNamed:to:type:filling:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, v13);

  a4;
  outlined consume of MLDataValue(a6, a8, a9);
  return v12;
}

uint64_t specialized Array<A>.featureSequence.getter(uint64_t a1)
{
  v1 = tc_v1_flex_list_create(0);
  if (!v1)
  {
    BUG();
  }

  v2 = v1;
  v3 = type metadata accessor for CMLSequence();
  v4 = swift_allocObject(v3, 25, 7);
  *(v4 + 16) = v2;
  v22 = v4;
  *(v4 + 24) = 1;
  v5 = *(a1 + 16);
  if (v5)
  {
    v23 = 0;
    v20 = type metadata accessor for CMLFeatureValue();
    a1;
    v21 = a1;
    v6 = (a1 + 40);
    do
    {
      v18 = v5;
      v7 = *(v6 - 1);
      v8 = *v6;
      v17[3] = &type metadata for String;
      v17[4] = &protocol witness table for String;
      v17[0] = v7;
      v17[1] = v8;
      v9 = __swift_project_boxed_opaque_existential_0Tm(v17, &type metadata for String);
      v10 = *v9;
      v11 = v9[1];
      v19 = v8;
      swift_bridgeObjectRetain_n(v8, 2);
      v11;
      v12 = v10;
      v13 = v23;
      v14 = CMLFeatureValue.__allocating_init(_:)(v12, v11);
      if (v13)
      {
        swift_unexpectedError(v13, "CreateML/MLDataValueConvertible.swift", 37, 1, 170);
        BUG();
      }

      v15 = v14;
      __swift_destroy_boxed_opaque_existential_1Tm(v17);
      CMLSequence.append(_:)(v15);

      v23 = 0;
      v19;
      v6 += 2;
      v5 = v18 - 1;
    }

    while (v18 != 1);
    v21;
  }

  return v22;
}

uint64_t MLDataTable.pack(columnsNamed:to:type:filling:)(uint64_t a1, uint64_t a2, uint64_t a3, char *a4, uint64_t a5, double a6)
{
  v8 = *v7;
  if (*(v7 + 8))
  {
    *v6 = v8;
    *(v6 + 8) = 1;
    return swift_errorRetain(v8);
  }

  else
  {
    v27 = a2;
    v26 = a3;
    v25 = v6;
    v32 = *a4;
    v11 = *a5;
    v12 = *(a5 + 8);
    v33 = *(a5 + 16);

    v13 = tc_v1_flex_list_create(0);
    if (!v13)
    {
      BUG();
    }

    v14 = v13;
    v29 = v11;
    v28 = v12;
    v15 = type metadata accessor for CMLSequence();
    inited = swift_initStackObject(v15, v24);
    *(inited + 16) = v14;
    *(inited + 24) = 1;
    if (*(a1 + 16))
    {
      v17 = specialized Array<A>.featureSequence.getter(a1);
    }

    else
    {
      _DataTable.columnNames.getter(v15);

      v17 = v31;
    }

    v30 = v17;
    v31 = v17;
    v18 = v26;
    v26;
    v19 = v33;
    v20 = v29;
    v21 = v28;
    outlined copy of MLDataValue(v29, v28, v33);
    v22 = _ss6ResultOsRi_zrlE8catchingAByxq_Gxyq_YKXE_tcfC8CreateML10_DataTableC_s5Error_pTt1g503_s8c10ML11MLDataf74V4pack12columnsNamed2to4type7fillingACSSd_SSAC8PackTypeOAA0C5ValueOtFAA05_E10D0CyKXEfU_AgE11CMLSequenceCSSAE0jS0O0sP0OAMTf1c_n(v8, &v31, v27, v18, (v32 & 1u) + 3, v20, a6, v21, v19);
    LOBYTE(v21) = v23;

    result = v25;
    *v25 = v22;
    *(result + 8) = v21 & 1;
  }

  return result;
}

void *closure #1 in MLDataTable.pack(columnsNamed:to:type:filling:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, char a5, void *a6, double a7, void *a8, char a9, void *a10)
{
  v30 = a4;
  v26 = a3;
  v29 = v10;
  v27 = v11;
  v13 = 0x5060403020100uLL >> (8 * a5);
  v28 = *(a1 + 16);
  v14 = *a2;
  v23 = a6;
  v24 = a8;
  v25 = a9;

  outlined copy of MLDataValue(a6, a8, a9);
  v15 = MLDataValue.featureValue.getter(a7);
  outlined consume of MLDataValue(v23, a8, a9);
  v19 = v28;
  v20 = v14;
  v21 = v13;
  v22 = v15;
  v30 = specialized String.withCString<A>(_:)(partial apply for closure #1 in CMLTable.pack(columnNames:newColumnName:type:value:), v18, v26, v30);

  if (v11)
  {
    result = a10;
    *a10 = v11;
  }

  else
  {
    v17 = type metadata accessor for _DataTable();
    swift_allocObject(v17, 40, 7);
    result = _DataTable.init(impl:)(v30);
    *v29 = result;
  }

  return result;
}

uint64_t MLDataTable.unpack(columnNamed:valueTypes:indexSubset:keySubset:)(uint64_t a1, void *a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  v9 = *v6;
  if (*(v6 + 8))
  {
    *v5 = v9;
    *(v5 + 8) = 1;
    return outlined copy of Result<_DataTable, Error>(v9, 1);
  }

  v74 = a5;
  *&v81 = a3;
  v82._countAndFlagsBits = a1;
  v83 = a2;
  v77 = v5;
  v84 = v9;
  outlined copy of Result<_DataTable, Error>(v9, 0);
  v11 = tc_v1_flex_list_create(0);
  if (!v11)
  {
    BUG();
  }

  v12 = v11;
  v13 = type metadata accessor for CMLSequence();
  inited = swift_initStackObject(v13, v67);
  *(inited + 16) = v12;
  *(inited + 24) = 1;
  if (a4)
  {
    v78 = inited;
    v79 = v84;
    v80 = 0;
    outlined copy of Result<_DataTable, Error>(v84, 0);
    v15._countAndFlagsBits = v82._countAndFlagsBits;
    v15._object = v83;
    MLDataTable.subscript.getter(v15);
    outlined consume of Result<_DataTable, Error>(v79, v80);
    v16 = v71;
    if (BYTE8(v71) == 1)
    {
      outlined consume of Result<_DataTable, Error>(v71, 1);
LABEL_15:
      *&v71 = 0;
      *(&v71 + 1) = 0xE000000000000000;
      _StringGuts.grow(_:)(80);
      v27._object = "ionary typed column. Column '" + 0x8000000000000000;
      v27._countAndFlagsBits = 0xD00000000000003DLL;
      String.append(_:)(v27);
      countAndFlagsBits = v82._countAndFlagsBits;
      v27._countAndFlagsBits = v82._countAndFlagsBits;
      v29 = v83;
      v27._object = v83;
      String.append(_:)(v27);
      v27._countAndFlagsBits = 0x20666F2073692027;
      v27._object = 0xEE00272065707974;
      String.append(_:)(v27);
      v30 = v84;
      v75 = v84;
      v76 = 0;
      outlined copy of Result<_DataTable, Error>(v84, 0);
      v27._countAndFlagsBits = countAndFlagsBits;
      v27._object = v29;
      MLDataTable.subscript.getter(v27);
      outlined consume of Result<_DataTable, Error>(v75, v76);
      v31 = v79;
      if (v80)
      {
        outlined consume of Result<_DataTable, Error>(v79, 1);
        v32 = 6;
      }

      else
      {

        _UntypedColumn.type.getter();
        outlined consume of Result<_DataTable, Error>(v31, 0);
        outlined consume of Result<_DataTable, Error>(v31, 0);
        v32 = v75;
      }

      specialized DefaultStringInterpolation.appendInterpolation<A>(_:)(v32);
      v33._countAndFlagsBits = 39;
      v33._object = 0xE100000000000000;
      String.append(_:)(v33);
      v81 = v71;
      v34 = lazy protocol witness table accessor for type MLCreateError and conformance MLCreateError();
      v35 = swift_allocError(&type metadata for MLCreateError, v34, 0, 0);
      *v36 = v81;
      *(v36 + 16) = 0;
      *(v36 + 32) = 0;
      *(v36 + 48) = 1;
      outlined consume of Result<_DataTable, Error>(v30, 0);
      swift_setDeallocating(v78);
      v37 = CMLFeatureValue.deinit(0.0);
      swift_deallocClassInstance(v37, 25, 7);
LABEL_38:
      result = v77;
      *v77 = v35;
      *(result + 8) = 1;
      return result;
    }

    _UntypedColumn.type.getter();
    outlined consume of Result<_DataTable, Error>(v16, 0);
    outlined consume of Result<_DataTable, Error>(v16, 0);
    if (v79 != 3)
    {
      goto LABEL_15;
    }

    v17 = tc_v1_flex_list_create(0);
    if (!v17)
    {
      BUG();
    }

    v18 = v17;
    v19 = swift_allocObject(v13, 25, 7);
    *(v19 + 16) = v18;
    v82._object = v19;
    *(v19 + 24) = 1;
    v70 = *(a4 + 16);
    if (v70)
    {
      a4;
      v20 = 0;
      do
      {
        v21 = *(a4 + 8 * v20 + 32);
        v72 = &type metadata for Int;
        v73 = &protocol witness table for Int;
        *&v71 = v21;
        v22 = __swift_project_boxed_opaque_existential_0Tm(&v71, &type metadata for Int);
        v23 = specialized handling<A, B>(_:_:)(*v22);
        if (!v23)
        {
          BUG();
        }

        v24 = a4;
        v25 = type metadata accessor for CMLFeatureValue();
        swift_initStackObject(v25, v68);
        v26 = CMLFeatureValue.init(rawValue:ownsValue:)(v23, 1);
        __swift_destroy_boxed_opaque_existential_1Tm(&v71);
        CMLSequence.append(_:)(v26);

        ++v20;
        a4 = v24;
      }

      while (v70 != v20);

      v24;
    }

    else
    {
    }
  }

  else
  {
    v82._object = inited;
  }

  v38 = v84;
  if (v74)
  {
    v79 = v84;
    v80 = 0;
    outlined copy of Result<_DataTable, Error>(v84, 0);
    v39._countAndFlagsBits = v82._countAndFlagsBits;
    v39._object = v83;
    MLDataTable.subscript.getter(v39);
    outlined consume of Result<_DataTable, Error>(v79, v80);
    v40 = v71;
    if (BYTE8(v71) == 1)
    {
      outlined consume of Result<_DataTable, Error>(v71, 1);
LABEL_32:
      *&v71 = 0;
      *(&v71 + 1) = 0xE000000000000000;
      _StringGuts.grow(_:)(80);
      v52._object = "CreateML/MLDataTable+Pack.swift" + 0x8000000000000000;
      v52._countAndFlagsBits = 0xD00000000000003DLL;
      String.append(_:)(v52);
      v53 = v82._countAndFlagsBits;
      v52._countAndFlagsBits = v82._countAndFlagsBits;
      v54 = v83;
      v52._object = v83;
      String.append(_:)(v52);
      v52._countAndFlagsBits = 0x20666F2073692027;
      v52._object = 0xEE00272065707974;
      String.append(_:)(v52);
      v75 = v38;
      v76 = 0;
      outlined copy of Result<_DataTable, Error>(v38, 0);
      v52._countAndFlagsBits = v53;
      v52._object = v54;
      MLDataTable.subscript.getter(v52);
      outlined consume of Result<_DataTable, Error>(v75, v76);
      v55 = v79;
      if (v80)
      {
        outlined consume of Result<_DataTable, Error>(v79, 1);
        v56 = 6;
      }

      else
      {

        _UntypedColumn.type.getter();
        outlined consume of Result<_DataTable, Error>(v55, 0);
        outlined consume of Result<_DataTable, Error>(v55, 0);
        v56 = v75;
      }

      specialized DefaultStringInterpolation.appendInterpolation<A>(_:)(v56);
      v60._countAndFlagsBits = 39;
      v60._object = 0xE100000000000000;
      String.append(_:)(v60);
      v81 = v71;
      v61 = lazy protocol witness table accessor for type MLCreateError and conformance MLCreateError();
      v35 = swift_allocError(&type metadata for MLCreateError, v61, 0, 0);
      *v62 = v81;
      *(v62 + 16) = 0;
      *(v62 + 32) = 0;
      *(v62 + 48) = 1;
      outlined consume of Result<_DataTable, Error>(v38, 0);

      goto LABEL_38;
    }

    _UntypedColumn.type.getter();
    outlined consume of Result<_DataTable, Error>(v40, 0);
    outlined consume of Result<_DataTable, Error>(v40, 0);
    if (v79 != 4)
    {
      goto LABEL_32;
    }

    v41 = specialized Array<A>.featureSequence.getter(v74);

    v82._object = v41;
  }

  v42 = tc_v1_flex_enum_list_create(0);
  if (!v42)
  {
    BUG();
  }

  v43 = v42;
  v44 = type metadata accessor for CMLFVTypeSequence();
  v45 = swift_initStackObject(v44, v69);
  v46 = v45;
  *(v45 + 16) = v43;
  if (v81 && (v47 = *(v81 + 16)) != 0)
  {
    v81;
    v48 = v81;
    for (i = 0; i != v47; ++i)
    {
      v50 = v46;
      CMLFVTypeSequence.append(_:)((0x5060403020100uLL >> (8 * *(v48 + i + 32))));
      if (v51)
      {
        swift_unexpectedError(v51, "CreateML/MLDataTable+Pack.swift", 31, 1, 118);
        BUG();
      }

      v48 = v81;
    }

    v81;
  }

  else
  {
    v50 = v45;
  }

  *&v81 = v66;
  v57 = alloca(40);
  v58 = alloca(48);
  v67[0] = *(v84 + 16);
  v67[1] = v50;
  v67[2] = v82._object;
  v59 = v83;
  v83;

  v63 = specialized String.withCString<A>(_:)(partial apply for closure #1 in CMLTable.unpack(columnName:types:limit:), v66, v82._countAndFlagsBits, v59);

  v64 = type metadata accessor for _DataTable();
  swift_allocObject(v64, 40, 7);
  v65 = _DataTable.init(impl:)(v63);
  outlined consume of Result<_DataTable, Error>(v84, 0);

  v83;
  swift_setDeallocating(v50);
  tc_v1_release(*(v50 + 16));

  result = v77;
  *v77 = v65;
  *(result + 8) = 0;
  return result;
}

Swift::Int MLDataTable.PackType.hashValue.getter()
{
  v1 = *v0;
  Hasher.init(_seed:)(0);
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t lazy protocol witness table accessor for type MLDataTable.PackType and conformance MLDataTable.PackType()
{
  result = lazy protocol witness table cache variable for type MLDataTable.PackType and conformance MLDataTable.PackType;
  if (!lazy protocol witness table cache variable for type MLDataTable.PackType and conformance MLDataTable.PackType)
  {
    result = swift_getWitnessTable(&protocol conformance descriptor for MLDataTable.PackType, &type metadata for MLDataTable.PackType);
    lazy protocol witness table cache variable for type MLDataTable.PackType and conformance MLDataTable.PackType = result;
  }

  return result;
}

uint64_t MLObjectDetector.ModelParameters.gridSize.getter()
{
  v1 = type metadata accessor for MLObjectDetector.ModelParameters(0);
  v2 = *(v1 + 28);
  result = *(v1 + 32);
  v4 = *(v0 + v2);
  v5 = *(v0 + result);
  return result;
}

uint64_t type metadata accessor for MLObjectDetector.ModelParameters(uint64_t a1)
{
  result = type metadata singleton initialization cache for MLObjectDetector.ModelParameters;
  if (!type metadata singleton initialization cache for MLObjectDetector.ModelParameters)
  {
    return swift_getSingletonMetadata(a1, &nominal type descriptor for MLObjectDetector.ModelParameters);
  }

  return result;
}

char MLObjectDetector.ModelParameters.algorithm.getter()
{
  v2 = v0;
  v3 = type metadata accessor for MLObjectDetector.ModelParameters(0);
  outlined init with copy of Any?(v1 + *(v3 + 40), v7);
  if (!v8)
  {
    outlined destroy of Any?(v7);
    goto LABEL_5;
  }

  if (!swift_dynamicCast(&v9, v7, &type metadata for Any + 8, &type metadata for MLObjectDetector.ModelParameters.ModelAlgorithmType, 6))
  {
LABEL_5:
    v5 = 1;
    v4 = 0;
    goto LABEL_6;
  }

  v4 = v9;
  v5 = v10;
LABEL_6:
  *v2 = v4;
  result = v5 & 1;
  *(v2 + 8) = result;
  return result;
}

uint64_t MLObjectDetector.ModelParameters.init(validation:batchSize:maxIterations:gridSize:algorithm:)(uint64_t a1, uint64_t a2, char a3, uint64_t a4, char a5, uint64_t *a6, double a7, double a8)
{
  v21 = a4;
  v10 = v8;
  v24 = a2;
  v22 = a8;
  v23 = a7;
  v26 = a1;
  v20 = *a6;
  v27 = *(a6 + 8);
  v12 = type metadata accessor for MLObjectDetector.ModelParameters(0);
  v13 = v12[5];
  v14 = v12[6];
  *(v10 + v12[7]) = 13;
  *(v10 + v12[8]) = 13;
  *(v10 + v12[9]) = 0;
  v15 = v12[10];
  v25 = v10 + v15;
  *(v10 + v15 + 16) = 0;
  *(v10 + v15) = 0;
  outlined init with copy of MLObjectDetector.ModelParameters.ValidationData(v26, v10);
  *(v10 + v13) = v24;
  *(v10 + v13 + 8) = a3 & 1;
  *(v10 + v14) = v21;
  *(v10 + v14 + 8) = a5 & 1;
  MLObjectDetector.ModelParameters.gridSize.setter(v23, v22);
  v19 = &type metadata for MLObjectDetector.ModelParameters.ModelAlgorithmType;
  v17 = v20;
  v18 = v27;
  outlined assign with take of Any?(&v17, v25);
  return outlined destroy of MLObjectDetector.ModelParameters.ValidationData(v26);
}

uint64_t MLObjectDetector.ModelParameters.init(validation:batchSize:maxIterations:)(uint64_t a1, uint64_t a2, char a3, uint64_t a4, char a5)
{
  v8 = v5;
  v9 = type metadata accessor for MLObjectDetector.ModelParameters(0);
  v10 = v9[5];
  v11 = v9[6];
  *(v8 + v9[7]) = 13;
  *(v8 + v9[8]) = 13;
  *(v8 + v9[9]) = 0;
  v12 = v9[10];
  *(v8 + v12 + 16) = 0;
  *(v8 + v12) = 0;
  outlined init with take of MLClassifierMetrics(a1, v8, type metadata accessor for MLObjectDetector.ModelParameters.ValidationData);
  *(v8 + v10) = a2;
  *(v8 + v10 + 8) = a3 & 1;
  result = a4;
  *(v8 + v11) = a4;
  *(v8 + v11 + 8) = a5 & 1;
  return result;
}

uint64_t MLObjectDetector.ModelParameters.batchSize.getter()
{
  v1 = *(type metadata accessor for MLObjectDetector.ModelParameters(0) + 20);
  result = *(v0 + v1);
  v3 = *(v0 + v1 + 8);
  return result;
}

uint64_t MLObjectDetector.ModelParameters.batchSize.setter(uint64_t a1, char a2)
{
  result = *(type metadata accessor for MLObjectDetector.ModelParameters(0) + 20);
  *(v2 + result) = a1;
  *(v2 + result + 8) = a2 & 1;
  return result;
}

uint64_t MLObjectDetector.ModelParameters.maxIterations.getter()
{
  v1 = *(type metadata accessor for MLObjectDetector.ModelParameters(0) + 24);
  result = *(v0 + v1);
  v3 = *(v0 + v1 + 8);
  return result;
}

uint64_t MLObjectDetector.ModelParameters.maxIterations.setter(uint64_t a1, char a2)
{
  result = *(type metadata accessor for MLObjectDetector.ModelParameters(0) + 24);
  *(v2 + result) = a1;
  *(v2 + result + 8) = a2 & 1;
  return result;
}

uint64_t MLObjectDetector.ModelParameters.gridSize.setter(double a1, double a2)
{
  if ((*&a1 & 0x7FFFFFFFFFFFFFFFuLL) > 0x7FEFFFFFFFFFFFFFLL)
  {
    BUG();
  }

  if (a1 <= -9.223372036854778e18)
  {
    BUG();
  }

  if (a1 >= 9.223372036854776e18)
  {
    BUG();
  }

  v3 = type metadata accessor for MLObjectDetector.ModelParameters(0);
  *(v2 + *(v3 + 28)) = a1;
  if ((*&a2 & 0x7FFFFFFFFFFFFFFFuLL) > 0x7FEFFFFFFFFFFFFFLL)
  {
    BUG();
  }

  if (a2 <= -9.223372036854778e18)
  {
    BUG();
  }

  if (a2 >= 9.223372036854776e18)
  {
    BUG();
  }

  result = *(v3 + 32);
  *(v2 + result) = a2;
  return result;
}

uint64_t (*MLObjectDetector.ModelParameters.gridSize.modify(uint64_t a1))(uint64_t a1)
{
  *(a1 + 16) = v1;
  v2 = type metadata accessor for MLObjectDetector.ModelParameters(0);
  v3 = *(v1 + *(v2 + 32));
  *a1 = *(v1 + *(v2 + 28));
  *(a1 + 8) = v3;
  return MLObjectDetector.ModelParameters.gridSize.modify;
}

uint64_t key path getter for MLObjectDetector.ModelParameters.algorithm : MLObjectDetector.ModelParameters()
{
  v1 = v0;
  MLObjectDetector.ModelParameters.algorithm.getter();
  result = v3;
  *v1 = v3;
  *(v1 + 8) = v4;
  return result;
}

uint64_t key path setter for MLObjectDetector.ModelParameters.algorithm : MLObjectDetector.ModelParameters(uint64_t a1)
{
  v1 = *(a1 + 8);
  v3 = *a1;
  v4 = v1;
  return MLObjectDetector.ModelParameters.algorithm.setter(&v3);
}

uint64_t MLObjectDetector.ModelParameters.algorithm.setter(uint64_t *a1)
{
  v2 = *a1;
  v3 = *(a1 + 8);
  v8 = &type metadata for MLObjectDetector.ModelParameters.ModelAlgorithmType;
  v6 = v2;
  v7 = v3;
  v4 = type metadata accessor for MLObjectDetector.ModelParameters(0);
  return outlined assign with take of Any?(&v6, v1 + *(v4 + 40));
}

uint64_t outlined destroy of MLObjectDetector.ModelParameters.ValidationData(uint64_t a1)
{
  v1 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
  (*(*(v1 - 8) + 8))(a1, v1);
  return a1;
}

void (*MLObjectDetector.ModelParameters.algorithm.modify(void *a1))(uint64_t a1)
{
  v2 = malloc(0x48uLL);
  *a1 = v2;
  *(v2 + 8) = v1;
  v3 = *(type metadata accessor for MLObjectDetector.ModelParameters(0) + 40);
  *(v2 + 11) = v3;
  outlined init with copy of Any?(v1 + v3, v2);
  if (!*(v2 + 3))
  {
    outlined destroy of Any?(v2);
    goto LABEL_5;
  }

  if (!swift_dynamicCast(v2 + 48, v2, &type metadata for Any + 8, &type metadata for MLObjectDetector.ModelParameters.ModelAlgorithmType, 6))
  {
LABEL_5:
    v5 = 1;
    v4 = 0;
    goto LABEL_6;
  }

  v4 = *(v2 + 6);
  v5 = v2[56];
LABEL_6:
  *(v2 + 4) = v4;
  v2[40] = v5 & 1;
  return MLObjectDetector.ModelParameters.algorithm.modify;
}

void MLObjectDetector.ModelParameters.algorithm.modify(uint64_t a1)
{
  v1 = *a1;
  v2 = *(*a1 + 32);
  v3 = *(*a1 + 40);
  v4 = *(*a1 + 64) + *(*a1 + 44);
  v1[3] = &type metadata for MLObjectDetector.ModelParameters.ModelAlgorithmType;
  *v1 = v2;
  *(v1 + 8) = v3;
  outlined assign with take of Any?(v1, v4);
  free(v1);
}

uint64_t MLObjectDetector.ModelParameters.init(validationData:batchSize:maxIterations:)(uint64_t a1, uint64_t a2, char a3, uint64_t a4, char a5)
{
  v8 = v5;
  v9 = type metadata accessor for MLObjectDetector.ModelParameters(0);
  v10 = v9[5];
  v15 = v9[6];
  *(v8 + v9[7]) = 13;
  *(v8 + v9[8]) = 13;
  *(v8 + v9[9]) = 0;
  v11 = v9[10];
  *(v8 + v11 + 16) = 0;
  *(v8 + v11) = 0;
  outlined init with take of MLClassifierMetrics(a1, v8, type metadata accessor for MLObjectDetector.DataSource);
  v12 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
  swift_storeEnumTagMultiPayload(v8, v12, 1);
  *(v8 + v10) = a2;
  *(v8 + v10 + 8) = a3 & 1;
  result = a4;
  *(v8 + v15) = a4;
  *(v8 + v15 + 8) = a5 & 1;
  return result;
}

unint64_t MLObjectDetector.ModelParameters.description.getter()
{
  _StringGuts.grow(_:)(19);
  0xE000000000000000;
  v16 = 0xD000000000000010;
  v17 = "ansformer have different types." + 0x8000000000000000;
  v18 = type metadata accessor for MLObjectDetector.ModelParameters(0);
  v1 = *(v18 + 24);
  v2 = *(v0 + v1 + 8);
  v15._countAndFlagsBits = *(v0 + v1);
  LOBYTE(v15._object) = v2;
  v3 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Int?);
  v4._countAndFlagsBits = String.init<A>(describing:)(&v15, v3);
  object = v4._object;
  String.append(_:)(v4);
  object;
  v6._object = 0xE100000000000000;
  v6._countAndFlagsBits = 10;
  String.append(_:)(v6);
  strcpy(&v15, "Batch Size: ");
  BYTE5(v15._object) = 0;
  HIWORD(v15._object) = -5120;
  v7 = *(v18 + 20);
  v8 = *(v0 + v7);
  LOBYTE(v7) = *(v0 + v7 + 8);
  v13 = v8;
  v14 = v7;
  v9._countAndFlagsBits = String.init<A>(describing:)(&v13, v3);
  v10 = v9._object;
  String.append(_:)(v9);
  v10;
  v6._countAndFlagsBits = 10;
  v6._object = 0xE100000000000000;
  String.append(_:)(v6);
  v11 = v15._object;
  String.append(_:)(v15);
  v11;
  return v16;
}

uint64_t outlined assign with take of MLObjectDetector.ModelParameters.ValidationData(uint64_t a1, uint64_t a2)
{
  v2 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
  (*(*(v2 - 8) + 40))(a2, a1, v2);
  return a2;
}

unint64_t MLObjectDetector.ModelParameters.playgroundDescription.getter()
{
  v1 = v0;
  result = MLObjectDetector.ModelParameters.description.getter();
  v1[3] = &type metadata for String;
  *v1 = result;
  v1[1] = v3;
  return result;
}

void sub_305ABD(double a1, double a2)
{
  v3 = v2;
  *v2 = MLObjectDetector.ModelParameters.gridSize.getter();
  v3[1] = a2;
}

void *initializeBufferWithCopyOfBuffer for MLObjectDetector.ModelParameters(char *__dst, char *__src, int *a3)
{
  v4 = __dst;
  v5 = *(*(a3 - 1) + 80);
  if ((v5 & 0x20000) != 0)
  {
    v13 = *__src;
    *v4 = *__src;
    v4 = (v13 + ((v5 + 16) & ~v5));

    return v4;
  }

  v7 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
  EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(__src, v7);
  switch(EnumCaseMultiPayload)
  {
    case 3:
      v21 = type metadata accessor for DataFrame(0);
      (*(*(v21 - 8) + 16))(__dst, __src, v21);
      v22 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
      v23 = *(v22 + 48);
      *&__dst[v23] = *&__src[v23];
      v24 = *&__src[v23 + 8];
      *(v4 + v23 + 8) = v24;
      v25 = *(v22 + 64);
      *(v4 + v25) = *&__src[v25];
      v26 = v7;
      v27 = *&__src[v25 + 8];
      *(v4 + v25 + 8) = v27;
      v24;
      v27;
      v18 = 3;
      v19 = v4;
      v20 = v26;
      break;
    case 2:
      v14 = *__src;
      v49 = v7;
      v15 = __src[8];
      outlined copy of Result<_DataTable, Error>(*__src, v15);
      *__dst = v14;
      __dst[8] = v15;
      *(__dst + 2) = *(__src + 2);
      v16 = *(__src + 3);
      v4[3] = v16;
      v4[4] = *(__src + 4);
      v17 = *(__src + 5);
      v4[5] = v17;
      v16;
      v17;
      v18 = 2;
      v19 = v4;
      v20 = v49;
      break;
    case 1:
      v9 = type metadata accessor for MLObjectDetector.DataSource(0);
      switch(swift_getEnumCaseMultiPayload(__src, v9))
      {
        case 0u:
          v10 = type metadata accessor for URL(0);
          (*(*(v10 - 8) + 16))(__dst, __src, v10);
          v11 = v9;
          v12 = 0;
          goto LABEL_15;
        case 1u:
          v52 = v9;
          v37 = type metadata accessor for URL(0);
          v51 = v7;
          v38 = *(*(v37 - 8) + 16);
          v38(__dst, __src, v37);
          v39 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (at: URL, annotationFile: URL));
          v38(&__dst[*(v39 + 48)], &__src[*(v39 + 48)], v37);
          v7 = v51;
          v48 = 1;
          goto LABEL_14;
        case 2u:
          v52 = v9;
          v28 = *__src;
          v50 = __src[8];
          outlined copy of Result<_DataTable, Error>(*__src, v50);
          *__dst = v28;
          __dst[8] = v50;
          *(__dst + 2) = *(__src + 2);
          v29 = *(__src + 3);
          v4[3] = v29;
          v4[4] = *(__src + 4);
          v30 = *(__src + 5);
          v4[5] = v30;
          v29;
          v30;
          v48 = 2;
          goto LABEL_14;
        case 3u:
          v31 = type metadata accessor for DataFrame(0);
          (*(*(v31 - 8) + 16))(__dst, __src, v31);
          v32 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
          v33 = *(v32 + 48);
          *&__dst[v33] = *&__src[v33];
          v34 = *&__src[v33 + 8];
          *(v4 + v33 + 8) = v34;
          v35 = *(v32 + 64);
          *(v4 + v35) = *&__src[v35];
          v52 = v9;
          v36 = *&__src[v35 + 8];
          *(v4 + v35 + 8) = v36;
          v34;
          v36;
          v48 = 3;
LABEL_14:
          v12 = v48;
          __dst = v4;
          v11 = v52;
LABEL_15:
          swift_storeEnumTagMultiPayload(__dst, v11, v12);
          v18 = 1;
          v19 = v4;
          v20 = v7;
          break;
      }

      break;
    default:
      memcpy(__dst, __src, *(*(v7 - 8) + 64));
      goto LABEL_17;
  }

  swift_storeEnumTagMultiPayload(v19, v20, v18);
LABEL_17:
  v40 = a3[5];
  *(v4 + v40 + 8) = __src[v40 + 8];
  *(v4 + v40) = *&__src[v40];
  v41 = a3[6];
  *(v4 + v41) = *&__src[v41];
  *(v4 + v41 + 8) = __src[v41 + 8];
  *(v4 + a3[7]) = *&__src[a3[7]];
  *(v4 + a3[8]) = *&__src[a3[8]];
  *(v4 + a3[9]) = __src[a3[9]];
  v42 = a3[10];
  v43 = v4 + v42;
  v44 = &__src[v42];
  v45 = *&__src[v42 + 24];
  if (v45)
  {
    *(v43 + 3) = v45;
    (**(v45 - 8))(v43, v44);
  }

  else
  {
    v46 = *v44;
    *(v43 + 1) = *(v44 + 1);
    *v43 = v46;
  }

  return v4;
}

uint64_t destroy for MLObjectDetector.ModelParameters(uint64_t a1, uint64_t a2)
{
  v3 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
  EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(a1, v3);
  switch(EnumCaseMultiPayload)
  {
    case 3:
LABEL_7:
      v8 = type metadata accessor for DataFrame(0);
      (*(*(v8 - 8) + 8))(a1, v8);
      v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
      *(a1 + *(v9 + 48) + 8);
      v7 = *(a1 + *(v9 + 64) + 8);
      goto LABEL_8;
    case 2:
LABEL_6:
      outlined consume of Result<_DataTable, Error>(*a1, *(a1 + 8));
      *(a1 + 24);
      v7 = *(a1 + 40);
LABEL_8:
      v7;
      break;
    case 1:
      v5 = type metadata accessor for MLObjectDetector.DataSource(0);
      switch(swift_getEnumCaseMultiPayload(a1, v5))
      {
        case 0u:
          v6 = type metadata accessor for URL(0);
          (*(*(v6 - 8) + 8))(a1, v6);
          break;
        case 1u:
          v11 = type metadata accessor for URL(0);
          v12 = *(*(v11 - 8) + 8);
          v12(a1, v11);
          v13 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (at: URL, annotationFile: URL));
          v12(a1 + *(v13 + 48), v11);
          break;
        case 2u:
          goto LABEL_6;
        case 3u:
          goto LABEL_7;
        default:
          goto LABEL_9;
      }

      break;
  }

LABEL_9:
  result = *(a2 + 40);
  if (*(a1 + result + 24))
  {
    return __swift_destroy_boxed_opaque_existential_1Tm((result + a1));
  }

  return result;
}

void *initializeWithCopy for MLObjectDetector.ModelParameters(char *__dst, char *__src, int *a3)
{
  v5 = __dst;
  v6 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
  EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(__src, v6);
  switch(EnumCaseMultiPayload)
  {
    case 3:
      v19 = type metadata accessor for DataFrame(0);
      (*(*(v19 - 8) + 16))(__dst, __src, v19);
      v20 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
      v21 = *(v20 + 48);
      *&__dst[v21] = *&__src[v21];
      v22 = *&__src[v21 + 8];
      *(v5 + v21 + 8) = v22;
      v23 = *(v20 + 64);
      *(v5 + v23) = *&__src[v23];
      v24 = v6;
      v25 = *&__src[v23 + 8];
      *(v5 + v23 + 8) = v25;
      v22;
      v25;
      v16 = 3;
      v17 = v5;
      v18 = v24;
      break;
    case 2:
      v12 = *__src;
      v47 = v6;
      v13 = __src[8];
      outlined copy of Result<_DataTable, Error>(*__src, v13);
      *__dst = v12;
      __dst[8] = v13;
      *(__dst + 2) = *(__src + 2);
      v14 = *(__src + 3);
      v5[3] = v14;
      v5[4] = *(__src + 4);
      v15 = *(__src + 5);
      v5[5] = v15;
      v14;
      v15;
      v16 = 2;
      v17 = v5;
      v18 = v47;
      break;
    case 1:
      v8 = type metadata accessor for MLObjectDetector.DataSource(0);
      switch(swift_getEnumCaseMultiPayload(__src, v8))
      {
        case 0u:
          v9 = type metadata accessor for URL(0);
          (*(*(v9 - 8) + 16))(__dst, __src, v9);
          v10 = v8;
          v11 = 0;
          goto LABEL_13;
        case 1u:
          v50 = v8;
          v35 = type metadata accessor for URL(0);
          v49 = v6;
          v36 = *(*(v35 - 8) + 16);
          v36(__dst, __src, v35);
          v37 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (at: URL, annotationFile: URL));
          v36(&__dst[*(v37 + 48)], &__src[*(v37 + 48)], v35);
          v6 = v49;
          v46 = 1;
          goto LABEL_12;
        case 2u:
          v50 = v8;
          v26 = *__src;
          v48 = __src[8];
          outlined copy of Result<_DataTable, Error>(*__src, v48);
          *__dst = v26;
          __dst[8] = v48;
          *(__dst + 2) = *(__src + 2);
          v27 = *(__src + 3);
          v5[3] = v27;
          v5[4] = *(__src + 4);
          v28 = *(__src + 5);
          v5[5] = v28;
          v27;
          v28;
          v46 = 2;
          goto LABEL_12;
        case 3u:
          v29 = type metadata accessor for DataFrame(0);
          (*(*(v29 - 8) + 16))(__dst, __src, v29);
          v30 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
          v31 = *(v30 + 48);
          *&__dst[v31] = *&__src[v31];
          v32 = *&__src[v31 + 8];
          *(v5 + v31 + 8) = v32;
          v33 = *(v30 + 64);
          *(v5 + v33) = *&__src[v33];
          v50 = v8;
          v34 = *&__src[v33 + 8];
          *(v5 + v33 + 8) = v34;
          v32;
          v34;
          v46 = 3;
LABEL_12:
          v11 = v46;
          __dst = v5;
          v10 = v50;
LABEL_13:
          swift_storeEnumTagMultiPayload(__dst, v10, v11);
          v16 = 1;
          v17 = v5;
          v18 = v6;
          break;
      }

      break;
    default:
      memcpy(__dst, __src, *(*(v6 - 8) + 64));
      goto LABEL_15;
  }

  swift_storeEnumTagMultiPayload(v17, v18, v16);
LABEL_15:
  v38 = a3[5];
  *(v5 + v38 + 8) = __src[v38 + 8];
  *(v5 + v38) = *&__src[v38];
  v39 = a3[6];
  *(v5 + v39) = *&__src[v39];
  *(v5 + v39 + 8) = __src[v39 + 8];
  *(v5 + a3[7]) = *&__src[a3[7]];
  *(v5 + a3[8]) = *&__src[a3[8]];
  *(v5 + a3[9]) = __src[a3[9]];
  v40 = a3[10];
  v41 = v5 + v40;
  v42 = &__src[v40];
  v43 = *&__src[v40 + 24];
  if (v43)
  {
    *(v41 + 3) = v43;
    (**(v43 - 8))(v41, v42);
  }

  else
  {
    v44 = *v42;
    *(v41 + 1) = *(v42 + 1);
    *v41 = v44;
  }

  return v5;
}

void *assignWithCopy for MLObjectDetector.ModelParameters(char *__dst, char *__src, int *a3)
{
  v5 = __dst;
  if (__dst != __src)
  {
    outlined destroy of MLObjectDetector.ModelParameters.ValidationData(__dst);
    v6 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
    EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(__src, v6);
    switch(EnumCaseMultiPayload)
    {
      case 3:
        v19 = type metadata accessor for DataFrame(0);
        (*(*(v19 - 8) + 16))(__dst, __src, v19);
        v20 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
        v21 = *(v20 + 48);
        *&__dst[v21] = *&__src[v21];
        v22 = *&__src[v21 + 8];
        *(v5 + v21 + 8) = v22;
        v23 = *(v20 + 64);
        *(v5 + v23) = *&__src[v23];
        v24 = v6;
        v25 = *&__src[v23 + 8];
        *(v5 + v23 + 8) = v25;
        v22;
        v25;
        v16 = 3;
        v17 = v5;
        v18 = v24;
        break;
      case 2:
        v12 = *__src;
        v47 = v6;
        v13 = __src[8];
        outlined copy of Result<_DataTable, Error>(*__src, v13);
        *__dst = v12;
        __dst[8] = v13;
        *(__dst + 2) = *(__src + 2);
        v14 = *(__src + 3);
        v5[3] = v14;
        v5[4] = *(__src + 4);
        v15 = *(__src + 5);
        v5[5] = v15;
        v14;
        v15;
        v16 = 2;
        v17 = v5;
        v18 = v47;
        break;
      case 1:
        v8 = type metadata accessor for MLObjectDetector.DataSource(0);
        switch(swift_getEnumCaseMultiPayload(__src, v8))
        {
          case 0u:
            v9 = type metadata accessor for URL(0);
            (*(*(v9 - 8) + 16))(__dst, __src, v9);
            v10 = v8;
            v11 = 0;
            goto LABEL_14;
          case 1u:
            v50 = v8;
            v35 = type metadata accessor for URL(0);
            v49 = v6;
            v36 = *(*(v35 - 8) + 16);
            v36(__dst, __src, v35);
            v37 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (at: URL, annotationFile: URL));
            v36(&__dst[*(v37 + 48)], &__src[*(v37 + 48)], v35);
            v6 = v49;
            v46 = 1;
            goto LABEL_13;
          case 2u:
            v50 = v8;
            v26 = *__src;
            v48 = __src[8];
            outlined copy of Result<_DataTable, Error>(*__src, v48);
            *__dst = v26;
            __dst[8] = v48;
            *(__dst + 2) = *(__src + 2);
            v27 = *(__src + 3);
            v5[3] = v27;
            v5[4] = *(__src + 4);
            v28 = *(__src + 5);
            v5[5] = v28;
            v27;
            v28;
            v46 = 2;
            goto LABEL_13;
          case 3u:
            v29 = type metadata accessor for DataFrame(0);
            (*(*(v29 - 8) + 16))(__dst, __src, v29);
            v30 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
            v31 = *(v30 + 48);
            *&__dst[v31] = *&__src[v31];
            v32 = *&__src[v31 + 8];
            *(v5 + v31 + 8) = v32;
            v33 = *(v30 + 64);
            *(v5 + v33) = *&__src[v33];
            v50 = v8;
            v34 = *&__src[v33 + 8];
            *(v5 + v33 + 8) = v34;
            v32;
            v34;
            v46 = 3;
LABEL_13:
            v11 = v46;
            __dst = v5;
            v10 = v50;
LABEL_14:
            swift_storeEnumTagMultiPayload(__dst, v10, v11);
            v16 = 1;
            v17 = v5;
            v18 = v6;
            break;
        }

        break;
      default:
        memcpy(__dst, __src, *(*(v6 - 8) + 64));
        goto LABEL_16;
    }

    swift_storeEnumTagMultiPayload(v17, v18, v16);
  }

LABEL_16:
  v38 = a3[5];
  *(v5 + v38 + 8) = __src[v38 + 8];
  *(v5 + v38) = *&__src[v38];
  v39 = a3[6];
  *(v5 + v39) = *&__src[v39];
  *(v5 + v39 + 8) = __src[v39 + 8];
  *(v5 + a3[7]) = *&__src[a3[7]];
  *(v5 + a3[8]) = *&__src[a3[8]];
  *(v5 + a3[9]) = __src[a3[9]];
  v40 = a3[10];
  v41 = v5 + v40;
  v42 = &__src[v40];
  v43 = *&__src[v40 + 24];
  if (!*(v5 + v40 + 24))
  {
    if (v43)
    {
      *(v41 + 3) = v43;
      (**(v43 - 8))(v41, v42);
      return v5;
    }

LABEL_22:
    v44 = *v42;
    *(v41 + 1) = *(v42 + 1);
    *v41 = v44;
    return v5;
  }

  if (!v43)
  {
    __swift_destroy_boxed_opaque_existential_1Tm((v5 + v40));
    goto LABEL_22;
  }

  __swift_assign_boxed_opaque_existential_0((v5 + v40), &__src[v40]);
  return v5;
}

char *initializeWithTake for MLObjectDetector.ModelParameters(char *__dst, char *__src, int *a3)
{
  v5 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
  EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(__src, v5);
  if (EnumCaseMultiPayload == 3)
  {
    v12 = type metadata accessor for DataFrame(0);
    (*(*(v12 - 8) + 32))(__dst, __src, v12);
    v13 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
    *&__dst[*(v13 + 48)] = *&__src[*(v13 + 48)];
    *&__dst[*(v13 + 64)] = *&__src[*(v13 + 64)];
    swift_storeEnumTagMultiPayload(__dst, v5, 3);
  }

  else
  {
    if (EnumCaseMultiPayload == 1)
    {
      v7 = type metadata accessor for MLObjectDetector.DataSource(0);
      v8 = swift_getEnumCaseMultiPayload(__src, v7);
      if (v8 == 3)
      {
        v15 = type metadata accessor for DataFrame(0);
        (*(*(v15 - 8) + 32))(__dst, __src, v15);
        v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
        *&__dst[*(v16 + 48)] = *&__src[*(v16 + 48)];
        *&__dst[*(v16 + 64)] = *&__src[*(v16 + 64)];
        v22 = 3;
      }

      else
      {
        if (v8 != 1)
        {
          if (v8)
          {
            memcpy(__dst, __src, *(*(v7 - 8) + 64));
            goto LABEL_14;
          }

          v9 = type metadata accessor for URL(0);
          (*(*(v9 - 8) + 32))(__dst, __src, v9);
          v10 = v7;
          v11 = 0;
LABEL_12:
          swift_storeEnumTagMultiPayload(__dst, v10, v11);
LABEL_14:
          swift_storeEnumTagMultiPayload(__dst, v5, 1);
          goto LABEL_15;
        }

        v23 = type metadata accessor for URL(0);
        v24 = *(*(v23 - 8) + 32);
        v24(__dst, __src, v23);
        v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (at: URL, annotationFile: URL));
        v24(&__dst[*(v14 + 48)], &__src[*(v14 + 48)], v23);
        v22 = 1;
      }

      v11 = v22;
      v10 = v7;
      goto LABEL_12;
    }

    memcpy(__dst, __src, *(*(v5 - 8) + 64));
  }

LABEL_15:
  v17 = a3[5];
  __dst[v17 + 8] = __src[v17 + 8];
  *&__dst[v17] = *&__src[v17];
  v18 = a3[6];
  *&__dst[v18] = *&__src[v18];
  __dst[v18 + 8] = __src[v18 + 8];
  *&__dst[a3[7]] = *&__src[a3[7]];
  *&__dst[a3[8]] = *&__src[a3[8]];
  __dst[a3[9]] = __src[a3[9]];
  v19 = a3[10];
  v20 = *&__src[v19];
  *&__dst[v19 + 16] = *&__src[v19 + 16];
  *&__dst[v19] = v20;
  return __dst;
}

char *assignWithTake for MLObjectDetector.ModelParameters(char *__dst, char *__src, int *a3)
{
  if (__dst != __src)
  {
    outlined destroy of MLObjectDetector.ModelParameters.ValidationData(__dst);
    v5 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
    EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(__src, v5);
    if (EnumCaseMultiPayload == 3)
    {
      v12 = type metadata accessor for DataFrame(0);
      (*(*(v12 - 8) + 32))(__dst, __src, v12);
      v13 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
      *&__dst[*(v13 + 48)] = *&__src[*(v13 + 48)];
      *&__dst[*(v13 + 64)] = *&__src[*(v13 + 64)];
      swift_storeEnumTagMultiPayload(__dst, v5, 3);
      goto LABEL_16;
    }

    if (EnumCaseMultiPayload != 1)
    {
      memcpy(__dst, __src, *(*(v5 - 8) + 64));
      goto LABEL_16;
    }

    v7 = type metadata accessor for MLObjectDetector.DataSource(0);
    v8 = swift_getEnumCaseMultiPayload(__src, v7);
    if (v8 == 3)
    {
      v15 = type metadata accessor for DataFrame(0);
      (*(*(v15 - 8) + 32))(__dst, __src, v15);
      v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (DataFrame, imageColumn: String, annotationColumn: String));
      *&__dst[*(v16 + 48)] = *&__src[*(v16 + 48)];
      *&__dst[*(v16 + 64)] = *&__src[*(v16 + 64)];
      v23 = 3;
    }

    else
    {
      if (v8 != 1)
      {
        if (v8)
        {
          memcpy(__dst, __src, *(*(v7 - 8) + 64));
          goto LABEL_15;
        }

        v9 = type metadata accessor for URL(0);
        (*(*(v9 - 8) + 32))(__dst, __src, v9);
        v10 = v7;
        v11 = 0;
LABEL_13:
        swift_storeEnumTagMultiPayload(__dst, v10, v11);
LABEL_15:
        swift_storeEnumTagMultiPayload(__dst, v5, 1);
        goto LABEL_16;
      }

      v24 = type metadata accessor for URL(0);
      v25 = *(*(v24 - 8) + 32);
      v25(__dst, __src, v24);
      v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (at: URL, annotationFile: URL));
      v25(&__dst[*(v14 + 48)], &__src[*(v14 + 48)], v24);
      v23 = 1;
    }

    v11 = v23;
    v10 = v7;
    goto LABEL_13;
  }

LABEL_16:
  v17 = a3[5];
  __dst[v17 + 8] = __src[v17 + 8];
  *&__dst[v17] = *&__src[v17];
  v18 = a3[6];
  *&__dst[v18] = *&__src[v18];
  __dst[v18 + 8] = __src[v18 + 8];
  *&__dst[a3[7]] = *&__src[a3[7]];
  *&__dst[a3[8]] = *&__src[a3[8]];
  __dst[a3[9]] = __src[a3[9]];
  v19 = a3[10];
  v20 = &__dst[v19];
  if (*&__dst[v19 + 24])
  {
    __swift_destroy_boxed_opaque_existential_1Tm(&__dst[v19]);
  }

  v21 = *&__src[v19];
  *(v20 + 1) = *&__src[v19 + 16];
  *v20 = v21;
  return __dst;
}

uint64_t sub_306A95(uint64_t a1, unsigned int a2, uint64_t a3)
{
  v4 = 0;
  v5 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
  if (*(*(v5 - 8) + 84) == a2)
  {
    return __swift_getEnumTagSinglePayload(a1, a2, v5);
  }

  v7 = -1;
  if (((*(a1 + *(a3 + 40) + 24) >> 1) - 1) >= 0)
  {
    v7 = (*(a1 + *(a3 + 40) + 24) >> 1) - 1;
  }

  v8 = v7 + 1;
  if ((*(a1 + *(a3 + 40) + 24) & 0xFFFFFFFF00000001) == 0)
  {
    return v8;
  }

  return v4;
}

uint64_t sub_306B1B(uint64_t a1, unsigned int a2, int a3, uint64_t a4)
{
  v6 = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(0);
  if (*(*(v6 - 8) + 84) == a3)
  {
    return __swift_storeEnumTagSinglePayload(a1, a2, a2, v6);
  }

  result = *(a4 + 40);
  *(a1 + result + 24) = 2 * a2;
  return result;
}

uint64_t type metadata completion function for MLObjectDetector.ModelParameters(uint64_t a1)
{
  result = type metadata accessor for MLObjectDetector.ModelParameters.ValidationData(319);
  if (v2 <= 0x3F)
  {
    v3[0] = *(result - 8) + 64;
    v3[1] = "\t";
    v3[2] = "\t";
    v3[3] = &value witness table for Builtin.Int64 + 64;
    v3[4] = &value witness table for Builtin.Int64 + 64;
    v3[5] = &unk_349958;
    v3[6] = &unk_349970;
    swift_initStructMetadata(a1, 256, 7, v3, a1 + 16);
    return 0;
  }

  return result;
}

void *static _AudioUtilities.validateAudioURLs(from:)(uint64_t a1)
{
  v1 = type metadata accessor for URL(0);
  v2 = *(v1 - 8);
  v3 = *(v2 + 64);
  v4 = alloca(v3);
  v5 = alloca(v3);
  v39 = &v31;
  v6 = alloca(v3);
  v7 = alloca(v3);
  v8 = &v31;
  if (!*(a1 + 16))
  {
    return _swiftEmptyArrayStorage;
  }

  v31 = *(a1 + 16);
  v36 = (*(v2 + 80) + 32) & ~*(v2 + 80);
  v9 = v36 + a1;
  v10 = *(v2 + 16);
  v41 = *(v2 + 72);
  v37 = "quence typed column. Column '" + 0x8000000000000000;
  v38 = a1;
  a1;
  v44 = _swiftEmptyArrayStorage;
  v40 = v2;
  v35 = &v31;
  v32 = v10;
  v34 = v1;
  do
  {
    v11 = v10(v8, v9, v1);
    if (static _AudioUtilities.validateOneAudioURL(from:)(v8, v11))
    {
      v10(v39, v8, v1);
      v12 = v44;
      if (!swift_isUniquelyReferenced_nonNull_native(v44))
      {
        v12 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, v12[2] + 1, 1, v12);
      }

      v13 = v12[2];
      if (v12[3] >> 1 <= v13)
      {
        v12 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(v12[3] >= 2uLL, v13 + 1, 1, v12);
      }

      v12[2] = v13 + 1;
      v44 = v12;
      v14 = v41;
      v15 = v12 + v36 + v41 * v13;
      v16 = v40;
      (*(v40 + 32))(v15, v39, v1);
      v17 = v35;
    }

    else
    {
      v42 = 0;
      v43 = 0xE000000000000000;
      _StringGuts.grow(_:)(35);
      v18 = v43;
      v43;
      v42 = 0xD000000000000020;
      v43 = v37;
      v19._countAndFlagsBits = URL.path.getter(v18);
      object = v19._object;
      String.append(_:)(v19);
      object;
      v21._countAndFlagsBits = 46;
      v21._object = 0xE100000000000000;
      String.append(_:)(v21);
      v22 = v42;
      v33 = v42;
      v17 = v8;
      v23 = v43;
      v45 = static os_log_type_t.info.getter();
      v24 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Any>);
      v25 = swift_allocObject(v24, 64, 7);
      v25[2] = 1;
      v25[3] = 2;
      v25[7] = &type metadata for String;
      v25[4] = v22;
      v25[5] = v23;
      v23;
      print(_:separator:terminator:)(v25, 32, 0xE100000000000000, 10, 0xE100000000000000);
      v25;
      type metadata accessor for OS_os_log();
      v26 = static OS_os_log.default.getter(0);
      v27 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<CVarArg>);
      v28 = swift_allocObject(v27, 72, 7);
      v28[2] = 1;
      v28[3] = 2;
      v28[7] = &type metadata for String;
      v28[8] = lazy protocol witness table accessor for type String and conformance String();
      v28[4] = v33;
      v28[5] = v23;
      v23;
      os_log(_:dso:log:type:_:)("%@\n", 3, 2, &dword_0, v26, v45, v28);
      v23;
      v21._countAndFlagsBits = v26;
      v1 = v34;

      v21._countAndFlagsBits = v28;
      v16 = v40;
      v21._countAndFlagsBits;
      v14 = v41;
    }

    (*(v16 + 8))(v17, v1);
    v9 += v14;
    v29 = v31-- == 1;
    v8 = v17;
    v10 = v32;
  }

  while (!v29);
  v38;
  return v44;
}

char static _AudioUtilities.validateOneAudioURL(from:)(uint64_t a1, double a2)
{
  v2 = type metadata accessor for URL(0);
  v3 = *(v2 - 8);
  v4 = *(v3 + 64);
  v5 = alloca(v4);
  v6 = alloca(v4);
  (*(v3 + 16))(&v22, a1, v2);
  objc_allocWithZone(AVAudioFile);
  v7 = @nonobjc AVAudioFile.init(forReading:)(&v22);
  v8 = [v7 fileFormat];
  v9 = v8;
  [v9 sampleRate];
  v25 = a2;

  if (v25 <= 0.0)
  {
    v23 = 0;
    v24 = 0xE000000000000000;
    _StringGuts.grow(_:)(64);
    v11 = *&v23;
    v12._object = "tted audio file " + 0x8000000000000000;
    v12._countAndFlagsBits = 0xD00000000000002BLL;
    String.append(_:)(v12);
    v13 = lazy protocol witness table accessor for type URL and conformance URL();
    v14 = dispatch thunk of CustomStringConvertible.description.getter(v2, v13);
    v16 = v15;
    v12._countAndFlagsBits = v14;
    v12._object = v15;
    String.append(_:)(v12);
    v16;
    v12._object = "s to be greater than zero, " + 0x8000000000000000;
    v12._countAndFlagsBits = 0xD000000000000010;
    String.append(_:)(v12);
    v17 = [v7 fileFormat];
    v18 = v17;
    [v18 sampleRate];
    v25 = v11;

    Double.write<A>(to:)(&v23, &type metadata for DefaultStringInterpolation, &protocol witness table for DefaultStringInterpolation);
    v12._countAndFlagsBits = 46;
    v12._object = 0xE100000000000000;
    String.append(_:)(v12);
    v19 = v23;
    v20 = v24;
    v21 = static os_log_type_t.default.getter(46);
    v12._countAndFlagsBits = v19;
    v12._object = v20;
    log(_:type:)(v12, v21);
    v20;

    return 0;
  }

  else
  {

    return 1;
  }
}

_BYTE *assignWithCopy for MLSupportVectorClassifier.ModelParameters.ValidationData(_BYTE *__dst, _BYTE *__src, uint64_t a3)
{
  if (__dst == __src)
  {
    return __dst;
  }

  outlined destroy of MLSupportVectorClassifier.ModelParameters.ValidationData(__dst);
  EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(__src, a3);
  if (EnumCaseMultiPayload == 2)
  {
    v7 = type metadata accessor for DataFrame(0);
    (*(*(v7 - 8) + 16))(__dst, __src, v7);
    swift_storeEnumTagMultiPayload(__dst, a3, 2);
    return __dst;
  }

  if (EnumCaseMultiPayload == 1)
  {
    v5 = *__src;
    v6 = __src[8];
    outlined copy of Result<_DataTable, Error>(*__src, v6);
    *__dst = v5;
    __dst[8] = v6;
    swift_storeEnumTagMultiPayload(__dst, a3, 1);
    return __dst;
  }

  return memcpy(__dst, __src, *(*(a3 - 8) + 64));
}

uint64_t type metadata accessor for MLSupportVectorClassifier.ModelParameters.ValidationData(uint64_t a1)
{
  result = type metadata singleton initialization cache for MLSupportVectorClassifier.ModelParameters.ValidationData;
  if (!type metadata singleton initialization cache for MLSupportVectorClassifier.ModelParameters.ValidationData)
  {
    return swift_getSingletonMetadata(a1, &nominal type descriptor for MLSupportVectorClassifier.ModelParameters.ValidationData);
  }

  return result;
}

void *assignWithTake for MLSupportVectorClassifier.ModelParameters.ValidationData(void *__dst, void *__src, uint64_t a3)
{
  if (__dst == __src)
  {
    return __dst;
  }

  outlined destroy of MLSupportVectorClassifier.ModelParameters.ValidationData(__dst);
  if (swift_getEnumCaseMultiPayload(__src, a3) == 2)
  {
    v4 = type metadata accessor for DataFrame(0);
    (*(*(v4 - 8) + 32))(__dst, __src, v4);
    swift_storeEnumTagMultiPayload(__dst, a3, 2);
    return __dst;
  }

  return memcpy(__dst, __src, *(*(a3 - 8) + 64));
}

uint64_t type metadata completion function for MLSupportVectorClassifier.ModelParameters.ValidationData(uint64_t a1)
{
  v5[0] = &unk_349998;
  v5[1] = &unk_3499B0;
  result = type metadata accessor for DataFrame(319);
  if (v4 <= 0x3F)
  {
    v5[2] = *(result - 8) + 64;
    swift_initEnumMetadataMultiPayload(a1, 256, 3, v5, v2, v3);
    return 0;
  }

  return result;
}

uint64_t MLSupportVectorClassifier.ModelParameters.ValidationData.asTable()(__m128 a1)
{
  v3 = v1;
  v4 = type metadata accessor for DataFrame(0);
  v27 = *(v4 - 8);
  v5 = *(v27 + 64);
  v6 = alloca(v5);
  v7 = alloca(v5);
  v29 = &v25;
  v8 = alloca(v5);
  v9 = alloca(v5);
  v28 = &v25;
  v10 = type metadata accessor for MLSupportVectorClassifier.ModelParameters.ValidationData(0);
  v11 = *(*(v10 - 8) + 64);
  v12 = alloca(v11);
  v13 = alloca(v11);
  outlined init with copy of MLSupportVectorClassifier.ModelParameters.ValidationData(v2, &v25);
  result = swift_getEnumCaseMultiPayload(&v25, v10);
  switch(result)
  {
    case 0:
      *v3 = 0;
      *(v3 + 8) = -1;
      break;
    case 1:
      result = v25;
      v15 = v26;
      goto LABEL_7;
    case 2:
      v16 = v28;
      v17 = v27;
      (*(v27 + 32))(v28, &v25, v4);
      v18 = v29;
      *a1.i64 = (*(v17 + 16))(v29, v16, v4);
      MLDataTable.init(_:convertArraysToShapedArrays:)(v18, 1, a1);
      (*(v17 + 8))(v16, v4);
      result = v30;
      v15 = v31;
LABEL_7:
      *v3 = result;
      *(v3 + 8) = v15;
      break;
    case 3:
      v19 = v3;
      empty = tc_v1_sframe_create_empty(0);
      if (!empty)
      {
        BUG();
      }

      v21 = empty;
      v22 = type metadata accessor for CMLTable();
      v23 = swift_allocObject(v22, 24, 7);
      *(v23 + 16) = v21;
      v24 = type metadata accessor for _DataTable();
      swift_allocObject(v24, 40, 7);
      result = _DataTable.init(impl:)(v23);
      *v19 = result;
      *(v19 + 8) = 0;
      break;
  }

  return result;
}

uint64_t MLSupportVectorClassifier.ModelParameters.ValidationData.generateDataFrames(trainingData:)(uint64_t a1, uint64_t a2, void (*a3)(uint64_t *, uint64_t, uint64_t))
{
  v56 = a3;
  v55 = a2;
  v54 = a1;
  v4 = type metadata accessor for DataFrame(0);
  v57 = *(v4 - 8);
  v5 = *(v57 + 64);
  v6 = alloca(v5);
  v7 = alloca(v5);
  v53 = &v44;
  v47 = type metadata accessor for DataFrame.Slice(0);
  v46 = *(v47 - 8);
  v8 = *(v46 + 64);
  v9 = alloca(v8);
  v10 = alloca(v8);
  v49 = &v44;
  v11 = alloca(v8);
  v12 = alloca(v8);
  v52 = &v44;
  v13 = alloca(v8);
  v14 = alloca(v8);
  v50 = &v44;
  v15 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for DataFrame.Slice?) - 8) + 64);
  v16 = alloca(v15);
  v17 = alloca(v15);
  v48 = &v44;
  v18 = alloca(v15);
  v19 = alloca(v15);
  v51 = &v44;
  v20 = type metadata accessor for MLSupportVectorClassifier.ModelParameters.ValidationData(0);
  v21 = *(*(v20 - 8) + 64);
  v22 = alloca(v21);
  v23 = alloca(v21);
  outlined init with copy of MLSupportVectorClassifier.ModelParameters.ValidationData(v3, &v44);
  switch(swift_getEnumCaseMultiPayload(&v44, v20))
  {
    case 0u:
      v57 = v4;
      v24 = v51;
      v25 = v50;
      DataFrame.randomSplit(strategy:)(v51, v50, &v44);
      v26 = v46;
      v27 = v52;
      v28 = v25;
      v29 = v47;
      v56 = *(v46 + 16);
      v56(v52, v28, v47);
      DataFrame.init(_:)(v27);
      v30 = v48;
      outlined init with copy of DataFrame.Slice?(v24, v48);
      v31 = v29;
      if (__swift_getEnumTagSinglePayload(v30, 1, v29) == 1)
      {
        __swift_storeEnumTagSinglePayload(v55, 1, 1, v57);
        (*(v26 + 8))(v50, v29);
      }

      else
      {
        v41 = v52;
        (*(v26 + 32))(v52, v30, v31);
        v42 = v49;
        v56(v49, v41, v31);
        DataFrame.init(_:)(v42);
        v43 = *(v26 + 8);
        v43(v41, v31);
        __swift_storeEnumTagSinglePayload(v55, 0, 1, v57);
        v43(v50, v31);
      }

      return outlined destroy of DataFrame.Slice?(v51);
    case 1u:
      v36 = v44;
      v37 = v45;
      (*(v57 + 16))(v54, v56, v4);
      v44 = v36;
      v45 = v37;
      v38 = v55;
      DataFrame.init(_:)(&v44);
      v34 = v38;
      goto LABEL_10;
    case 2u:
      v32 = *(v57 + 32);
      v32(v53, &v44, v4);
      if (DataFrameProtocol.isEmpty.getter(v4, &protocol witness table for DataFrame))
      {
        v33 = v57;
        (*(v57 + 8))(v53, v4);
        (*(v33 + 16))(v54, v56, v4);
LABEL_7:
        v34 = v55;
        v35 = 1;
      }

      else
      {
        (*(v57 + 16))(v54, v56, v4);
        v39 = v55;
        v32(v55, v53, v4);
        v34 = v39;
LABEL_10:
        v35 = 0;
      }

      return __swift_storeEnumTagSinglePayload(v34, v35, 1, v4);
    case 3u:
      (*(v57 + 16))(v54, v56, v4);
      goto LABEL_7;
  }
}

uint64_t specialized MLImageClassifier.Classifier.fitted<A>(to:eventHandler:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v5[6] = v4;
  v5[5] = a4;
  v5[4] = a3;
  v5[3] = a1;
  v6 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FullyConnectedNetworkClassifierModel<Float, String>);
  v5[7] = v6;
  v7 = *(v6 - 8);
  v5[8] = v7;
  v5[9] = swift_task_alloc((*(v7 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FullyConnectedNetworkClassifier<Float, String>);
  v5[10] = v8;
  v9 = *(v8 - 8);
  v5[11] = v9;
  v5[12] = swift_task_alloc((*(v9 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v10 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for LogisticRegressionClassifierModel<Float, String>);
  v5[13] = v10;
  v11 = *(v10 - 8);
  v5[14] = v11;
  v5[15] = swift_task_alloc((*(v11 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for LogisticRegressionClassifier<Float, String>);
  v5[16] = v12;
  v13 = *(v12 - 8);
  v5[17] = v13;
  v5[18] = swift_task_alloc((*(v13 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>);
  v5[19] = v14;
  v5[20] = swift_task_alloc((*(*(v14 - 8) + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v5[2] = a2;
  return swift_task_switch(specialized MLImageClassifier.Classifier.fitted<A>(to:eventHandler:), 0, 0);
}

uint64_t specialized MLImageClassifier.Classifier.fitted<A>(to:eventHandler:)()
{
  v14 = v0 + 2;
  v1 = v0[20];
  v2 = v0[19];
  outlined init with copy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v0[6], v1, &demangling cache variable for type metadata for Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>);
  EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(v1, v2);
  v4 = v0[20];
  if (EnumCaseMultiPayload == 1)
  {
    (*(v0[11] + 32))(v0[12], v4, v0[10]);
    v5 = swift_task_alloc(async function pointer to FullyConnectedNetworkClassifier.fitted<A>(to:eventHandler:)[1]);
    v0[23] = v5;
    v6 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [AnnotatedFeature<MLShapedArray<Float>, String>]);
    v7 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type [AnnotatedFeature<MLShapedArray<Float>, String>] and conformance [A], &demangling cache variable for type metadata for [AnnotatedFeature<MLShapedArray<Float>, String>], &protocol conformance descriptor for [A]);
    *v5 = v0;
    v5[1] = specialized MLImageClassifier.Classifier.fitted<A>(to:eventHandler:);
    v8 = v0[12];
    return FullyConnectedNetworkClassifier.fitted<A>(to:eventHandler:)(v0[9], v14, v0[4], v0[5], v0[10], v6, v7);
  }

  else
  {
    (*(v0[17] + 32))(v0[18], v4, v0[16]);
    v10 = swift_task_alloc(async function pointer to LogisticRegressionClassifier.fitted<A>(to:eventHandler:)[1]);
    v0[21] = v10;
    v11 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [AnnotatedFeature<MLShapedArray<Float>, String>]);
    v12 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type [AnnotatedFeature<MLShapedArray<Float>, String>] and conformance [A], &demangling cache variable for type metadata for [AnnotatedFeature<MLShapedArray<Float>, String>], &protocol conformance descriptor for [A]);
    *v10 = v0;
    v10[1] = specialized MLImageClassifier.Classifier.fitted<A>(to:eventHandler:);
    v13 = v0[18];
    return LogisticRegressionClassifier.fitted<A>(to:eventHandler:)(v0[15], v14, v0[4], v0[5], v0[16], v11, v12);
  }
}

{
  v2 = *(*v1 + 168);
  *(*v1 + 176) = v0;
  v2;
  if (v0)
  {
    v3 = specialized MLSoundClassifier.Classifier.fitted<A>(to:eventHandler:);
  }

  else
  {
    v3 = specialized MLImageClassifier.Classifier.fitted<A>(to:eventHandler:);
  }

  return swift_task_switch(v3, 0, 0);
}

{
  v2 = *(*v1 + 184);
  *(*v1 + 192) = v0;
  v2;
  if (v0)
  {
    v3 = specialized MLSoundClassifier.Classifier.fitted<A>(to:eventHandler:);
  }

  else
  {
    v3 = specialized MLImageClassifier.Classifier.fitted<A>(to:eventHandler:);
  }

  return swift_task_switch(v3, 0, 0);
}

{
  v10 = *(v0 + 144);
  v1 = *(v0 + 120);
  v2 = *(v0 + 112);
  v3 = *(v0 + 104);
  v9 = *(v0 + 160);
  v8 = *(v0 + 96);
  v4 = *(v0 + 24);
  v7 = *(v0 + 72);
  (*(*(v0 + 136) + 8))(v10, *(v0 + 128));
  (*(v2 + 32))(v4, v1, v3);
  v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Either<LogisticRegressionClassifierModel<Float, String>, FullyConnectedNetworkClassifierModel<Float, String>>);
  swift_storeEnumTagMultiPayload(v4, v5, 0);
  v9;
  v10;
  v1;
  v8;
  v7;
  return (*(v0 + 8))();
}

{
  v7 = *(v0 + 96);
  v1 = *(v0 + 72);
  v2 = *(v0 + 64);
  v3 = *(v0 + 24);
  v4 = *(v0 + 56);
  v10 = *(v0 + 160);
  v9 = *(v0 + 144);
  v8 = *(v0 + 120);
  (*(*(v0 + 88) + 8))(v7, *(v0 + 80));
  (*(v2 + 32))(v3, v1, v4);
  v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Either<LogisticRegressionClassifierModel<Float, String>, FullyConnectedNetworkClassifierModel<Float, String>>);
  swift_storeEnumTagMultiPayload(v3, v5, 1);
  v10;
  v9;
  v8;
  v7;
  v1;
  return (*(v0 + 8))();
}

uint64_t specialized MLImageClassifier.Classifier.fitted<A, B>(to:validateOn:eventHandler:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  v6[7] = v5;
  v6[6] = a5;
  v6[5] = a4;
  v6[4] = a1;
  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FullyConnectedNetworkClassifierModel<Float, String>);
  v6[8] = v8;
  v9 = *(v8 - 8);
  v6[9] = v9;
  v6[10] = swift_task_alloc((*(v9 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v10 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FullyConnectedNetworkClassifier<Float, String>);
  v6[11] = v10;
  v11 = *(v10 - 8);
  v6[12] = v11;
  v6[13] = swift_task_alloc((*(v11 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for LogisticRegressionClassifierModel<Float, String>);
  v6[14] = v12;
  v13 = *(v12 - 8);
  v6[15] = v13;
  v6[16] = swift_task_alloc((*(v13 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for LogisticRegressionClassifier<Float, String>);
  v6[17] = v14;
  v15 = *(v14 - 8);
  v6[18] = v15;
  v6[19] = swift_task_alloc((*(v15 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>);
  v6[20] = v16;
  v6[21] = swift_task_alloc((*(*(v16 - 8) + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v6[2] = a2;
  v6[3] = a3;
  return swift_task_switch(specialized MLImageClassifier.Classifier.fitted<A, B>(to:validateOn:eventHandler:), 0, 0);
}

uint64_t specialized MLImageClassifier.Classifier.fitted<A, B>(to:validateOn:eventHandler:)()
{
  v13 = v0 + 2;
  v12 = v0 + 3;
  v1 = v0[21];
  v2 = v0[20];
  outlined init with copy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v0[7], v1, &demangling cache variable for type metadata for Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>);
  EnumCaseMultiPayload = swift_getEnumCaseMultiPayload(v1, v2);
  v4 = v0[21];
  if (EnumCaseMultiPayload == 1)
  {
    (*(v0[12] + 32))(v0[13], v4, v0[11]);
    v5 = swift_task_alloc(async function pointer to FullyConnectedNetworkClassifier.fitted<A, B>(to:validateOn:eventHandler:)[1]);
    v0[24] = v5;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [AnnotatedFeature<MLShapedArray<Float>, String>]);
    v6 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type [AnnotatedFeature<MLShapedArray<Float>, String>] and conformance [A], &demangling cache variable for type metadata for [AnnotatedFeature<MLShapedArray<Float>, String>], &protocol conformance descriptor for [A]);
    *v5 = v0;
    v5[1] = specialized MLImageClassifier.Classifier.fitted<A, B>(to:validateOn:eventHandler:);
    v7 = v0[13];
    retaddr = v6;
    return FullyConnectedNetworkClassifier.fitted<A, B>(to:validateOn:eventHandler:)(v0[10], v13, v12, v0[5], v0[6], v0[11]);
  }

  else
  {
    (*(v0[18] + 32))(v0[19], v4, v0[17]);
    v9 = swift_task_alloc(async function pointer to LogisticRegressionClassifier.fitted<A, B>(to:validateOn:eventHandler:)[1]);
    v0[22] = v9;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [AnnotatedFeature<MLShapedArray<Float>, String>]);
    v10 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type [AnnotatedFeature<MLShapedArray<Float>, String>] and conformance [A], &demangling cache variable for type metadata for [AnnotatedFeature<MLShapedArray<Float>, String>], &protocol conformance descriptor for [A]);
    *v9 = v0;
    v9[1] = specialized MLImageClassifier.Classifier.fitted<A, B>(to:validateOn:eventHandler:);
    v11 = v0[19];
    retaddr = v10;
    return LogisticRegressionClassifier.fitted<A, B>(to:validateOn:eventHandler:)(v0[16], v13, v12, v0[5], v0[6], v0[17]);
  }
}

{
  v2 = *(*v1 + 176);
  *(*v1 + 184) = v0;
  v2;
  if (v0)
  {
    v3 = specialized MLSoundClassifier.Classifier.fitted<A, B>(to:validateOn:eventHandler:);
  }

  else
  {
    v3 = specialized MLImageClassifier.Classifier.fitted<A, B>(to:validateOn:eventHandler:);
  }

  return swift_task_switch(v3, 0, 0);
}

{
  v2 = *(*v1 + 192);
  *(*v1 + 200) = v0;
  v2;
  if (v0)
  {
    v3 = specialized MLSoundClassifier.Classifier.fitted<A, B>(to:validateOn:eventHandler:);
  }

  else
  {
    v3 = specialized MLImageClassifier.Classifier.fitted<A, B>(to:validateOn:eventHandler:);
  }

  return swift_task_switch(v3, 0, 0);
}

{
  v10 = *(v0 + 152);
  v1 = *(v0 + 128);
  v2 = *(v0 + 120);
  v3 = *(v0 + 112);
  v9 = *(v0 + 168);
  v8 = *(v0 + 104);
  v4 = *(v0 + 32);
  v7 = *(v0 + 80);
  (*(*(v0 + 144) + 8))(v10, *(v0 + 136));
  (*(v2 + 32))(v4, v1, v3);
  v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Either<LogisticRegressionClassifierModel<Float, String>, FullyConnectedNetworkClassifierModel<Float, String>>);
  swift_storeEnumTagMultiPayload(v4, v5, 0);
  v9;
  v10;
  v1;
  v8;
  v7;
  return (*(v0 + 8))();
}

{
  v7 = *(v0 + 104);
  v1 = *(v0 + 80);
  v2 = *(v0 + 72);
  v3 = *(v0 + 32);
  v4 = *(v0 + 64);
  v10 = *(v0 + 168);
  v9 = *(v0 + 152);
  v8 = *(v0 + 128);
  (*(*(v0 + 96) + 8))(v7, *(v0 + 88));
  (*(v2 + 32))(v3, v1, v4);
  v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Either<LogisticRegressionClassifierModel<Float, String>, FullyConnectedNetworkClassifierModel<Float, String>>);
  swift_storeEnumTagMultiPayload(v3, v5, 1);
  v10;
  v9;
  v8;
  v7;
  v1;
  return (*(v0 + 8))();
}

NSURL *_s8CreateML21AnnotatedFeatureStoreVyACxcSTRz0A12MLComponents0cD0Vy04CoreB013MLShapedArrayVySfGSSG7ElementRtzlufCSayAKG_Tt0g5(uint64_t a1)
{
  v2 = v1;
  v47 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLShapedArray<Float>);
  v48 = *(v47 - 8);
  v3 = *(v48 + 64);
  v4 = alloca(v3);
  v5 = alloca(v3);
  v49 = &v42;
  v6 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<MLShapedArray<Float>, String>);
  v7 = *(v6 - 8);
  v8 = *(v7 + 64);
  v9 = alloca(v8);
  v10 = alloca(v8);
  v60 = &v42;
  BlobsFile.init()();
  qmemcpy(v72, v73, 0x58uLL);
  v72[11] = a1;
  v50 = *(a1 + 16);
  if (v50)
  {
    v57 = v2;
    v11 = 0;
    v61 = HIDWORD(v73[2]);
    v42 = *&v73[3];
    v43 = *&v73[5];
    v44 = *&v73[7];
    v53 = v73[9];
    v64 = v73[10];
    v12 = a1 + ((*(v7 + 80) + 32) & ~*(v7 + 80));
    v51 = *(v7 + 16);
    v56 = *(v7 + 72);
    v63 = v73[2];
    v52 = ~LODWORD(v73[2]);
    v58 = a1;
    swift_bridgeObjectRetain_n(a1, 2);
    v13 = v12;
    v62 = 0;
    v54 = v6;
    v14 = v60;
    v55 = v7;
    while (2)
    {
      v45 = v13;
      v51(v14, v13, v6);
      v15 = v49;
      AnnotatedFeature.feature.getter(v6);
      v16 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type MLShapedArray<Float> and conformance MLShapedArray<A>, &demangling cache variable for type metadata for MLShapedArray<Float>, &protocol conformance descriptor for MLShapedArray<A>);
      v17 = v47;
      v18 = MLShapedArrayProtocol.scalars.getter(v47, v16);
      (*(v48 + 8))(v15, v17);
      v59 = v18;
      v19 = *(v18 + 16);
      if (v19 > 0x1FFFFFFFFFFFFFFFLL)
      {
        BUG();
      }

      v20 = (4 * v19 - 1) / 64;
      if (((v20 - 0x1FFFFFFFFFFFFFFLL) >> 58) < 0x3F)
      {
        BUG();
      }

      switch(v72[1] >> 62)
      {
        case 0:
          v21 = BYTE6(v72[1]);
          break;
        case 1:
          if (__OFSUB__(HIDWORD(v72[0]), v72[0]))
          {
            BUG();
          }

          v21 = HIDWORD(v72[0]) - LODWORD(v72[0]);
          break;
        case 2:
          v22 = *(v72[0] + 24);
          v23 = __OFSUB__(v22, *(v72[0] + 16));
          v21 = v22 - *(v72[0] + 16);
          if (v23)
          {
            BUG();
          }

          return result;
        case 3:
          v21 = 0;
          break;
      }

      v23 = __OFADD__(64, v21);
      v24 = v21 + 64 < 0;
      v25 = v21 + 64;
      if (v23)
      {
        BUG();
      }

      if (v24)
      {
        BUG();
      }

      v26 = (v20 << 6) + 64;
      if (v26 < 0)
      {
        BUG();
      }

      if (!swift_isUniquelyReferenced_nonNull_native(v64))
      {
        v64 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(v64 + 2) + 1, 1, v64);
      }

      v27 = *(v64 + 2);
      if (*(v64 + 3) >> 1 <= v27)
      {
        v64 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(*(v64 + 3) >= 2uLL, v27 + 1, 1, v64);
      }

      v28 = v64;
      *(v64 + 2) = v27 + 1;
      v29 = v27 << 6;
      *&v28[v29 + 32] = 0x2DEADBEEFLL;
      *&v28[v29 + 40] = v26;
      *&v28[v29 + 48] = v25;
      *&v28[v29 + 88] = 0;
      *&v28[v29 + 56] = 0;
      *&v28[v29 + 72] = 0;
      if (v52 == v62)
      {
        BUG();
      }

      v65 = __PAIR64__(v61, ++v63);
      v66 = v42;
      v67 = v43;
      v68 = v44;
      v69 = v53;
      switch(v72[1] >> 62)
      {
        case 0:
          v30 = BYTE6(v72[1]);
          goto LABEL_24;
        case 1:
          JUMPOUT(0x30879CLL);
        case 2:
          v31 = *(v72[0] + 24);
          v23 = __OFSUB__(v31, *(v72[0] + 16));
          v30 = v31 - *(v72[0] + 16);
          if (v23)
          {
            BUG();
          }

LABEL_24:
          if (v30 < 64)
          {
            goto LABEL_25;
          }

          goto LABEL_26;
        case 3:
LABEL_25:
          specialized Data.append<A>(contentsOf:)(&stru_20.vmsize, 0);
LABEL_26:
          specialized Data._Representation.withUnsafeMutableBytes<A>(_:)(v72, &v65);
          v46 = v11;
          v71[0] = 0x2DEADBEEFLL;
          v71[1] = v26;
          v71[2] = v25;
          memset(&v71[3], 0, 40);
          v70[3] = &type metadata for UnsafeRawBufferPointer;
          v70[4] = &protocol witness table for UnsafeRawBufferPointer;
          v70[0] = v71;
          v70[1] = &v71[8];
          v32 = __swift_project_boxed_opaque_existential_0Tm(v70, &type metadata for UnsafeRawBufferPointer);
          Data._Representation.append(contentsOf:)(*v32, v32[1]);
          __swift_destroy_boxed_opaque_existential_1Tm(v70);
          v33 = v59;
          v34 = *(v59 + 16);
          if (v34 > 0x1FFFFFFFFFFFFFFFLL)
          {
            BUG();
          }

          v35 = 4 * v34;
          v71[3] = &type metadata for UnsafeRawBufferPointer;
          v71[4] = &protocol witness table for UnsafeRawBufferPointer;
          v71[0] = v59 + 32;
          v71[1] = v59 + v35 + 32;
          v36 = __swift_project_boxed_opaque_existential_0Tm(v71, &type metadata for UnsafeRawBufferPointer);
          Data._Representation.append(contentsOf:)(*v36, v36[1]);
          __swift_destroy_boxed_opaque_existential_1Tm(v71);
          v38 = v26 <= v35;
          v37 = (v26 - v35) < 0;
          v39 = (v26 - v35);
          v14 = v60;
          v11 = v46;
          if (!v38)
          {
            if (v37)
            {
              BUG();
            }

            specialized Data.append<A>(contentsOf:)(v39, 0);
            v33 = v59;
          }

          v40 = v62 + 1;
          v33;
          v6 = v54;
          (*(v55 + 8))(v14, v54);
          v13 = v56 + v45;
          v62 = v40;
          if (v50 != v40)
          {
            continue;
          }

          swift_bridgeObjectRelease_n(v58, 2);
          v72[10] = v64;
          LODWORD(v72[2]) = v63;
          v2 = v57;
          break;
      }

      break;
    }
  }

  qmemcpy(v70, v72, sizeof(v70));
  qmemcpy(v71, v72, sizeof(v71));
  outlined retain of AnnotatedFeatureStore(v70);
  outlined release of AnnotatedFeatureStore(v71);
  qmemcpy(v2, v70, 0x60uLL);
  return __stack_chk_guard;
}

uint64_t _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5(uint64_t a1, uint64_t a2, uint64_t a3)
{
  v3[39] = a3;
  v3[38] = a2;
  v3[37] = a1;
  v3[35] = a1;
  return swift_task_switch(_s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TY0_, 0, 0);
}

uint64_t _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TY0_()
{
  v1 = *(v0 + 304);
  v2 = *(v0 + 296);
  if (*(v0 + 312) == 0 || v1 == 0)
  {
    *(v0 + 296);
    return (*(v0 + 8))(v2);
  }

  else
  {
    v32 = *(v0 + 312);
    v4 = swift_allocEmptyBox();
    *(v0 + 320) = v4;
    v33 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Augmenter<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0, SystemRandomNumberGenerator>);
    *(v0 + 328) = v33;
    v5 = *(v33 - 8);
    *(v0 + 336) = v5;
    *(v0 + 344) = swift_task_alloc((*(v5 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
    v34 = swift_allocObject(&unk_396030, 32, 7);
    *(v34 + 16) = v1;
    *(v34 + 24) = v4;
    v2;

    v35 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0);
    v6 = type metadata accessor for OS_os_log(0, &lazy cache variable for type metadata for CIImage, CIImage_ptr);
    v36 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0);
    v37 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>);
    v38 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0);
    v39 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>);
    v40 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0);
    v41 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>);
    v42 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0);
    v43 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>);
    v44 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0);
    v7 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>);
    v45 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0);
    v8 = __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>);
    v9 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0> and conformance ApplyRandomly<A>, &demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>, &protocol conformance descriptor for ApplyRandomly<A>);
    *(v0 + 256) = v6;
    *(v0 + 264) = v8;
    *(v0 + 272) = v9;
    OpaqueTypeConformance2 = swift_getOpaqueTypeConformance2(v0 + 256, &opaque type descriptor for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>, 1);
    v11 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0> and conformance ApplyRandomly<A>, &demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>, &protocol conformance descriptor for ApplyRandomly<A>);
    *(v0 + 16) = v6;
    *(v0 + 24) = v45;
    *(v0 + 32) = v7;
    *(v0 + 40) = OpaqueTypeConformance2;
    *(v0 + 48) = v11;
    v12 = swift_getOpaqueTypeConformance2(v0 + 16, &opaque type descriptor for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>, 1);
    *(v0 + 56) = v6;
    *(v0 + 64) = v44;
    *(v0 + 72) = v7;
    *(v0 + 80) = v12;
    *(v0 + 88) = v11;
    v13 = swift_getOpaqueTypeConformance2(v0 + 56, &opaque type descriptor for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>, 1);
    v14 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0> and conformance ApplyRandomly<A>, &demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>, &protocol conformance descriptor for ApplyRandomly<A>);
    *(v0 + 96) = v6;
    *(v0 + 104) = v42;
    *(v0 + 112) = v43;
    *(v0 + 120) = v13;
    *(v0 + 128) = v14;
    v15 = swift_getOpaqueTypeConformance2(v0 + 96, &opaque type descriptor for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>, 1);
    v16 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0> and conformance ApplyRandomly<A>, &demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>, &protocol conformance descriptor for ApplyRandomly<A>);
    *(v0 + 136) = v6;
    *(v0 + 144) = v40;
    *(v0 + 152) = v41;
    *(v0 + 160) = v15;
    *(v0 + 168) = v16;
    v17 = swift_getOpaqueTypeConformance2(v0 + 136, &opaque type descriptor for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>, 1);
    v18 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0> and conformance ApplyRandomly<A>, &demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>, &protocol conformance descriptor for ApplyRandomly<A>);
    *(v0 + 176) = v6;
    *(v0 + 184) = v38;
    *(v0 + 192) = v39;
    *(v0 + 200) = v17;
    *(v0 + 208) = v18;
    v19 = swift_getOpaqueTypeConformance2(v0 + 176, &opaque type descriptor for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>, 1);
    v20 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0> and conformance ApplyRandomly<A>, &demangling cache variable for type metadata for ApplyRandomly<<<opaque return type of static AugmentationBuilder.buildPartialBlock<A>(first:)>>.0>, &protocol conformance descriptor for ApplyRandomly<A>);
    *(v0 + 216) = v6;
    *(v0 + 224) = v36;
    *(v0 + 232) = v37;
    *(v0 + 240) = v19;
    *(v0 + 248) = v20;
    v21 = swift_getOpaqueTypeConformance2(v0 + 216, &opaque type descriptor for <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>, 1);
    Augmenter.init<A>(generator:_:)(v21, partial apply for closure #1 in static MLImageClassifier.applyAugmentations<A>(to:augmentationOptions:upsampleFactor:), v34, v35, &type metadata for SystemRandomNumberGenerator, v6, v21, &protocol witness table for SystemRandomNumberGenerator);
    v22 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UpsampledAugmentationSequence<[AnnotatedFeature<CIImage, String>], <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0, SystemRandomNumberGenerator, String>.AsyncIterator);
    *(v0 + 352) = v22;
    v23 = *(v22 - 8);
    *(v0 + 360) = v23;
    *(v0 + 368) = swift_task_alloc((*(v23 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
    v24 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UpsampledAugmentationSequence<[AnnotatedFeature<CIImage, String>], <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0, SystemRandomNumberGenerator, String>);
    v46 = *(v24 - 8);
    v25 = swift_task_alloc((*(v46 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
    v26 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [AnnotatedFeature<CIImage, String>]);
    v27 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type [AnnotatedFeature<CIImage, String>] and conformance [A], &demangling cache variable for type metadata for [AnnotatedFeature<CIImage, String>], &protocol conformance descriptor for [A]);
    Augmenter.applied<A, B>(to:upsampledBy:)(v0 + 280, v32, v33, v26, &type metadata for String, v27, &protocol witness table for String);
    UpsampledAugmentationSequence.makeAsyncIterator()(v24);
    (*(v46 + 8))(v25, v24);
    v25;
    v28 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<CIImage, String>?);
    *(v0 + 376) = swift_task_alloc((*(*(v28 - 8) + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
    *(v0 + 384) = *(v0 + 296);
    v29 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type UpsampledAugmentationSequence<[AnnotatedFeature<CIImage, String>], <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0, SystemRandomNumberGenerator, String>.AsyncIterator and conformance UpsampledAugmentationSequence<A, B, C, D>.AsyncIterator, &demangling cache variable for type metadata for UpsampledAugmentationSequence<[AnnotatedFeature<CIImage, String>], <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0, SystemRandomNumberGenerator, String>.AsyncIterator, &protocol conformance descriptor for UpsampledAugmentationSequence<A, B, C, D>.AsyncIterator);
    v30 = swift_task_alloc(async function pointer to dispatch thunk of AsyncIteratorProtocol.next()[1]);
    *(v0 + 392) = v30;
    *v30 = v0;
    v30[1] = _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TQ1_;
    v31 = *(v0 + 368);
    return dispatch thunk of AsyncIteratorProtocol.next()(*(v0 + 376), *(v0 + 352), v29);
  }
}

uint64_t _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TQ1_()
{
  v2 = *(*v1 + 392);
  *(*v1 + 400) = v0;
  v2;
  if (v0)
  {
    v3 = _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TY3_;
  }

  else
  {
    v3 = _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TY2_;
  }

  return swift_task_switch(v3, 0, 0);
}

uint64_t _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TY2_()
{
  v1 = *(v0 + 376);
  v2 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<CIImage, String>);
  if (__swift_getEnumTagSinglePayload(v1, 1, v2) == 1)
  {
    v3 = *(v0 + 368);
    v4 = *(v0 + 344);
    v17 = *(v0 + 336);
    v15 = *(v0 + 320);
    v5 = *(v0 + 328);
    (*(*(v0 + 360) + 8))(v3, *(v0 + 352));
    (*(v17 + 8))(v4, v5);

    v1;
    v3;
    v4;
    return (*(v0 + 8))(*(v0 + 384));
  }

  else
  {
    v7 = *(v0 + 384);
    v16 = *(v2 - 8);
    v18 = swift_task_alloc((*(v16 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
    v14 = *(v16 + 32);
    v14(v18, v1, v2);
    isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native(v7);
    v9 = *(v0 + 384);
    if (!isUniquelyReferenced_nonNull_native)
    {
      v9 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, v9[2] + 1, 1, *(v0 + 384));
    }

    v10 = v9[2];
    if (v9[3] >> 1 <= v10)
    {
      v9 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(v9[3] >= 2uLL, v10 + 1, 1, v9);
    }

    v9[2] = v10 + 1;
    v14(v9 + ((*(v16 + 80) + 32) & ~*(v16 + 80)) + *(v16 + 72) * v10, v18, v2);
    v18;
    *(v0 + 384) = v9;
    v11 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type UpsampledAugmentationSequence<[AnnotatedFeature<CIImage, String>], <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0, SystemRandomNumberGenerator, String>.AsyncIterator and conformance UpsampledAugmentationSequence<A, B, C, D>.AsyncIterator, &demangling cache variable for type metadata for UpsampledAugmentationSequence<[AnnotatedFeature<CIImage, String>], <<opaque return type of static AugmentationBuilder.buildPartialBlock<A, B>(accumulated:next:)>>.0, SystemRandomNumberGenerator, String>.AsyncIterator, &protocol conformance descriptor for UpsampledAugmentationSequence<A, B, C, D>.AsyncIterator);
    v12 = swift_task_alloc(async function pointer to dispatch thunk of AsyncIteratorProtocol.next()[1]);
    *(v0 + 392) = v12;
    *v12 = v0;
    v12[1] = _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TQ1_;
    v13 = *(v0 + 368);
    return dispatch thunk of AsyncIteratorProtocol.next()(*(v0 + 376), *(v0 + 352), v11);
  }
}

uint64_t _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TY3_()
{
  v0[36] = v0[50];
  if (_stdlib_isOSVersionAtLeastOrVariantVersionAtLeast(_:_:_:_:_:_:)(0xFuLL, 0, 0, 0x12uLL, 0, 0))
  {
    v1 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Error);
    swift_willThrowTypedImpl(v0 + 36, v1, &protocol self-conformance witness table for Error);
  }

  v0[48];
  return swift_task_switch(_s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TY4_, 0, 0);
}

uint64_t _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5TY4_()
{
  v7 = *(v0 + 376);
  v1 = *(v0 + 368);
  v2 = *(v0 + 344);
  v3 = *(v0 + 336);
  v8 = *(v0 + 320);
  v4 = *(v0 + 328);
  (*(*(v0 + 360) + 8))(v1, *(v0 + 352));
  (*(v3 + 8))(v2, v4);
  v8;
  v7;
  v1;
  v2;
  v5 = *(v0 + 400);
  return (*(v0 + 8))();
}

void *specialized Collection.randomSplit<A, B>(strategy:)(uint64_t a1, uint64_t a2, __int16 a3, uint64_t a4)
{
  if ((a3 & 0x100) != 0)
  {
    v6 = *(a4 + 16);
    if (v6 < 0x32)
    {
LABEL_7:
      a4;
      return _swiftEmptyArrayStorage;
    }

    v5 = dbl_33FA70[v6 < 0xC8];
    a2 = 1;
  }

  else
  {
    if (a3)
    {
      a2 = 1;
    }

    v5 = *&a1;
    if (*&a1 == 0.0)
    {
      goto LABEL_7;
    }
  }

  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [AnnotatedFeature<MLShapedArray<Float>, String>]);
  v9 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLShapedArray<Float>);
  v10 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type [AnnotatedFeature<MLShapedArray<Float>, String>] and conformance [A], &demangling cache variable for type metadata for [AnnotatedFeature<MLShapedArray<Float>, String>], &protocol conformance descriptor for [A]);
  return Sequence.randomSplit<A, B>(by:seed:)(a2, 0, v8, v9, &type metadata for String, v10, v5, &protocol witness table for String, a4);
}

{
  if ((a3 & 0x100) != 0)
  {
    v6 = *(a4 + 16);
    if (v6 < 0x32)
    {
LABEL_7:
      a4;
      return _swiftEmptyArrayStorage;
    }

    v5 = dbl_33FA70[v6 < 0xC8];
    a2 = 1;
  }

  else
  {
    if (a3)
    {
      a2 = 1;
    }

    v5 = *&a1;
    if (*&a1 == 0.0)
    {
      goto LABEL_7;
    }
  }

  v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [AnnotatedFeature<URL, String>]);
  v9 = type metadata accessor for URL(0);
  v10 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type [AnnotatedFeature<URL, String>] and conformance [A], &demangling cache variable for type metadata for [AnnotatedFeature<URL, String>], &protocol conformance descriptor for [A]);
  return Sequence.randomSplit<A, B>(by:seed:)(a2, 0, v8, v9, &type metadata for String, v10, v5, &protocol witness table for String, a4);
}

uint64_t specialized MLImageClassifier.FeatureExtractor.extractFeatures<A>(from:)(uint64_t a1)
{
  *(v2 + 24) = v1;
  *(v2 + 16) = a1;
  return swift_task_switch(specialized MLImageClassifier.FeatureExtractor.extractFeatures<A>(from:), 0, 0);
}

{
  v5 = *(*v2 + 32);
  v4 = *v2;
  *(*v2 + 40) = v1;
  v5;
  if (v1)
  {
    v6 = specialized MLImageClassifier.FeatureExtractor.extractFeatures<A>(from:);
  }

  else
  {
    *(v4 + 48) = a1;
    v6 = specialized MLImageClassifier.FeatureExtractor.extractFeatures<A>(from:);
  }

  return swift_task_switch(v6, 0, 0);
}

uint64_t specialized MLImageClassifier.FeatureExtractor.extractFeatures<A>(from:)()
{
  v1 = *(v0 + 24);
  v2 = v1[3];
  v7 = v1[4];
  __swift_project_boxed_opaque_existential_0Tm(v1, v2);
  v3 = swift_task_alloc(async function pointer to Transformer.applied<A, B>(to:eventHandler:)[1]);
  *(v0 + 32) = v3;
  v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [AnnotatedFeature<CIImage, String>]);
  v5 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type [AnnotatedFeature<CIImage, String>] and conformance [A], &demangling cache variable for type metadata for [AnnotatedFeature<CIImage, String>], &protocol conformance descriptor for [A]);
  *v3 = v0;
  v3[1] = specialized MLImageClassifier.FeatureExtractor.extractFeatures<A>(from:);
  retaddr = v5;
  return Transformer.applied<A, B>(to:eventHandler:)(v0 + 16, 0, 0, v2, v4, &type metadata for String);
}

{
  return (*(v0 + 8))(*(v0 + 48));
}

{
  v1 = *(v0 + 40);
  return (*(v0 + 8))();
}

uint64_t specialized Transformer.applied<A, B>(to:eventHandler:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  v7[12] = v6;
  v7[11] = a6;
  v7[10] = a5;
  v7[9] = a4;
  v7[8] = a3;
  v7[7] = a2;
  v7[6] = a1;
  v8 = type metadata accessor for ImageReader(0);
  v7[13] = v8;
  v9 = *(v8 - 8);
  v7[14] = v9;
  v7[15] = swift_task_alloc((*(v9 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v11 = type metadata accessor for Event(0, a2, v10);
  v7[16] = v11;
  v12 = *(v11 - 8);
  v7[17] = v12;
  v7[18] = swift_task_alloc((*(v12 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v13 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<CIImage, String>);
  v7[19] = v13;
  v14 = *(v13 - 8);
  v7[20] = v14;
  v7[21] = swift_task_alloc((*(v14 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v15 = type metadata accessor for URL(0);
  v7[22] = v15;
  v16 = *(v15 - 8);
  v7[23] = v16;
  v7[24] = swift_task_alloc((*(v16 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v17 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<URL, String>);
  v7[25] = v17;
  v18 = *(v17 - 8);
  v7[26] = v18;
  v7[27] = swift_task_alloc((*(v18 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  return swift_task_switch(specialized Transformer.applied<A, B>(to:eventHandler:), 0, 0);
}

uint64_t specialized Transformer.applied<A, B>(to:eventHandler:)()
{
  v1 = *(v0 + 64);
  v2 = *(v0 + 72) >> 1;
  v3 = v2 - v1;
  *(v0 + 224) = v2 - v1;
  if (__OFSUB__(v2, v1))
  {
    BUG();
  }

  if (v3 <= 0)
  {
    v3 = 0;
  }

  v4 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, v3, 0, _swiftEmptyArrayStorage);
  if (v2 == v1)
  {
    v5 = *(v0 + 192);
    v6 = *(v0 + 168);
    v18 = v4;
    v7 = *(v0 + 120);
    v8 = *(v0 + 144);
    *(v0 + 216);
    v5;
    v6;
    v8;
    v7;
    return (*(v0 + 8))(v18);
  }

  else
  {
    v9 = *(v0 + 208);
    v10 = *(v0 + 48);
    v11 = *(v0 + 64);
    *(v0 + 240) = v4;
    *(v0 + 232) = v11;
    v12 = *(v0 + 72) >> 1;
    swift_unknownObjectRetain(v10);
    if (v11 >= v12)
    {
      BUG();
    }

    (*(v9 + 16))(*(v0 + 216), *(v0 + 56) + *(v9 + 72) * v11, *(v0 + 200));
    static Task<>.checkCancellation()();
    v14 = *(v0 + 216);
    v15 = *(v0 + 192);
    AnnotatedFeature.feature.getter(*(v0 + 200));
    v16 = swift_task_alloc(async function pointer to dispatch thunk of Transformer.applied(to:eventHandler:)[1]);
    *(v0 + 248) = v16;
    *v16 = v0;
    v16[1] = specialized Transformer.applied<A, B>(to:eventHandler:);
    v17 = *(v0 + 96);
    return dispatch thunk of Transformer.applied(to:eventHandler:)(v0 + 32, *(v0 + 192), *(v0 + 80), *(v0 + 88), *(v0 + 104), &protocol witness table for ImageReader);
  }
}

{
  v3 = *(*v1 + 248);
  v2 = *v1;
  *(*v1 + 256) = v0;
  v3;
  if (v0)
  {
    v4 = v2[30];
    (*(v2[23] + 8))(v2[24], v2[22]);
    v4;
    v5 = specialized Transformer.applied<A, B>(to:eventHandler:);
  }

  else
  {
    (*(v2[23] + 8))(v2[24], v2[22]);
    v5 = specialized Transformer.applied<A, B>(to:eventHandler:);
  }

  return swift_task_switch(v5, 0, 0);
}

{
  v1 = v0[30];
  v2 = v0[27];
  v3 = v0[25];
  v4 = v0[4];
  v52 = v0[21];
  v0[5] = v4;
  v45 = v4;
  AnnotatedFeature.annotation.getter(v3);
  v5 = type metadata accessor for OS_os_log(0, &lazy cache variable for type metadata for CIImage, CIImage_ptr);
  AnnotatedFeature.init(feature:annotation:)(v0 + 5, v0 + 2, v5, &type metadata for String);
  v6 = *(v1 + 16);
  v7 = v0[30];
  if (*(v1 + 24) >> 1 <= v6)
  {
    v7 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(*(v1 + 24) >= 2uLL, v6 + 1, 1, v0[30]);
  }

  v8 = v0[21];
  v9 = v0[19];
  v10 = v0[20];
  v11 = v0[10];
  v7[2] = v6 + 1;
  v12 = v7 + ((*(v10 + 80) + 32) & ~*(v10 + 80)) + *(v10 + 72) * v6;
  v13 = v7;
  (*(v10 + 32))(v12, v8, v9);
  v55 = v13;
  if (v11)
  {
    v47 = v0[28];
    v14 = v0[18];
    v53 = v0[17];
    v46 = v0[16];
    v15 = v0[15];
    v16 = v0[13];
    v48 = v0[10];
    v17 = v0[11];
    (*(v0[14] + 16))(v15, v0[12], v16);

    v49 = String.init<A>(describing:)(v15, v16);
    v50 = v18;
    v51 = v55[2];
    v19 = type metadata accessor for MetricsKey(0);
    v20 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Sendable);
    v21 = lazy protocol witness table accessor for type VNImageOption and conformance VNImageOption(&lazy protocol witness table cache variable for type MetricsKey and conformance MetricsKey, &type metadata accessor for MetricsKey, &protocol conformance descriptor for MetricsKey);
    v22 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, v19, v20, v21);
    Event.init(origin:itemCount:totalItemCount:metrics:)(v49, v50, v51, v47, 0, v22);
    v48(v14);
    _sxRi_zRi0_zlySaySdGIsegr_SgWOe(v48, v17);
    (*(v53 + 8))(v14, v46);
  }

  v23 = v0[29];
  v24 = v0[27];
  v54 = v0[26];
  v25 = v0[9];
  v26 = v0[25];

  (*(v54 + 8))(v24, v26);
  if (v23 + 1 == v25 >> 1)
  {
    swift_unknownObjectRelease(v0[6]);
    v27 = v0[24];
    v28 = v0[21];
    v29 = v0[15];
    v30 = v0[18];
    v0[27];
    v27;
    v28;
    v30;
    v29;
    v31 = v0[1];
    v32 = v55;
    return v31(v32);
  }

  v33 = v0[32];
  v34 = v0[9];
  v35 = v0[29] + 1;
  v0[30] = v55;
  v0[29] = v35;
  if (v35 >= (v34 >> 1))
  {
    BUG();
  }

  (*(v0[26] + 16))(v0[27], v0[7] + *(v0[26] + 72) * v35, v0[25]);
  static Task<>.checkCancellation()();
  v36 = v0[27];
  if (v33)
  {
    v37 = v0[6];
    (*(v0[26] + 8))(v0[27], v0[25]);
    swift_unknownObjectRelease(v37);
    v55;
    v38 = v0[24];
    v39 = v0[21];
    v56 = v0[15];
    v40 = v0[18];
    v0[27];
    v38;
    v39;
    v40;
    v32 = v56;
    v56;
    v31 = v0[1];
    return v31(v32);
  }

  v42 = v0[24];
  AnnotatedFeature.feature.getter(v0[25]);
  v43 = swift_task_alloc(async function pointer to dispatch thunk of Transformer.applied(to:eventHandler:)[1]);
  v0[31] = v43;
  *v43 = v0;
  v43[1] = specialized Transformer.applied<A, B>(to:eventHandler:);
  v44 = v0[12];
  return dispatch thunk of Transformer.applied(to:eventHandler:)(v0 + 4, v0[24], v0[10], v0[11], v0[13], &protocol witness table for ImageReader);
}

{
  v1 = *(v0 + 48);
  (*(*(v0 + 208) + 8))(*(v0 + 216), *(v0 + 200));
  swift_unknownObjectRelease(v1);
  v7 = *(v0 + 256);
  v2 = *(v0 + 192);
  v3 = *(v0 + 168);
  v4 = *(v0 + 120);
  v5 = *(v0 + 144);
  *(v0 + 216);
  v2;
  v3;
  v5;
  v4;
  return (*(v0 + 8))();
}

{
  v1 = *(*(v0 + 48) + 16);
  *(v0 + 200) = v1;
  v2 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, v1, 0, _swiftEmptyArrayStorage);
  if (v1)
  {
    v3 = *(v0 + 184);
    v4 = *(v3 + 80);
    *(v0 + 256) = v4;
    v5 = *(v3 + 16);
    *(v0 + 208) = *(v3 + 72);
    *(v0 + 216) = v5;
    *(v0 + 232) = v2;
    *(v0 + 224) = 0;
    v6 = *(v0 + 192);
    v7 = *(v0 + 48);
    v8 = *(v0 + 176);
    v9 = v7 + ((v4 + 32) & ~v4);
    v7;
    v5(v6, v9, v8);
    static Task<>.checkCancellation()();
    v13 = *(v0 + 192);
    v14 = *(v0 + 168);
    AnnotatedFeature.feature.getter(*(v0 + 176));
    v15 = swift_task_alloc(async function pointer to dispatch thunk of Transformer.applied(to:eventHandler:)[1]);
    *(v0 + 240) = v15;
    *v15 = v0;
    v15[1] = specialized Transformer.applied<A, B>(to:eventHandler:);
    v16 = *(v0 + 72);
    return dispatch thunk of Transformer.applied(to:eventHandler:)(v0 + 32, *(v0 + 168), *(v0 + 56), *(v0 + 64), *(v0 + 80), &protocol witness table for ImageReader);
  }

  else
  {
    v10 = *(v0 + 168);
    v11 = *(v0 + 144);
    v18 = *(v0 + 96);
    v12 = *(v0 + 120);
    *(v0 + 192);
    v10;
    v11;
    v12;
    v18;
    return (*(v0 + 8))();
  }
}

{
  v3 = *(*v1 + 240);
  v2 = *v1;
  *(*v1 + 248) = v0;
  v3;
  if (v0)
  {
    v4 = v2[29];
    (*(v2[20] + 8))(v2[21], v2[19]);
    v4;
    v5 = specialized Transformer.applied<A, B>(to:eventHandler:);
  }

  else
  {
    (*(v2[20] + 8))(v2[21], v2[19]);
    v5 = specialized Transformer.applied<A, B>(to:eventHandler:);
  }

  return swift_task_switch(v5, 0, 0);
}

{
  v1 = *(v0 + 232);
  v2 = *(v0 + 192);
  v3 = *(v0 + 176);
  v4 = *(v0 + 32);
  v51 = *(v0 + 144);
  *(v0 + 40) = v4;
  v44 = v4;
  AnnotatedFeature.annotation.getter(v3);
  v5 = type metadata accessor for OS_os_log(0, &lazy cache variable for type metadata for CIImage, CIImage_ptr);
  AnnotatedFeature.init(feature:annotation:)(v0 + 40, v0 + 16, v5, &type metadata for String);
  v6 = *(v1 + 16);
  v7 = *(v0 + 232);
  if (*(v1 + 24) >> 1 <= v6)
  {
    v7 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(*(v1 + 24) >= 2uLL, v6 + 1, 1, *(v0 + 232));
  }

  v8 = *(v0 + 144);
  v9 = *(v0 + 128);
  v10 = *(v0 + 136);
  v11 = *(v0 + 56);
  v7[2] = v6 + 1;
  v12 = v7 + ((*(v10 + 80) + 32) & ~*(v10 + 80)) + *(v10 + 72) * v6;
  v13 = v7;
  (*(v10 + 32))(v12, v8, v9);
  v54 = v13;
  if (v11)
  {
    v46 = *(v0 + 200);
    v14 = *(v0 + 120);
    v52 = *(v0 + 112);
    v45 = *(v0 + 104);
    v15 = *(v0 + 96);
    v16 = *(v0 + 80);
    v47 = *(v0 + 56);
    v17 = *(v0 + 64);
    (*(*(v0 + 88) + 16))(v15, *(v0 + 72), v16);

    v48 = String.init<A>(describing:)(v15, v16);
    v49 = v18;
    v50 = v54[2];
    v19 = type metadata accessor for MetricsKey(0);
    v20 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Sendable);
    v21 = lazy protocol witness table accessor for type VNImageOption and conformance VNImageOption(&lazy protocol witness table cache variable for type MetricsKey and conformance MetricsKey, &type metadata accessor for MetricsKey, &protocol conformance descriptor for MetricsKey);
    v22 = Dictionary.init(dictionaryLiteral:)(_swiftEmptyArrayStorage, v19, v20, v21);
    Event.init(origin:itemCount:totalItemCount:metrics:)(v48, v49, v50, v46, 0, v22);
    v47(v14);
    _sxRi_zRi0_zlySaySdGIsegr_SgWOe(v47, v17);
    (*(v52 + 8))(v14, v45);
  }

  v23 = *(v0 + 224);
  v24 = *(v0 + 192);
  v25 = *(v0 + 176);
  v26 = *(v0 + 184);
  v53 = *(v0 + 200);

  (*(v26 + 8))(v24, v25);
  if (v23 + 1 == v53)
  {
    *(v0 + 48);
    v27 = *(v0 + 168);
    v28 = *(v0 + 144);
    v29 = *(v0 + 96);
    v30 = *(v0 + 120);
    *(v0 + 192);
    v27;
    v28;
    v30;
    v29;
    v31 = *(v0 + 8);
    v32 = v54;
    return v31(v32);
  }

  v33 = *(v0 + 248);
  v34 = *(v0 + 224) + 1;
  *(v0 + 232) = v54;
  *(v0 + 224) = v34;
  (*(v0 + 216))(*(v0 + 192), *(v0 + 48) + ((*(v0 + 256) + 32) & ~*(v0 + 256)) + *(v0 + 208) * v34, *(v0 + 176));
  static Task<>.checkCancellation()();
  v35 = *(v0 + 192);
  if (v33)
  {
    v36 = *(v0 + 48);
    (*(*(v0 + 184) + 8))(*(v0 + 192), *(v0 + 176));
    v36;
    v54;
    v37 = *(v0 + 168);
    v38 = *(v0 + 144);
    v55 = *(v0 + 96);
    v39 = *(v0 + 120);
    *(v0 + 192);
    v37;
    v38;
    v39;
    v32 = v55;
    v55;
    v31 = *(v0 + 8);
    return v31(v32);
  }

  v41 = *(v0 + 168);
  AnnotatedFeature.feature.getter(*(v0 + 176));
  v42 = swift_task_alloc(async function pointer to dispatch thunk of Transformer.applied(to:eventHandler:)[1]);
  *(v0 + 240) = v42;
  *v42 = v0;
  v42[1] = specialized Transformer.applied<A, B>(to:eventHandler:);
  v43 = *(v0 + 72);
  return dispatch thunk of Transformer.applied(to:eventHandler:)(v0 + 32, *(v0 + 168), *(v0 + 56), *(v0 + 64), *(v0 + 80), &protocol witness table for ImageReader);
}

{
  v1 = *(v0 + 48);
  (*(*(v0 + 184) + 8))(*(v0 + 192), *(v0 + 176));
  v1;
  v7 = *(v0 + 248);
  v2 = *(v0 + 168);
  v3 = *(v0 + 144);
  v4 = *(v0 + 96);
  v5 = *(v0 + 120);
  *(v0 + 192);
  v2;
  v3;
  v5;
  v4;
  return (*(v0 + 8))();
}

uint64_t specialized Transformer.applied<A, B>(to:eventHandler:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  v4[9] = v3;
  v4[8] = a3;
  v4[7] = a2;
  v4[6] = a1;
  v5 = type metadata accessor for ImageReader(0);
  v4[10] = v5;
  v6 = *(v5 - 8);
  v4[11] = v6;
  v4[12] = swift_task_alloc((*(v6 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v8 = type metadata accessor for Event(0, a2, v7);
  v4[13] = v8;
  v9 = *(v8 - 8);
  v4[14] = v9;
  v4[15] = swift_task_alloc((*(v9 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v10 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<CIImage, String>);
  v4[16] = v10;
  v11 = *(v10 - 8);
  v4[17] = v11;
  v4[18] = swift_task_alloc((*(v11 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v12 = type metadata accessor for URL(0);
  v4[19] = v12;
  v13 = *(v12 - 8);
  v4[20] = v13;
  v4[21] = swift_task_alloc((*(v13 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<URL, String>);
  v4[22] = v14;
  v15 = *(v14 - 8);
  v4[23] = v15;
  v4[24] = swift_task_alloc((*(v15 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  return swift_task_switch(specialized Transformer.applied<A, B>(to:eventHandler:), 0, 0);
}

NSURL *specialized Data.append<A>(contentsOf:)(uint64_t *a1, char a2)
{
  v36 = a1;
  v37 = a2;
  v34 = a1;
  LOBYTE(v35) = a2;
  v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Repeated<UInt8>);
  v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for ContiguousBytes);
  if (swift_dynamicCast(v38, &v34, v4, v5, 6))
  {
    outlined init with take of TabularRegressionTask(v38, v40);
    v6 = v41;
    v33[0] = v42;
    __swift_project_boxed_opaque_existential_0Tm(v40, v41);
    v7 = alloca(24);
    v8 = alloca(32);
    v34 = v2;
    dispatch thunk of ContiguousBytes.withUnsafeBytes<A>(_:)(partial apply for closure #1 in Data.append<A>(contentsOf:), v33, &type metadata for () + 8, v6, v33[0]);
    __swift_destroy_boxed_opaque_existential_1Tm(v40);
  }

  else
  {
    memset(v38, 0, sizeof(v38));
    v39 = 0;
    outlined destroy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v38, &demangling cache variable for type metadata for ContiguousBytes?);
    v9 = *v2;
    v10 = v2[1];
    switch(v10 >> 62)
    {
      case 0uLL:
        v11 = BYTE6(v10);
        v12 = BYTE6(v10);
        break;
      case 1uLL:
        if (__OFSUB__(HIDWORD(v9), v9))
        {
          BUG();
        }

        v12 = HIDWORD(v9) - v9;
        v11 = v9 >> 32;
        break;
      case 2uLL:
        v11 = *(v9 + 24);
        v12 = v11 - *(v9 + 16);
        if (__OFSUB__(v11, *(v9 + 16)))
        {
          BUG();
        }

        return result;
      case 3uLL:
        v11 = 0;
        v12 = 0;
        break;
    }

    if (__OFADD__(a1, v11))
    {
      BUG();
    }

    if (a1 + v11 < v11)
    {
      BUG();
    }

    if (v11 < 0)
    {
      BUG();
    }

    Data._Representation.resetBytes(in:)();
    v13 = alloca(40);
    v14 = alloca(48);
    v33[0] = v12;
    v34 = v12;
    v35 = a1;
    v36 = &v36;
    v15 = specialized Data._Representation.withUnsafeMutableBytes<A>(_:)(partial apply for specialized closure #3 in Data.append<A>(contentsOf:), v33);
    if (v18 == a1)
    {
      memset(v40, 0, 15);
      if (v17 != v15)
      {
        v19 = v16;
        v20 = v15 - 1;
        v21 = 0;
        v22 = v17;
        v33[0] = v15 - 1;
        while (1)
        {
          if (v17 < 0 || v22 >= v15)
          {
            BUG();
          }

          *(v40 + v21++) = v19;
          if (!v21)
          {
            BUG();
          }

          if (v21 == 14)
          {
            *(v38 + 6) = *(v40 + 6);
            *&v38[0] = v40[0];
            v23 = v17;
            v24 = v15;
            Data._Representation.append(contentsOf:)(v38, v38 + 14);
            v20 = v33[0];
            if (v33[0] == v22)
            {
              return __stack_chk_guard;
            }

            v17 = v23;
            v15 = v24;
            v21 = 0;
          }

          else if (v20 == v22)
          {
            *(v38 + 6) = *(v40 + 6);
            *&v38[0] = v40[0];
            Data._Representation.append(contentsOf:)(v38, v38 + v21);
            return __stack_chk_guard;
          }

          ++v22;
        }
      }
    }

    else
    {
      v25 = *v2;
      v26 = v2[1] >> 62;
      switch(v26)
      {
        case 0uLL:
        case 3uLL:
          v27 = 0;
          break;
        case 1uLL:
          v27 = v25;
          break;
        case 2uLL:
          v27 = *(v25 + 16);
          break;
      }

      v28 = __OFADD__(v33[0], v27);
      v29 = v33[0] + v27;
      if (v28)
      {
        BUG();
      }

      v28 = __OFADD__(v18, v29);
      v30 = v18 + v29;
      if (v28)
      {
        BUG();
      }

      switch(v26)
      {
        case 0:
          JUMPOUT(0x30AD00);
        case 1:
          v31 = v25 >> 32;
          break;
        case 2:
          v31 = *(v25 + 24);
          break;
        case 3:
          v31 = 0;
          break;
      }

      if (v31 < v30)
      {
        BUG();
      }

      Data._Representation.replaceSubrange(_:with:count:)(v30, v31, 0, 0);
    }
  }

  return __stack_chk_guard;
}

uint64_t ImageClassifierTrainingSessionDelegate.init(sessionParameters:)(uint64_t a1)
{
  v19 = a1;
  v2 = v1 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingParameters;
  v3 = type metadata accessor for MLImageClassifier.PersistentParameters(0);
  __swift_storeEnumTagSinglePayload(v2, 1, 1, v3);
  *(v1 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFiles) = _swiftEmptyArrayStorage;
  *(v1 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFiles) = _swiftEmptyArrayStorage;
  v4 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFeatureStore;
  v5 = (OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFeatureStore + v1);
  BlobsFile.init()();
  qmemcpy(v5, v18, 0x58uLL);
  *(v1 + v4 + 88) = _swiftEmptyArrayStorage;
  v6 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFeatureStore;
  v7 = (OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFeatureStore + v1);
  BlobsFile.init()();
  qmemcpy(v7, v17, 0x58uLL);
  *(v1 + v6 + 88) = _swiftEmptyArrayStorage;
  v8 = v1 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_classifier;
  v9 = type metadata accessor for MLImageClassifier.Classifier(0);
  __swift_storeEnumTagSinglePayload(v8, 1, 1, v9);
  v10 = v1 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_model;
  v11 = type metadata accessor for MLImageClassifier.Model(0);
  __swift_storeEnumTagSinglePayload(v10, 1, 1, v11);
  v12 = v1 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingMetrics;
  v13 = type metadata accessor for MLClassifierMetrics(0);
  __swift_storeEnumTagSinglePayload(v12, 1, 1, v13);
  __swift_storeEnumTagSinglePayload(v1 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationMetrics, 1, 1, v13);
  v14 = v1 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_tablePrinter;
  v15 = type metadata accessor for TrainingTablePrinter(0);
  __swift_storeEnumTagSinglePayload(v14, 1, 1, v15);
  outlined init with take of MLClassifierMetrics(v19, v1 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_sessionParameters, type metadata accessor for MLTrainingSessionParameters);
  return v1;
}

uint64_t ImageClassifierTrainingSessionDelegate.init(filesByLabel:modelParameters:sessionParameters:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  v27 = a3;
  v29 = a2;
  v28 = a1;
  v4 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?) - 8) + 64);
  v5 = alloca(v4);
  v6 = alloca(v4);
  v31 = v24;
  v7 = v3 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingParameters;
  v30 = v3 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingParameters;
  v32 = type metadata accessor for MLImageClassifier.PersistentParameters(0);
  __swift_storeEnumTagSinglePayload(v7, 1, 1, v32);
  *(v3 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFiles) = _swiftEmptyArrayStorage;
  *(v3 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFiles) = _swiftEmptyArrayStorage;
  v8 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFeatureStore;
  v9 = (OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFeatureStore + v3);
  BlobsFile.init()();
  qmemcpy(v9, v25, 0x58uLL);
  *(v3 + v8 + 88) = _swiftEmptyArrayStorage;
  v10 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFeatureStore;
  v11 = (OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFeatureStore + v3);
  BlobsFile.init()();
  qmemcpy(v11, v24, 0x58uLL);
  *(v3 + v10 + 88) = _swiftEmptyArrayStorage;
  v12 = v3 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_classifier;
  v13 = type metadata accessor for MLImageClassifier.Classifier(0);
  __swift_storeEnumTagSinglePayload(v12, 1, 1, v13);
  v14 = v3 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_model;
  v15 = type metadata accessor for MLImageClassifier.Model(0);
  __swift_storeEnumTagSinglePayload(v14, 1, 1, v15);
  v16 = v3 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingMetrics;
  v17 = type metadata accessor for MLClassifierMetrics(0);
  __swift_storeEnumTagSinglePayload(v16, 1, 1, v17);
  __swift_storeEnumTagSinglePayload(v3 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationMetrics, 1, 1, v17);
  v18 = v3 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_tablePrinter;
  v19 = type metadata accessor for TrainingTablePrinter(0);
  __swift_storeEnumTagSinglePayload(v18, 1, 1, v19);
  v20 = v29;
  outlined init with copy of MLImageClassifier.ModelParameters(v29, v26);
  v21 = v31;
  MLImageClassifier.PersistentParameters.init(trainingData:modelParameters:)(v28, v26);
  outlined destroy of MLImageClassifier.ModelParameters(v20);
  __swift_storeEnumTagSinglePayload(v21, 0, 1, v32);
  v22 = v30;
  swift_beginAccess(v30, v26, 33, 0);
  outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v21, v22, &demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?);
  swift_endAccess(v26);
  outlined init with take of MLClassifierMetrics(v27, v3 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_sessionParameters, type metadata accessor for MLTrainingSessionParameters);
  return v3;
}

Swift::OpaquePointer ImageClassifierTrainingSessionDelegate.populateFiles(parameters:)(Swift::OpaquePointer *a1)
{
  v2 = v1;
  v3 = a1 + *(type metadata accessor for MLImageClassifier.PersistentParameters(0) + 20);
  v4 = MLImageClassifier.ModelParameters.ValidationData.extractFilesByLabel(trainingFiles:)(a1->_rawValue);
  if (!v5)
  {
    rawValue = v4.training._rawValue;
    v7 = v4.validation._rawValue;
    v8 = specialized Sequence.flatMap<A>(_:)(v4.training._rawValue);
    rawValue;
    v9 = *(v2 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFiles);
    *(v2 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFiles) = v8;
    v9;
    v10 = specialized Sequence.flatMap<A>(_:)(v7);
    v7;
    v11 = *(v2 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFiles);
    *(v2 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFiles) = v10;
    v4.training._rawValue = v11;
  }

  return v4.training;
}

Swift::Void __swiftcall __spoils<cf,zf,sf,of,pf,rax,rdx,rcx,rdi,rsi,r8,r9,r10,r11,r12,xmm0,xmm1,xmm2,xmm3,xmm4,xmm5,xmm6,xmm7> ImageClassifierTrainingSessionDelegate.setUp()()
{
  v80 = v0;
  v79 = v1;
  v2 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLImageClassifier.Model?) - 8) + 64);
  v3 = alloca(v2);
  v4 = alloca(v2);
  v73 = v54;
  v68 = type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType(0);
  v5 = *(*(v68 - 8) + 64);
  v6 = alloca(v5);
  v7 = alloca(v5);
  v69 = v54;
  v8 = alloca(v5);
  v9 = alloca(v5);
  v67 = v54;
  v71 = type metadata accessor for MLImageClassifier.ModelParameters.ValidationData(0);
  v10 = *(*(v71 - 8) + 64);
  v11 = alloca(v10);
  v12 = alloca(v10);
  v72 = v54;
  v13 = alloca(v10);
  v14 = alloca(v10);
  v77 = v54;
  v15 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLImageClassifier.Classifier?) - 8) + 64);
  v16 = alloca(v15);
  v17 = alloca(v15);
  v66 = v54;
  v18 = alloca(v15);
  v19 = alloca(v15);
  v70 = v54;
  v20 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?) - 8) + 64);
  v21 = alloca(v20);
  v22 = alloca(v20);
  v23 = type metadata accessor for MLImageClassifier.PersistentParameters(0);
  v24 = *(*(v23 - 1) + 64);
  v25 = alloca(v24);
  v26 = alloca(v24);
  v78 = v54;
  v27 = v79 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingParameters;
  swift_beginAccess(v79 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingParameters, v58, 0, 0);
  outlined init with copy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v27, v54, &demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?);
  if (__swift_getEnumTagSinglePayload(v54, 1, v23) == 1)
  {
    outlined destroy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v54, &demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?);
    _assertionFailure(_:_:file:line:flags:)("Fatal error", 11, 2, 0xD00000000000002CLL, "SessionDelegate.swift" + 0x8000000000000000, "CreateML/ImageClassifierTrainingSessionDelegate.swift", 53, 2, 68, 0);
    BUG();
  }

  v28 = v78;
  outlined init with take of MLClassifierMetrics(v54, v78, type metadata accessor for MLImageClassifier.PersistentParameters);
  v29 = v80;
  ImageClassifierTrainingSessionDelegate.populateFiles(parameters:)(v28);
  if (v29)
  {
    outlined destroy of MLActivityClassifier.ModelParameters(v78, type metadata accessor for MLImageClassifier.PersistentParameters);
  }

  else
  {
    BlobsFile.init()();
    v80 = 0;
    v30 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFeatureStore;
    v31 = v79;
    v32 = (v79 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFeatureStore);
    swift_beginAccess(v79 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFeatureStore, v59, 1, 0);
    qmemcpy(v55, v32, sizeof(v55));
    qmemcpy(v32, v57, 0x58uLL);
    *(v31 + v30 + 88) = _swiftEmptyArrayStorage;
    outlined release of AnnotatedFeatureStore(v55);
    BlobsFile.init()();
    v33 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFeatureStore;
    v34 = (v31 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFeatureStore);
    swift_beginAccess(v31 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFeatureStore, v60, 1, 0);
    qmemcpy(v54, v34, sizeof(v54));
    qmemcpy(v34, v56, 0x58uLL);
    *(v31 + v33 + 88) = _swiftEmptyArrayStorage;
    outlined release of AnnotatedFeatureStore(v54);
    v35 = *(v31 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFiles);
    v35;
    v36 = v80;
    MLComponents16AnnotatedFeatureVy6CoreML13MLShapedArrayVySfGSSGG_SSs5NeverOTg503_s8d169ML38SoundClassifierTrainingSessionDelegateC13populateFiles33_6DADCD271D509E5C075FB900187437D410parametersyAA07MLSoundD0V20PersistentParametersV_tKFSS0A12MLComponents16fg4Vy04h4B013jK61VySfGSSGcfu0_32c7cfd4b680d8003eade90301c2a1b770ARSSTf3nnnpk_nTf1cn_nTm = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSay18CreateMLComponents16AnnotatedFeatureVy6CoreML13MLShapedArrayVySfGSSGG_SSs5NeverOTg503_s8d169ML38SoundClassifierTrainingSessionDelegateC13populateFiles33_6DADCD271D509E5C075FB900187437D410parametersyAA07MLSoundD0V20PersistentParametersV_tKFSS0A12MLComponents16fg4Vy04h4B013jK61VySfGSSGcfu0_32c7cfd4b680d8003eade90301c2a1b770ARSSTf3nnnpk_nTf1cn_nTm(v35, &demangling cache variable for type metadata for AnnotatedFeature<URL, String>, &unk_349B40);
    v80 = v36;
    v35;
    v74 = _sShyShyxGqd__nc7ElementQyd__RszSTRd__lufCSS_SaySSGTt0g5(MLComponents16AnnotatedFeatureVy6CoreML13MLShapedArrayVySfGSSGG_SSs5NeverOTg503_s8d169ML38SoundClassifierTrainingSessionDelegateC13populateFiles33_6DADCD271D509E5C075FB900187437D410parametersyAA07MLSoundD0V20PersistentParametersV_tKFSS0A12MLComponents16fg4Vy04h4B013jK61VySfGSSGcfu0_32c7cfd4b680d8003eade90301c2a1b770ARSSTf3nnnpk_nTf1cn_nTm);
    v38 = v78;
    outlined init with copy of MLTrainingSessionParameters(v78 + v23[5], v77, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
    v75 = *(v38 + v23[8]);
    v76 = *(v38 + v23[9]);
    v39 = *(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType)) + 48);
    v40 = v67;
    outlined init with copy of MLTrainingSessionParameters(v38 + v23[6], v67, type metadata accessor for MLImageClassifier.FeatureExtractorType);
    v41 = *(v38 + v23[7]);
    v42 = v41;
    if (v41 == 2)
    {
      v42 = 0;
    }

    *(v40 + v39) = v42;
    memset(v63, 0, sizeof(v63));
    memset(v62, 0, sizeof(v62));
    v61[0] = v75;
    v61[1] = v76;
    v43 = v72;
    outlined init with copy of MLTrainingSessionParameters(v77, v72, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
    v65 = v71;
    boxed_opaque_existential_0 = __swift_allocate_boxed_opaque_existential_0(v64);
    outlined init with take of MLClassifierMetrics(v43, boxed_opaque_existential_0, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
    outlined copy of MLImageClassifier.ModelParameters.ClassifierType?(v41);
    outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v64, v62, &demangling cache variable for type metadata for Any?);
    v45 = v69;
    outlined init with copy of MLTrainingSessionParameters(v40, v69, type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType);
    v65 = v68;
    v46 = __swift_allocate_boxed_opaque_existential_0(v64);
    outlined init with take of MLClassifierMetrics(v45, v46, type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType);
    outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v64, v63, &demangling cache variable for type metadata for Any?);
    outlined destroy of MLActivityClassifier.ModelParameters(v40, type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType);
    outlined destroy of MLActivityClassifier.ModelParameters(v77, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
    v47 = v70;
    MLImageClassifier.Classifier.init(labels:parameters:)(v74, v61);
    v48 = type metadata accessor for MLImageClassifier.Classifier(0);
    __swift_storeEnumTagSinglePayload(v47, 0, 1, v48);
    v49 = v79 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_classifier;
    swift_beginAccess(v79 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_classifier, v61, 33, 0);
    outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v47, v49, &demangling cache variable for type metadata for MLImageClassifier.Classifier?);
    swift_endAccess(v61);
    v50 = v66;
    outlined init with copy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v49, v66, &demangling cache variable for type metadata for MLImageClassifier.Classifier?);
    if (__swift_getEnumTagSinglePayload(v50, 1, v48) == 1)
    {
      BUG();
    }

    v51 = v73;
    MLImageClassifier.Classifier.makeTransformer()();
    outlined destroy of MLActivityClassifier.ModelParameters(v78, type metadata accessor for MLImageClassifier.PersistentParameters);
    outlined destroy of MLActivityClassifier.ModelParameters(v50, type metadata accessor for MLImageClassifier.Classifier);
    v52 = type metadata accessor for MLImageClassifier.Model(0);
    __swift_storeEnumTagSinglePayload(v51, 0, 1, v52);
    v53 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_model + v79;
    swift_beginAccess(OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_model + v79, v61, 33, 0);
    outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v51, v53, &demangling cache variable for type metadata for MLImageClassifier.Model?);
    swift_endAccess(v61);
  }
}

Swift::Void __swiftcall __spoils<cf,zf,sf,of,pf,rax,rdx,rcx,rdi,rsi,r8,r9,r10,r11,r12,xmm0,xmm1,xmm2,xmm3,xmm4,xmm5,xmm6,xmm7> ImageClassifierTrainingSessionDelegate.resume(from:)(Swift::OpaquePointer from)
{
  v167 = v1;
  v176 = v2;
  rawValue = from._rawValue;
  v166 = type metadata accessor for MLImageClassifier.Classifier(0);
  v7 = *(*(v166 - 8) + 64);
  v8 = alloca(v7);
  v9 = alloca(v7);
  v155 = &v134;
  v10 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLImageClassifier.Model?) - 8) + 64);
  v11 = alloca(v10);
  v12 = alloca(v10);
  v154 = &v134;
  v13 = alloca(v10);
  v14 = alloca(v10);
  v156 = &v134;
  v159 = type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType(0);
  v15 = *(*(v159 - 8) + 64);
  v16 = alloca(v15);
  v17 = alloca(v15);
  v160 = &v134;
  v18 = alloca(v15);
  v19 = alloca(v15);
  v158 = &v134;
  v162 = type metadata accessor for MLImageClassifier.ModelParameters.ValidationData(0);
  v20 = *(*(v162 - 8) + 64);
  v21 = alloca(v20);
  v22 = alloca(v20);
  v163 = &v134;
  v23 = alloca(v20);
  v24 = alloca(v20);
  v170 = &v134;
  v25 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLImageClassifier.Classifier?) - 8) + 64);
  v26 = alloca(v25);
  v27 = alloca(v25);
  v150 = &v134;
  v28 = alloca(v25);
  v29 = alloca(v25);
  v157 = &v134;
  v30 = alloca(v25);
  v31 = alloca(v25);
  v161 = &v134;
  v171 = type metadata accessor for URL(0);
  v151 = *(v171 - 8);
  v32 = *(v151 + 64);
  v33 = alloca(v32);
  v34 = alloca(v32);
  v152 = &v134;
  v35 = alloca(v32);
  v36 = alloca(v32);
  v168 = &v134;
  v37 = alloca(v32);
  v38 = alloca(v32);
  v153 = &v134;
  v39 = alloca(v32);
  v40 = alloca(v32);
  v149 = &v134;
  v41 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLCheckpoint?) - 8) + 64);
  v42 = alloca(v41);
  v43 = alloca(v41);
  v148 = &v134;
  v44 = alloca(v41);
  v45 = alloca(v41);
  v178 = &v134;
  v175 = type metadata accessor for MLCheckpoint(0);
  v165 = *(v175 - 1);
  v46 = *(v165 + 64);
  v47 = alloca(v46);
  v48 = alloca(v46);
  v172 = &v134;
  v49 = alloca(v46);
  v50 = alloca(v46);
  v169 = &v134;
  v51 = alloca(v46);
  v52 = alloca(v46);
  v174 = &v134;
  v53 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?) - 8) + 64);
  v54 = alloca(v53);
  v55 = alloca(v53);
  v56 = type metadata accessor for MLImageClassifier.PersistentParameters(0);
  v57 = *(*(v56 - 8) + 64);
  v58 = alloca(v57);
  v59 = alloca(v57);
  v60 = v176 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingParameters;
  swift_beginAccess(v176 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingParameters, v138, 0, 0);
  outlined init with copy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v60, &v134, &demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?);
  v164 = v56;
  if (__swift_getEnumTagSinglePayload(&v134, 1, v56) == 1)
  {
    outlined destroy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(&v134, &demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?);
    _assertionFailure(_:_:file:line:flags:)("Fatal error", 11, 2, 0xD00000000000002CLL, "SessionDelegate.swift" + 0x8000000000000000, "CreateML/ImageClassifierTrainingSessionDelegate.swift", 53, 2, 87, 0);
    BUG();
  }

  outlined init with take of MLClassifierMetrics(&v134, &v134, type metadata accessor for MLImageClassifier.PersistentParameters);
  v61 = v178;
  v62 = rawValue;
  specialized BidirectionalCollection.last.getter(rawValue);
  if (__swift_getEnumTagSinglePayload(v61, 1, v175) == 1)
  {
    outlined destroy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v61, &demangling cache variable for type metadata for MLCheckpoint?);
    v63 = lazy protocol witness table accessor for type MLCreateError and conformance MLCreateError();
    swift_allocError(&type metadata for MLCreateError, v63, 0, 0);
    *v64 = 0xD00000000000001DLL;
    *(v64 + 8) = "reated." + 0x8000000000000000;
    *(v64 + 16) = 0;
    *(v64 + 32) = 0;
    *(v64 + 48) = 0;
    swift_willThrow(&type metadata for MLCreateError, v63);
    v65 = &v134;
    goto LABEL_24;
  }

  v66 = v174;
  outlined init with take of MLClassifierMetrics(v61, v174, type metadata accessor for MLCheckpoint);
  v67 = v167;
  ImageClassifierTrainingSessionDelegate.populateFiles(parameters:)(&v134);
  v178 = v67;
  if (v67)
  {
    outlined destroy of MLActivityClassifier.ModelParameters(v66, type metadata accessor for MLCheckpoint);
    v65 = &v134;
    goto LABEL_24;
  }

  v177 = &v134;
  v68 = *(v66 + v175[5]);
  if (v68 != 2)
  {
    if (v68 == 1)
    {
      v69 = v149;
      URL.appendingPathComponent(_:isDirectory:)(0x676E696E69617274, 0xE800000000000000, 1);
      AnnotatedFeatureStore.init(contentsOf:)(v69, v3, v4, v5, v6);
      v70 = (v176 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFeatureStore);
      swift_beginAccess(v176 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFeatureStore, v140, 1, 0);
      qmemcpy(v139, v70, sizeof(v139));
      qmemcpy(v70, v136, 0x60uLL);
      outlined release of AnnotatedFeatureStore(v139);
      v71 = v153;
      URL.appendingPathComponent(_:isDirectory:)(0x69746164696C6176, 0xEA00000000006E6FLL, 1);
      AnnotatedFeatureStore.init(contentsOf:)(v71, v3, v4, v5, v6);
      v72 = v176;
      v73 = (v176 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFeatureStore);
      swift_beginAccess(v176 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFeatureStore, v141, 1, 0);
      qmemcpy(v137, v73, sizeof(v137));
      qmemcpy(v73, v135, 0x60uLL);
      outlined release of AnnotatedFeatureStore(v137);
      v74 = *(v72 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFiles);
      v74;
      MLComponents16AnnotatedFeatureVy6CoreML13MLShapedArrayVySfGSSGG_SSs5NeverOTg503_s8d169ML38SoundClassifierTrainingSessionDelegateC13populateFiles33_6DADCD271D509E5C075FB900187437D410parametersyAA07MLSoundD0V20PersistentParametersV_tKFSS0A12MLComponents16fg4Vy04h4B013jK61VySfGSSGcfu0_32c7cfd4b680d8003eade90301c2a1b770ARSSTf3nnnpk_nTf1cn_nTm = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSay18CreateMLComponents16AnnotatedFeatureVy6CoreML13MLShapedArrayVySfGSSGG_SSs5NeverOTg503_s8d169ML38SoundClassifierTrainingSessionDelegateC13populateFiles33_6DADCD271D509E5C075FB900187437D410parametersyAA07MLSoundD0V20PersistentParametersV_tKFSS0A12MLComponents16fg4Vy04h4B013jK61VySfGSSGcfu0_32c7cfd4b680d8003eade90301c2a1b770ARSSTf3nnnpk_nTf1cn_nTm(v74, &demangling cache variable for type metadata for AnnotatedFeature<URL, String>, &unk_349B40);
      v178 = 0;
      v74;
      v175 = _sShyShyxGqd__nc7ElementQyd__RszSTRd__lufCSS_SaySSGTt0g5(MLComponents16AnnotatedFeatureVy6CoreML13MLShapedArrayVySfGSSGG_SSs5NeverOTg503_s8d169ML38SoundClassifierTrainingSessionDelegateC13populateFiles33_6DADCD271D509E5C075FB900187437D410parametersyAA07MLSoundD0V20PersistentParametersV_tKFSS0A12MLComponents16fg4Vy04h4B013jK61VySfGSSGcfu0_32c7cfd4b680d8003eade90301c2a1b770ARSSTf3nnnpk_nTf1cn_nTm);
      v76 = v164;
      v77 = v177;
      outlined init with copy of MLTrainingSessionParameters(v177 + v164[5], v170, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
      v172 = *(v77 + v76[8]);
      rawValue = *(v77 + v76[9]);
      v78 = *(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType)) + 48);
      v79 = v158;
      outlined init with copy of MLTrainingSessionParameters(v77 + v76[6], v158, type metadata accessor for MLImageClassifier.FeatureExtractorType);
      v80 = *(v77 + v76[7]);
      v81 = v80;
      if (v80 == 2)
      {
        v81 = 0;
      }

      *(v79 + v78) = v81;
      memset(v144, 0, sizeof(v144));
      memset(v143, 0, sizeof(v143));
      v142[0] = v172;
      v142[1] = rawValue;
      v82 = v163;
      outlined init with copy of MLTrainingSessionParameters(v170, v163, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
      v146 = v162;
      boxed_opaque_existential_0 = __swift_allocate_boxed_opaque_existential_0(v145);
      outlined init with take of MLClassifierMetrics(v82, boxed_opaque_existential_0, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
      outlined copy of MLImageClassifier.ModelParameters.ClassifierType?(v80);
      outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v145, v143, &demangling cache variable for type metadata for Any?);
      v84 = v160;
      outlined init with copy of MLTrainingSessionParameters(v79, v160, type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType);
      v146 = v159;
      v85 = __swift_allocate_boxed_opaque_existential_0(v145);
      outlined init with take of MLClassifierMetrics(v84, v85, type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType);
      outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v145, v144, &demangling cache variable for type metadata for Any?);
      outlined destroy of MLActivityClassifier.ModelParameters(v79, type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType);
      outlined destroy of MLActivityClassifier.ModelParameters(v170, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
      v86 = v161;
      MLImageClassifier.Classifier.init(labels:parameters:)(v175, v142);
      v87 = v166;
      __swift_storeEnumTagSinglePayload(v86, 0, 1, v166);
      v88 = v176 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_classifier;
      swift_beginAccess(v176 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_classifier, v142, 33, 0);
      outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v86, v88, &demangling cache variable for type metadata for MLImageClassifier.Classifier?);
      swift_endAccess(v142);
      v89 = v157;
      outlined init with copy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v88, v157, &demangling cache variable for type metadata for MLImageClassifier.Classifier?);
      if (__swift_getEnumTagSinglePayload(v89, 1, v87) == 1)
      {
        BUG();
      }

      v90 = v156;
      MLImageClassifier.Classifier.makeTransformer()();
      outlined destroy of MLActivityClassifier.ModelParameters(v174, type metadata accessor for MLCheckpoint);
      outlined destroy of MLActivityClassifier.ModelParameters(v177, type metadata accessor for MLImageClassifier.PersistentParameters);
      outlined destroy of MLActivityClassifier.ModelParameters(v89, type metadata accessor for MLImageClassifier.Classifier);
      v91 = type metadata accessor for MLImageClassifier.Model(0);
      __swift_storeEnumTagSinglePayload(v90, 0, 1, v91);
      v92 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_model + v176;
      swift_beginAccess(OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_model + v176, v142, 33, 0);
      outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v90, v92, &demangling cache variable for type metadata for MLImageClassifier.Model?);
      swift_endAccess(v142);
      return;
    }

    v106 = lazy protocol witness table accessor for type MLCreateError and conformance MLCreateError();
    swift_allocError(&type metadata for MLCreateError, v106, 0, 0);
    *v107 = 0xD00000000000003ELL;
    *(v107 + 8) = "No checkpoints to be resumed." + 0x8000000000000000;
    *(v107 + 16) = 0;
    *(v107 + 32) = 0;
    *(v107 + 48) = 0;
    swift_willThrow(&type metadata for MLCreateError, v106);
    v108 = v66;
LABEL_23:
    outlined destroy of MLActivityClassifier.ModelParameters(v108, type metadata accessor for MLCheckpoint);
    v65 = v177;
    goto LABEL_24;
  }

  v93 = v62;
  v139[0] = v62;
  v94 = *(v62 + 16);
  v95 = 1;
  v96 = v178;
  if (!v94)
  {
    v97 = 0;
    goto LABEL_28;
  }

  v97 = v94 - 1;
  v98 = v97 * *(v165 + 72) + ((*(v165 + 80) + 32) & ~*(v165 + 80)) + v93;
  rawValue = -*(v165 + 72);
  v167 = (&loc_308EF - 0x14FFFFFFFF9B9A86);
  while (2)
  {
    v99 = v172;
    outlined init with copy of MLTrainingSessionParameters(v98, v172, type metadata accessor for MLCheckpoint);
    switch(*(v99 + v175[5]))
    {
      case 0:
        v101 = *(v99 + v175[5]);
        v100 = 0xEB0000000064657ALL;
        v102 = 0x696C616974696E69;
        goto LABEL_19;
      case 1:
        0xEA0000000000676ELL;
        outlined destroy of MLActivityClassifier.ModelParameters(v172, type metadata accessor for MLCheckpoint);
        goto LABEL_27;
      case 2:
        v100 = 0xE800000000000000;
        v102 = 0x676E696E69617274;
        goto LABEL_19;
      case 3:
        v100 = 0xEA0000000000676ELL;
        v102 = 0x697461756C617665;
        goto LABEL_19;
      case 4:
        v100 = v167;
        v102 = 0x636E657265666E69;
LABEL_19:
        v103 = *(v99 + v175[5]);
        v104 = _stringCompareWithSmolCheck(_:_:expecting:)(v102, v100, 0x6974636172747865, 0xEA0000000000676ELL, 0);
        v100;
        outlined destroy of MLActivityClassifier.ModelParameters(v172, type metadata accessor for MLCheckpoint);
        if ((v104 & 1) == 0)
        {
          v98 += rawValue;
          if (v97-- == 0)
          {
            v97 = 0;
            v96 = v178;
            v95 = 1;
            goto LABEL_28;
          }

          continue;
        }

LABEL_27:
        v95 = 0;
        v96 = v178;
LABEL_28:
        v109 = alloca(24);
        v110 = alloca(32);
        v135[1] = v139;
        v111 = v148;
        _sSq3mapyqd_0_Sgqd_0_xqd__YKXEqd__YKs5ErrorRd__Ri_d_0_r0_lFxq0_q_Ri_zRi0_zRi__Ri0__Ri_0_Ri0_0_r1_lyxs5NeverOqd_0_Isgnrzr_xSgAb2ERsd__Ri_d_0_r_0_lIetMgnrzo_Tpq5Si_8CreateML12MLCheckpointVTg5(partial apply for specialized closure #1 in BidirectionalCollection.last(where:), &v134, v97, v95, v147);
        EnumTagSinglePayload = __swift_getEnumTagSinglePayload(v111, 1, v175);
        v178 = v96;
        if (EnumTagSinglePayload == 1)
        {
          outlined destroy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v111, &demangling cache variable for type metadata for MLCheckpoint?);
          v113 = v176;
          goto LABEL_33;
        }

        v114 = v169;
        outlined init with take of MLClassifierMetrics(v111, v169, type metadata accessor for MLCheckpoint);
        v115 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<MLCheckpoint>);
        v116 = *(v165 + 80);
        v117 = (v116 + 32) & ~*(v165 + 80);
        v118._rawValue = swift_allocObject(v115, v117 + *(v165 + 72), v116 | 7);
        *(v118._rawValue + 2) = 1;
        *(v118._rawValue + 3) = 2;
        outlined init with copy of MLTrainingSessionParameters(v114, v118._rawValue + v117, type metadata accessor for MLCheckpoint);
        v113 = v176;
        ImageClassifierTrainingSessionDelegate.resume(from:)(v118);
        v178 = v119;
        if (!v119)
        {
          outlined destroy of MLActivityClassifier.ModelParameters(v169, type metadata accessor for MLCheckpoint);
          swift_setDeallocating(v118._rawValue);
          specialized _ContiguousArrayStorage.__deallocating_deinit();
LABEL_33:
          v120 = v152;
          URL.appendingPathComponent(_:)(0x6C65646F6DLL, 0xE500000000000000);
          URL.appendingPathExtension(_:)(6777712, 0xE300000000000000);
          v121 = *(v151 + 8);
          v121(v120, v171);
          v122 = v113 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_classifier;
          swift_beginAccess(v113 + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_classifier, v139, 0, 0);
          v123 = v122;
          v124 = v150;
          outlined init with copy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v123, v150, &demangling cache variable for type metadata for MLImageClassifier.Classifier?);
          v125 = v166;
          if (__swift_getEnumTagSinglePayload(v124, 1, v166) == 1)
          {
            v121(v168, v171);
            outlined destroy of MLActivityClassifier.ModelParameters(v174, type metadata accessor for MLCheckpoint);
            outlined destroy of MLActivityClassifier.ModelParameters(v177, type metadata accessor for MLImageClassifier.PersistentParameters);
            outlined destroy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v124, &demangling cache variable for type metadata for MLImageClassifier.Classifier?);
            return;
          }

          v175 = v121;
          v126 = v155;
          outlined init with take of MLClassifierMetrics(v124, v155, type metadata accessor for MLImageClassifier.Classifier);
          v127 = v126;
          v128 = lazy protocol witness table accessor for type VNImageOption and conformance VNImageOption(&lazy protocol witness table cache variable for type MLImageClassifier.Classifier and conformance MLImageClassifier.Classifier, type metadata accessor for MLImageClassifier.Classifier, &protocol conformance descriptor for MLImageClassifier.Classifier);
          v129 = v154;
          v130 = v168;
          v131 = v178;
          UpdatableSupervisedEstimator.readWithOptimizer(from:)(v168, v125, v128);
          if (!v131)
          {
            outlined destroy of MLActivityClassifier.ModelParameters(v127, type metadata accessor for MLImageClassifier.Classifier);
            (v175)(v130, v171);
            outlined destroy of MLActivityClassifier.ModelParameters(v174, type metadata accessor for MLCheckpoint);
            outlined destroy of MLActivityClassifier.ModelParameters(v177, type metadata accessor for MLImageClassifier.PersistentParameters);
            v132 = type metadata accessor for MLImageClassifier.Model(0);
            __swift_storeEnumTagSinglePayload(v129, 0, 1, v132);
            v133 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_model + v176;
            swift_beginAccess(OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_model + v176, v137, 33, 0);
            outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v129, v133, &demangling cache variable for type metadata for MLImageClassifier.Model?);
            swift_endAccess(v137);
            return;
          }

          outlined destroy of MLActivityClassifier.ModelParameters(v127, type metadata accessor for MLImageClassifier.Classifier);
          (v175)(v130, v171);
          v108 = v174;
          goto LABEL_23;
        }

        swift_setDeallocating(v118._rawValue);
        specialized _ContiguousArrayStorage.__deallocating_deinit();
        outlined destroy of MLActivityClassifier.ModelParameters(v169, type metadata accessor for MLCheckpoint);
        outlined destroy of MLActivityClassifier.ModelParameters(v174, type metadata accessor for MLCheckpoint);
        v65 = v177;
LABEL_24:
        outlined destroy of MLActivityClassifier.ModelParameters(v65, type metadata accessor for MLImageClassifier.PersistentParameters);
        return;
    }
  }
}

uint64_t ImageClassifierTrainingSessionDelegate.extractFeatures(from:)(uint64_t a1)
{
  v2[87] = v1;
  v2[86] = a1;
  v3 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLShapedArray<Float>);
  v2[88] = v3;
  v4 = *(v3 - 8);
  v2[89] = v4;
  v5 = (*(v4 + 64) + 15) & 0xFFFFFFFFFFFFFFF0;
  v2[90] = swift_task_alloc(v5);
  v2[91] = swift_task_alloc(v5);
  v6 = type metadata accessor for ImageReader(0);
  v2[92] = v6;
  v7 = *(v6 - 8);
  v2[93] = v7;
  v2[94] = swift_task_alloc((*(v7 + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v8 = type metadata accessor for MLImageClassifier.FeatureExtractorType(0);
  v2[95] = swift_task_alloc((*(*(v8 - 8) + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v9 = type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType(0);
  v2[96] = v9;
  v10 = (*(*(v9 - 8) + 64) + 15) & 0xFFFFFFFFFFFFFFF0;
  v2[97] = swift_task_alloc(v10);
  v2[98] = swift_task_alloc(v10);
  v11 = type metadata accessor for MLImageClassifier.ModelParameters.ValidationData(0);
  v2[99] = v11;
  v12 = (*(*(v11 - 8) + 64) + 15) & 0xFFFFFFFFFFFFFFF0;
  v2[100] = swift_task_alloc(v12);
  v2[101] = swift_task_alloc(v12);
  v13 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?);
  v2[102] = swift_task_alloc((*(*(v13 - 8) + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  v14 = type metadata accessor for MLImageClassifier.PersistentParameters(0);
  v2[103] = v14;
  v2[104] = swift_task_alloc((*(*(v14 - 8) + 64) + 15) & 0xFFFFFFFFFFFFFFF0);
  return swift_task_switch(ImageClassifierTrainingSessionDelegate.extractFeatures(from:), 0, 0);
}

{
  v4 = *(*v2 + 896);
  v5 = *v2;
  *(v5 + 904) = a1;
  *(v5 + 912) = v1;
  v4;
  v6 = *(v5 + 880);
  if (v1)
  {
    v6;
    return swift_task_switch(ImageClassifierTrainingSessionDelegate.extractFeatures(from:), 0, 0);
  }

  else
  {
    v8 = *(v5 + 1032);
    v9 = *(v5 + 832);
    v6;
    v10 = *(v9 + v8);
    v11 = swift_task_alloc(416);
    *(v5 + 920) = v11;
    *v11 = v5;
    v11[1] = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
    return ((&_s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5Tu + _s8CreateML17MLImageClassifierV18applyAugmentations2to19augmentationOptions14upsampleFactorSay0A12MLComponents16AnnotatedFeatureVySo7CIImageCSSGGx_AC017ImageAugmentationI0VSitYaKSlRzAM7ElementRtzlFZAN_Tt2B5Tu))(a1, v10, 1);
  }
}

{
  v4 = *(*v2 + 920);
  v5 = *v2;
  v5[116] = a1;
  v5[117] = v1;
  v4;
  if (v1)
  {
    return swift_task_switch(ImageClassifierTrainingSessionDelegate.extractFeatures(from:), 0, 0);
  }

  v5[113];
  v7 = swift_task_alloc(64);
  v5[118] = v7;
  *v7 = v5;
  v7[1] = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
  return ((&async function pointer to specialized MLImageClassifier.FeatureExtractor.extractFeatures<A>(from:) + async function pointer to specialized MLImageClassifier.FeatureExtractor.extractFeatures<A>(from:)))(a1);
}

{
  v4 = *(*v2 + 944);
  v3 = *v2;
  v3[119] = a1;
  v3[120] = v1;
  v4;
  if (v1)
  {
    v5 = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
  }

  else
  {
    v3[116];
    v5 = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
  }

  return swift_task_switch(v5, 0, 0);
}

{
  v4 = *(*v2 + 984);
  v5 = *v2;
  v5[124] = a1;
  v5[125] = v1;
  v4;
  v5[122];
  if (v1)
  {
    return swift_task_switch(ImageClassifierTrainingSessionDelegate.extractFeatures(from:), 0, 0);
  }

  v7 = swift_task_alloc(64);
  v5[126] = v7;
  *v7 = v5;
  v7[1] = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
  return ((&async function pointer to specialized MLImageClassifier.FeatureExtractor.extractFeatures<A>(from:) + async function pointer to specialized MLImageClassifier.FeatureExtractor.extractFeatures<A>(from:)))(a1);
}

{
  v4 = *(*v2 + 1008);
  v3 = *v2;
  v3[127] = a1;
  v3[128] = v1;
  v4;
  if (v1)
  {
    v5 = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
  }

  else
  {
    v3[124];
    v5 = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
  }

  return swift_task_switch(v5, 0, 0);
}

NSURL *ImageClassifierTrainingSessionDelegate.extractFeatures(from:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  v8 = *(v7 + 824);
  v9 = *(v7 + 816);
  v10 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingParameters + *(v7 + 696);
  swift_beginAccess(v10, v7 + 616, 0, 0);
  outlined init with copy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(v10, v9, &demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?);
  if (__swift_getEnumTagSinglePayload(v9, 1, v8) == 1)
  {
    outlined destroy of Either<LogisticRegressionClassifier<Float, String>, FullyConnectedNetworkClassifier<Float, String>>(*(v7 + 816), &demangling cache variable for type metadata for MLImageClassifier.PersistentParameters?);
    _assertionFailure(_:_:file:line:flags:)("Fatal error", 11, 2, 0xD00000000000002CLL, "SessionDelegate.swift" + 0x8000000000000000, "CreateML/ImageClassifierTrainingSessionDelegate.swift", 53, 2, 145, 0);
    return __stack_chk_guard;
  }

  else
  {
    v12 = *(v7 + 696);
    outlined init with take of MLClassifierMetrics(*(v7 + 816), *(v7 + 832), type metadata accessor for MLImageClassifier.PersistentParameters);
    v13 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFiles;
    *(v7 + 840) = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFiles;
    v14 = *(*(v12 + v13) + 16);
    v15 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFiles;
    *(v7 + 848) = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFiles;
    v16 = *(v12 + v15);
    v17 = __OFADD__(*(v16 + 16), v14);
    v18 = *(v16 + 16) + v14;
    *(v7 + 856) = v18;
    if (v17)
    {
      BUG();
    }

    if (v18 <= *(v7 + 688))
    {
      outlined destroy of MLActivityClassifier.ModelParameters(*(v7 + 832), type metadata accessor for MLImageClassifier.PersistentParameters);
      v29 = *(v7 + 816);
      v30 = *(v7 + 808);
      v31 = *(v7 + 800);
      v32 = *(v7 + 784);
      v48 = *(v7 + 776);
      v46 = *(v7 + 760);
      v44 = *(v7 + 752);
      v50 = *(v7 + 720);
      v43 = *(v7 + 728);
      *(v7 + 832);
      v29;
      v30;
      v31;
      v32;
      v48;
      v46;
      v44;
      v43;
      v50;
      return (*(v7 + 8))(0, 1, v33, __stack_chk_guard, v34);
    }

    else
    {
      v45 = (v7 + 584);
      v37 = (v7 + 552);
      v19 = *(v7 + 832);
      v20 = *(v7 + 824);
      v49 = *(v7 + 808);
      v39 = *(v7 + 800);
      v38 = *(v7 + 792);
      v21 = *(v7 + 784);
      v36 = *(v7 + 776);
      v42 = *(v7 + 760);
      v35 = *(v7 + 768);
      outlined init with copy of MLTrainingSessionParameters(v19 + v20[5], v49, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
      v40 = *(v19 + v20[8]);
      v22 = v20[9];
      *(v7 + 1032) = v22;
      v41 = *(v19 + v22);
      v47 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for (featureExtractor: MLImageClassifier.FeatureExtractorType, classifier: MLImageClassifier.ModelParameters.ClassifierType));
      v23 = *(v47 + 48);
      outlined init with copy of MLTrainingSessionParameters(v19 + v20[6], v21, type metadata accessor for MLImageClassifier.FeatureExtractorType);
      v24 = *(v19 + v20[7]);
      v25 = 0;
      if (v24 != 2)
      {
        v25 = v24;
      }

      *(v21 + v23) = v25;
      *(v7 + 80) = 0;
      *(v7 + 64) = 0;
      *(v7 + 48) = 0;
      *(v7 + 32) = 0;
      *(v7 + 16) = v40;
      *(v7 + 24) = v41;
      outlined init with copy of MLTrainingSessionParameters(v49, v39, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
      *(v7 + 576) = v38;
      boxed_opaque_existential_0 = __swift_allocate_boxed_opaque_existential_0(v37);
      outlined init with take of MLClassifierMetrics(v39, boxed_opaque_existential_0, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
      outlined copy of MLImageClassifier.ModelParameters.ClassifierType?(v24);
      outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v37, v7 + 32, &demangling cache variable for type metadata for Any?);
      outlined init with copy of MLTrainingSessionParameters(v21, v36, type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType);
      *(v7 + 608) = v35;
      v27 = __swift_allocate_boxed_opaque_existential_0(v45);
      outlined init with take of MLClassifierMetrics(v36, v27, type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType);
      outlined assign with take of MLTrainingSession<MLImageClassifier>.Metadata(v45, v7 + 64, &demangling cache variable for type metadata for Any?);
      outlined destroy of MLActivityClassifier.ModelParameters(v21, type metadata accessor for MLImageClassifier.ModelParameters.ModelAlgorithmType);
      outlined destroy of MLActivityClassifier.ModelParameters(v49, type metadata accessor for MLImageClassifier.ModelParameters.ValidationData);
      MLImageClassifier.ModelParameters.algorithm.getter(v49);
      *(v36 + *(v47 + 48));
      outlined init with take of MLClassifierMetrics(v36, v42, type metadata accessor for MLImageClassifier.FeatureExtractorType);
      outlined destroy of MLImageClassifier.ModelParameters(v7 + 16);
      v28 = swift_task_alloc(144);
      *(v7 + 864) = v28;
      *v28 = v7;
      v28[1] = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
      return MLImageClassifier.FeatureExtractor.init(type:)(v7 + 352, *(v7 + 760));
    }
  }
}

uint64_t ImageClassifierTrainingSessionDelegate.extractFeatures(from:)()
{
  v2 = *(*v1 + 864);
  *(*v1 + 872) = v0;
  v2;
  if (v0)
  {
    v3 = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
  }

  else
  {
    v3 = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
  }

  return swift_task_switch(v3, 0, 0);
}

{
  v1 = v0[105];
  v2 = v0[94];
  v3 = v0[86];
  v4 = v0[87];
  ImageReader.init()();
  v5 = *(v4 + v1);
  v0[110] = v5;
  v6 = *(v5 + 16);
  v7 = OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_sessionParameters + v4;
  v8 = *(*(type metadata accessor for MLTrainingSessionParameters(0) + 20) + v7);
  v9 = __OFADD__(v3, v8);
  v10 = v3 + v8;
  if (v6 <= v3)
  {
    if (v9)
    {
      BUG();
    }

    v20 = v0[86];
    if (v0[107] < v10)
    {
      v10 = v0[107];
    }

    v0[121] = v10;
    v9 = __OFSUB__(v10, v6);
    v21 = v10 - v6;
    if (v9)
    {
      BUG();
    }

    v22 = v20 - v6;
    if (v21 < v22)
    {
      BUG();
    }

    v23 = *(v0[87] + v0[106]);
    v0[122] = v23;
    if (v22 < 0)
    {
      BUG();
    }

    v24 = *(v23 + 16);
    if (v24 < v22 || v24 < v21)
    {
      BUG();
    }

    v25 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<URL, String>) - 8) + 80);
    v23;
    v26 = swift_task_alloc(272);
    v0[123] = v26;
    *v26 = v0;
    v26[1] = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
    v27 = v0[94];
    v16 = 2 * v21 + 1;
    v17 = &async function pointer to specialized Transformer.applied<A, B>(to:eventHandler:) + async function pointer to specialized Transformer.applied<A, B>(to:eventHandler:);
    v18 = v23;
    v19 = v23 + ((v25 + 32) & ~v25);
    v14 = v22;
  }

  else
  {
    if (v9)
    {
      BUG();
    }

    v11 = v0[86];
    if (v6 < v10)
    {
      v10 = v6;
    }

    v0[111] = v10;
    if (v10 < v11)
    {
      BUG();
    }

    if (v11 < 0)
    {
      BUG();
    }

    v12 = *(*(__swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<URL, String>) - 8) + 80);
    v5;
    v13 = swift_task_alloc(272);
    v0[112] = v13;
    *v13 = v0;
    v13[1] = ImageClassifierTrainingSessionDelegate.extractFeatures(from:);
    v14 = v0[86];
    v15 = v0[94];
    v16 = 2 * v10 + 1;
    v17 = &async function pointer to specialized Transformer.applied<A, B>(to:eventHandler:) + async function pointer to specialized Transformer.applied<A, B>(to:eventHandler:);
    v18 = v5;
    v19 = v5 + ((v12 + 32) & ~v12);
  }

  return (v17)(v18, v19, v14, v16, 0, 0);
}

{
  v56 = (v0 + 44);
  v1 = v0[119];
  v2 = v0[87] + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_trainingFeatureStore;
  v55 = v0 + 83;
  swift_beginAccess(v2, v0 + 83, 33, 0);
  v1;
  specialized Array.append<A>(contentsOf:)(v1);
  v3 = *(v1 + 16);
  v70 = v0;
  if (v3)
  {
    v64 = v0 + 20;
    v50 = v0 + 36;
    v51 = v0 + 59;
    v53 = v0 + 64;
    v47 = v0[89];
    v4 = v0[119];
    v48 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<MLShapedArray<Float>, String>);
    v5 = *(v48 - 8);
    v6 = v2;
    v7 = v4 + ((*(v5 + 80) + 32) & ~*(v5 + 80));
    v49 = (v6 + 16);
    v54 = *(v5 + 72);
    v52 = v0 + 39;
    v66 = v0[120];
    v4;
    v68 = v6;
    while (2)
    {
      v57 = v3;
      v8 = v0[88];
      v9 = v0[91];
      AnnotatedFeature.feature.getter(v48);
      v10 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type MLShapedArray<Float> and conformance MLShapedArray<A>, &demangling cache variable for type metadata for MLShapedArray<Float>, &protocol conformance descriptor for MLShapedArray<A>);
      v11 = MLShapedArrayProtocol.scalars.getter(v8, v10);
      (*(v47 + 8))(v9, v8);
      v12 = *(v11 + 16);
      if (v12 > 0x1FFFFFFFFFFFFFFFLL)
      {
        BUG();
      }

      v13 = (4 * v12 - 1) / 64;
      if (((v13 - 0x1FFFFFFFFFFFFFFLL) >> 58) < 0x3F)
      {
        BUG();
      }

      v14 = *v68;
      v15 = *(v68 + 8);
      switch(v15 >> 62)
      {
        case 0uLL:
          v16 = BYTE6(v15);
          break;
        case 1uLL:
          LODWORD(v16) = HIDWORD(v14) - v14;
          if (__OFSUB__(HIDWORD(v14), v14))
          {
            BUG();
          }

          v16 = v16;
          break;
        case 2uLL:
          v17 = *(v14 + 24);
          v18 = __OFSUB__(v17, *(v14 + 16));
          v16 = v17 - *(v14 + 16);
          if (v18)
          {
            BUG();
          }

          return result;
        case 3uLL:
          v16 = 0;
          break;
      }

      v18 = __OFADD__(64, v16);
      v19 = v16 + 64 < 0;
      v20 = v16 + 64;
      if (v18)
      {
        BUG();
      }

      if (v19)
      {
        BUG();
      }

      v21 = (v13 << 6) + 64;
      if (v21 < 0)
      {
        BUG();
      }

      v61 = v20;
      v22 = *(v68 + 80);
      isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native(v22);
      *(v68 + 80) = v22;
      if (!isUniquelyReferenced_nonNull_native)
      {
        v22 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(v22 + 2) + 1, 1, v22);
        *(v68 + 80) = v22;
      }

      v24 = *(v22 + 2);
      if (*(v22 + 3) >> 1 <= v24)
      {
        v22 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(*(v22 + 3) >= 2uLL, v24 + 1, 1, v22);
      }

      *(v22 + 2) = v24 + 1;
      v25 = v24 << 6;
      *&v22[v25 + 32] = 0x2DEADBEEFLL;
      *&v22[v25 + 40] = v21;
      *&v22[v25 + 48] = v61;
      *&v22[v25 + 56] = 0;
      *&v22[v25 + 88] = 0;
      *&v22[v25 + 72] = 0;
      *(v68 + 80) = v22;
      if (*(v68 + 16) == -1)
      {
        BUG();
      }

      ++*(v68 + 16);
      v26 = *v49;
      v27 = v49[1];
      v28 = v49[2];
      *(v64 + 3) = v49[3];
      *(v64 + 2) = v28;
      *(v64 + 1) = v27;
      *v64 = v26;
      v29 = *v68;
      v30 = *(v68 + 8);
      switch(v30 >> 62)
      {
        case 0uLL:
          v31 = BYTE6(v30);
          goto LABEL_25;
        case 1uLL:
          v32 = *v68;
          JUMPOUT(0x30D700);
        case 2uLL:
          v33 = *(v29 + 24);
          v18 = __OFSUB__(v33, *(v29 + 16));
          v31 = v33 - *(v29 + 16);
          if (v18)
          {
            BUG();
          }

LABEL_25:
          if (v31 < 64)
          {
            goto LABEL_26;
          }

          goto LABEL_27;
        case 3uLL:
LABEL_26:
          specialized Data.append<A>(contentsOf:)(&stru_20.vmsize, 0);
LABEL_27:
          specialized Data._Representation.withUnsafeMutableBytes<A>(_:)(v68, v64);
          v70[36] = 0x2DEADBEEFLL;
          v70[37] = v21;
          v70[38] = v61;
          *(v52 + 1) = 0;
          *v52 = 0;
          v52[4] = 0;
          v70[62] = &type metadata for UnsafeRawBufferPointer;
          v70[63] = &protocol witness table for UnsafeRawBufferPointer;
          v70[59] = v50;
          v70[60] = v56;
          v34 = __swift_project_boxed_opaque_existential_0Tm(v51, &type metadata for UnsafeRawBufferPointer);
          Data._Representation.append(contentsOf:)(*v34, v34[1]);
          __swift_destroy_boxed_opaque_existential_1Tm(v51);
          v35 = v11;
          v36 = *(v11 + 16);
          if (v36 > 0x1FFFFFFFFFFFFFFFLL)
          {
            BUG();
          }

          v37 = 4 * v36;
          v70[67] = &type metadata for UnsafeRawBufferPointer;
          v70[68] = &protocol witness table for UnsafeRawBufferPointer;
          v70[64] = v11 + 32;
          v70[65] = v37 + v11 + 32;
          v38 = __swift_project_boxed_opaque_existential_0Tm(v53, &type metadata for UnsafeRawBufferPointer);
          Data._Representation.append(contentsOf:)(*v38, v38[1]);
          __swift_destroy_boxed_opaque_existential_1Tm(v53);
          if (v21 > v37)
          {
            if ((v21 - v37) < 0)
            {
              BUG();
            }

            specialized Data.append<A>(contentsOf:)((v21 - v37), 0);
            v35 = v11;
          }

          v35;
          v7 += v54;
          v3 = v57 - 1;
          v0 = v70;
          if (v57 != 1)
          {
            continue;
          }

          v70[119];
          break;
      }

      break;
    }
  }

  swift_endAccess(v55);
  v39 = v0[119];
  v40 = v0[111];
  v41 = v0[104];
  v42 = v0[86];
  (*(v0[93] + 8))(v0[94], v0[92]);
  outlined destroy of MLImageClassifier.FeatureExtractor(v56);
  outlined destroy of MLActivityClassifier.ModelParameters(v41, type metadata accessor for MLImageClassifier.PersistentParameters);
  v39;
  if (__OFSUB__(v40, v42))
  {
    BUG();
  }

  v58 = v40 - v42;
  v59 = v40 >= v0[107];
  v43 = v0[102];
  v44 = v0[101];
  v45 = v70[100];
  v67 = v70[98];
  v65 = v70[97];
  v63 = v70[95];
  v62 = v70[94];
  v69 = v70[90];
  v60 = v70[91];
  v0[104];
  v43;
  v44;
  v45;
  v67;
  v65;
  v63;
  v62;
  v60;
  v69;
  return (v70[1])(v58, v59);
}

{
  v1 = v0[127];
  v2 = v0[87] + OBJC_IVAR____TtC8CreateML38ImageClassifierTrainingSessionDelegate_validationFeatureStore;
  v57 = v0 + 80;
  swift_beginAccess(v2, v0 + 80, 33, 0);
  v1;
  specialized Array.append<A>(contentsOf:)(v1);
  v3 = *(v1 + 16);
  if (v3)
  {
    v64 = v0 + 12;
    v51 = v0 + 28;
    v52 = v0 + 49;
    v55 = v0 + 54;
    v47 = v0[89];
    v4 = v0[127];
    v48 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for AnnotatedFeature<MLShapedArray<Float>, String>);
    v5 = *(v48 - 8);
    v6 = v2;
    v7 = v4 + ((*(v5 + 80) + 32) & ~*(v5 + 80));
    v49 = (v6 + 16);
    v56 = *(v5 + 72);
    v53 = v0 + 31;
    v54 = v0 + 36;
    v66 = v0[128];
    v4;
    v50 = v0;
    v70 = v6;
    while (2)
    {
      v58 = v3;
      v8 = v0[88];
      v9 = v0[90];
      AnnotatedFeature.feature.getter(v48);
      v10 = lazy protocol witness table accessor for type FullyConnectedNetworkClassifier<Float, String> and conformance FullyConnectedNetworkClassifier<A, B>(&lazy protocol witness table cache variable for type MLShapedArray<Float> and conformance MLShapedArray<A>, &demangling cache variable for type metadata for MLShapedArray<Float>, &protocol conformance descriptor for MLShapedArray<A>);
      v11 = MLShapedArrayProtocol.scalars.getter(v8, v10);
      (*(v47 + 8))(v9, v8);
      v12 = *(v11 + 16);
      if (v12 > 0x1FFFFFFFFFFFFFFFLL)
      {
        BUG();
      }

      v13 = (4 * v12 - 1) / 64;
      if (((v13 - 0x1FFFFFFFFFFFFFFLL) >> 58) < 0x3F)
      {
        BUG();
      }

      v14 = *v70;
      v15 = *(v70 + 8);
      switch(v15 >> 62)
      {
        case 0uLL:
          v16 = BYTE6(v15);
          break;
        case 1uLL:
          LODWORD(v16) = HIDWORD(v14) - v14;
          if (__OFSUB__(HIDWORD(v14), v14))
          {
            BUG();
          }

          v16 = v16;
          break;
        case 2uLL:
          v17 = *(v14 + 24);
          v18 = __OFSUB__(v17, *(v14 + 16));
          v16 = v17 - *(v14 + 16);
          if (v18)
          {
            BUG();
          }

          return result;
        case 3uLL:
          v16 = 0;
          break;
      }

      v18 = __OFADD__(64, v16);
      v19 = v16 + 64 < 0;
      v20 = v16 + 64;
      if (v18)
      {
        BUG();
      }

      if (v19)
      {
        BUG();
      }

      v21 = (v13 << 6) + 64;
      if (v21 < 0)
      {
        BUG();
      }

      v62 = v20;
      v68 = v11;
      v22 = *(v70 + 80);
      isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native(v22);
      *(v70 + 80) = v22;
      if (!isUniquelyReferenced_nonNull_native)
      {
        v22 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(v22 + 2) + 1, 1, v22);
        *(v70 + 80) = v22;
      }

      v24 = *(v22 + 2);
      if (*(v22 + 3) >> 1 <= v24)
      {
        v22 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(*(v22 + 3) >= 2uLL, v24 + 1, 1, v22);
      }

      *(v22 + 2) = v24 + 1;
      v25 = v24 << 6;
      *&v22[v25 + 32] = 0x2DEADBEEFLL;
      *&v22[v25 + 40] = v21;
      *&v22[v25 + 48] = v62;
      *&v22[v25 + 56] = 0;
      *&v22[v25 + 88] = 0;
      *&v22[v25 + 72] = 0;
      *(v70 + 80) = v22;
      if (*(v70 + 16) == -1)
      {
        BUG();
      }

      ++*(v70 + 16);
      v26 = *v49;
      v27 = v49[1];
      v28 = v49[2];
      *(v64 + 3) = v49[3];
      *(v64 + 2) = v28;
      *(v64 + 1) = v27;
      *v64 = v26;
      v29 = *v70;
      v30 = *(v70 + 8);
      switch(v30 >> 62)
      {
        case 0uLL:
          v31 = BYTE6(v30);
          goto LABEL_25;
        case 1uLL:
          v32 = *v70;
          JUMPOUT(0x30DFE1);
        case 2uLL:
          v33 = *(v29 + 24);
          v18 = __OFSUB__(v33, *(v29 + 16));
          v31 = v33 - *(v29 + 16);
          if (v18)
          {
            BUG();
          }

LABEL_25:
          if (v31 < 64)
          {
            goto LABEL_26;
          }

          goto LABEL_27;
        case 3uLL:
LABEL_26:
          specialized Data.append<A>(contentsOf:)(&stru_20.vmsize, 0);
LABEL_27:
          specialized Data._Representation.withUnsafeMutableBytes<A>(_:)(v70, v64);
          v0 = v50;
          v50[28] = 0x2DEADBEEFLL;
          v50[29] = v21;
          v50[30] = v62;
          *(v53 + 1) = 0;
          *v53 = 0;
          v53[4] = 0;
          v50[52] = &type metadata for UnsafeRawBufferPointer;
          v50[53] = &protocol witness table for UnsafeRawBufferPointer;
          v50[49] = v51;
          v50[50] = v54;
          v34 = __swift_project_boxed_opaque_existential_0Tm(v52, &type metadata for UnsafeRawBufferPointer);
          Data._Representation.append(contentsOf:)(*v34, v34[1]);
          __swift_destroy_boxed_opaque_existential_1Tm(v52);
          v35 = *(v68 + 16);
          if (v35 > 0x1FFFFFFFFFFFFFFFLL)
          {
            BUG();
          }

          v36 = 4 * v35;
          v50[57] = &type metadata for UnsafeRawBufferPointer;
          v50[58] = &protocol witness table for UnsafeRawBufferPointer;
          v50[54] = v68 + 32;
          v50[55] = v36 + v68 + 32;
          v37 = __swift_project_boxed_opaque_existential_0Tm(v55, &type metadata for UnsafeRawBufferPointer);
          Data._Representation.append(contentsOf:)(*v37, v37[1]);
          __swift_destroy_boxed_opaque_existential_1Tm(v55);
          v38 = v68;
          v39 = v7;
          if (v21 > v36)
          {
            if ((v21 - v36) < 0)
            {
              BUG();
            }

            specialized Data.append<A>(contentsOf:)((v21 - v36), 0);
            v38 = v68;
            v39 = v7;
          }

          v38;
          v3 = v58 - 1;
          v7 = v56 + v39;
          if (v58 != 1)
          {
            continue;
          }

          v50[127];
          break;
      }

      break;
    }
  }

  swift_endAccess(v57);
  v71 = v0[127];
  v40 = v0[121];
  v41 = v0[104];
  v42 = v0[86];
  (*(v0[93] + 8))(v0[94], v0[92]);
  outlined destroy of MLImageClassifier.FeatureExtractor((v0 + 44));
  outlined destroy of MLActivityClassifier.ModelParameters(v41, type metadata accessor for MLImageClassifier.PersistentParameters);
  v71;
  if (__OFSUB__(v40, v42))
  {
    BUG();
  }

  v59 = v40 - v42;
  v60 = v40 >= v0[107];
  v43 = v0[102];
  v44 = v0[101];
  v45 = v0[100];
  v67 = v0[98];
  v65 = v0[97];
  v63 = v0[95];
  v61 = v0[94];
  v72 = v0[90];
  v69 = v0[91];
  v0[104];
  v43;
  v44;
  v45;
  v67;
  v65;
  v63;
  v61;
  v69;
  v72;
  return (v0[1])(v59, v60);
}

{
  outlined destroy of MLActivityClassifier.ModelParameters(*(v0 + 832), type metadata accessor for MLImageClassifier.PersistentParameters);
  v12 = *(v0 + 872);
  v1 = *(v0 + 816);
  v2 = *(v0 + 808);
  v3 = *(v0 + 800);
  v4 = *(v0 + 784);
  v11 = *(v0 + 776);
  v10 = *(v0 + 760);
  v9 = *(v0 + 752);
  v7 = *(v0 + 720);
  v8 = *(v0 + 728);
  *(v0 + 832);
  v1;
  v2;
  v3;
  v4;
  v11;
  v10;
  v9;
  v8;
  v7;
  return (*(v0 + 8))(v7, type metadata accessor for MLImageClassifier.PersistentParameters, v5, __stack_chk_guard);
}

{
  v1 = *(v0 + 832);
  (*(*(v0 + 744) + 8))(*(v0 + 752), *(v0 + 736));
  outlined destroy of MLImageClassifier.FeatureExtractor(v0 + 352);
  outlined destroy of MLActivityClassifier.ModelParameters(v1, type metadata accessor for MLImageClassifier.PersistentParameters);
  v13 = *(v0 + 912);
  v2 = *(v0 + 816);
  v3 = *(v0 + 808);
  v4 = *(v0 + 800);
  v5 = *(v0 + 784);
  v12 = *(v0 + 776);
  v11 = *(v0 + 760);
  v10 = *(v0 + 752);
  v8 = *(v0 + 720);
  v9 = *(v0 + 728);
  *(v0 + 832);
  v2;
  v3;
  v4;
  v5;
  v12;
  v11;
  v10;
  v9;
  v8;
  return (*(v0 + 8))(v8, type metadata accessor for MLImageClassifier.PersistentParameters, v6, __stack_chk_guard);
}

{
  v1 = *(v0 + 904);
  v2 = *(v0 + 832);
  (*(*(v0 + 744) + 8))(*(v0 + 752), *(v0 + 736));
  outlined destroy of MLImageClassifier.FeatureExtractor(v0 + 352);
  outlined destroy of MLActivityClassifier.ModelParameters(v2, type metadata accessor for MLImageClassifier.PersistentParameters);
  v1;
  v14 = *(v0 + 936);
  v3 = *(v0 + 816);
  v4 = *(v0 + 808);
  v5 = *(v0 + 800);
  v6 = *(v0 + 784);
  v13 = *(v0 + 776);
  v12 = *(v0 + 760);
  v11 = *(v0 + 752);
  v9 = *(v0 + 720);
  v10 = *(v0 + 728);
  *(v0 + 832);
  v3;
  v4;
  v5;
  v6;
  v13;
  v12;
  v11;
  v10;
  v9;
  return (*(v0 + 8))(v9, type metadata accessor for MLImageClassifier.PersistentParameters, v7, __stack_chk_guard);
}

{
  v1 = *(v0 + 928);
  v2 = *(v0 + 832);
  (*(*(v0 + 744) + 8))(*(v0 + 752), *(v0 + 736));
  outlined destroy of MLImageClassifier.FeatureExtractor(v0 + 352);
  outlined destroy of MLActivityClassifier.ModelParameters(v2, type metadata accessor for MLImageClassifier.PersistentParameters);
  v1;
  v14 = *(v0 + 960);
  v3 = *(v0 + 816);
  v4 = *(v0 + 808);
  v5 = *(v0 + 800);
  v6 = *(v0 + 784);
  v13 = *(v0 + 776);
  v12 = *(v0 + 760);
  v11 = *(v0 + 752);
  v9 = *(v0 + 720);
  v10 = *(v0 + 728);
  *(v0 + 832);
  v3;
  v4;
  v5;
  v6;
  v13;
  v12;
  v11;
  v10;
  v9;
  return (*(v0 + 8))(v9, type metadata accessor for MLImageClassifier.PersistentParameters, v7, __stack_chk_guard);
}

{
  v1 = *(v0 + 832);
  (*(*(v0 + 744) + 8))(*(v0 + 752), *(v0 + 736));
  outlined destroy of MLImageClassifier.FeatureExtractor(v0 + 352);
  outlined destroy of MLActivityClassifier.ModelParameters(v1, type metadata accessor for MLImageClassifier.PersistentParameters);
  v13 = *(v0 + 1000);
  v2 = *(v0 + 816);
  v3 = *(v0 + 808);
  v4 = *(v0 + 800);
  v5 = *(v0 + 784);
  v12 = *(v0 + 776);
  v11 = *(v0 + 760);
  v10 = *(v0 + 752);
  v8 = *(v0 + 720);
  v9 = *(v0 + 728);
  *(v0 + 832);
  v2;
  v3;
  v4;
  v5;
  v12;
  v11;
  v10;
  v9;
  v8;
  return (*(v0 + 8))(v8, type metadata accessor for MLImageClassifier.PersistentParameters, v6, __stack_chk_guard);
}

{
  v1 = *(v0 + 992);
  v2 = *(v0 + 832);
  (*(*(v0 + 744) + 8))(*(v0 + 752), *(v0 + 736));
  outlined destroy of MLImageClassifier.FeatureExtractor(v0 + 352);
  outlined destroy of MLActivityClassifier.ModelParameters(v2, type metadata accessor for MLImageClassifier.PersistentParameters);
  v1;
  v14 = *(v0 + 1024);
  v3 = *(v0 + 816);
  v4 = *(v0 + 808);
  v5 = *(v0 + 800);
  v6 = *(v0 + 784);
  v13 = *(v0 + 776);
  v12 = *(v0 + 760);
  v11 = *(v0 + 752);
  v9 = *(v0 + 720);
  v10 = *(v0 + 728);
  *(v0 + 832);
  v3;
  v4;
  v5;
  v6;
  v13;
  v12;
  v11;
  v10;
  v9;
  return (*(v0 + 8))(v9, type metadata accessor for MLImageClassifier.PersistentParameters, v7, __stack_chk_guard);
}