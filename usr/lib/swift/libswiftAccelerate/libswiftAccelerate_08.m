uint64_t static vForce.cosPi<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.cosPi<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.cosPi<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.cosPi<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.cosPi<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.cosPi<A, B>(_:result:));
}

uint64_t static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  v20 = *(a7 + 16);
  v12 = v20(a4, a7);
  v19 = a8;
  v13 = *(a8 + 8);
  v14 = *(v13 + 16);
  if (v12 != v14(a5, v13))
  {
    __break(1u);
    goto LABEL_7;
  }

  v15 = v14(a5, v13);
  if (v15 != (*(*(a9 + 8) + 16))(a6))
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }

  v16 = v20(a4, a7);
  if (v16 < 0xFFFFFFFF80000000)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }

  if (v16 > 0x7FFFFFFF)
  {
    goto LABEL_9;
  }

  MEMORY[0x1EEE9AC00](v16);
  return (*(v19 + 16))(v17);
}

uint64_t closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2, uint64_t *a3, void *a4, uint64_t a5, uint64_t (*a6)(void))
{
  result = *a3;
  if (!*a3)
  {
    __break(1u);
    goto LABEL_6;
  }

  if (!*a4)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }

  if (a1)
  {
    return a6();
  }

LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vForce.tan<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.tan<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.tan<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.tan<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.tan<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.tan<A, B>(_:result:));
}

uint64_t static vForce.tanPi<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.tanPi<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.tanPi<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.tanPi<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.tanPi<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.tanPi<A, B>(_:result:));
}

uint64_t static vForce.asin<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.asin<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.asin<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.asin<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.asin<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.asin<A, B>(_:result:));
}

uint64_t static vForce.acos<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.acos<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.acos<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.acos<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.acos<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.acos<A, B>(_:result:));
}

uint64_t static vForce.atan<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.atan<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.atan<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.atan<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.atan<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.atan<A, B>(_:result:));
}

uint64_t static vForce.sinh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.sinh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.sinh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.sinh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.sinh<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.sinh<A, B>(_:result:));
}

uint64_t static vForce.cosh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.cosh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.cosh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.cosh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.cosh<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.cosh<A, B>(_:result:));
}

uint64_t static vForce.tanh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.tanh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.tanh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.tanh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.tanh<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.tanh<A, B>(_:result:));
}

uint64_t static vForce.asinh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.asinh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.asinh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.asinh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.asinh<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.asinh<A, B>(_:result:));
}

uint64_t static vForce.acosh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.acosh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.acosh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.acosh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.acosh<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.acosh<A, B>(_:result:));
}

uint64_t static vForce.atanh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.atanh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSf_Tt1gq5);
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, partial apply for closure #1 in static vForce.atanh<A>(_:), _sSa28_unsafeUninitializedCapacity16initializingWithSayxGSi_ySryxGz_SiztKXEtKcfCSd_Tt1gq5);
}

uint64_t static vForce.atanh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.atanh<A, B>(_:result:));
}

{

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, partial apply for closure #1 in static vForce.atanh<A, B>(_:result:));
}

uint64_t static vForce.ceil<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void *))
{
  v10 = (*(a3 + 16))(a2, a3);
  v12[2] = a2;
  v12[3] = a3;
  v12[4] = a1;
  return a5(v10, a4, v12);
}

uint64_t static vForce.ceil<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  v11 = *(a5 + 16);
  v12 = v11(a3, a5);
  if (v12 != (*(*(a6 + 8) + 16))(a4))
  {
    __break(1u);
    goto LABEL_6;
  }

  v13 = v11(a3, a5);
  if (v13 < 0xFFFFFFFF80000000)
  {
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }

  if (v13 > 0x7FFFFFFF)
  {
    goto LABEL_7;
  }

  MEMORY[0x1EEE9AC00](v13);
  return (*(a6 + 16))(a7);
}

uint64_t partial apply for closure #1 in static vForce.ceil<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.ceil<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.ceil<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.floor<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.floor<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.floor<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.floor<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.floor<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.floor<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, v2[6], v2[7], v2[2], v2[3], v2[4], v2[5], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

{
  return closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, v2[6], v2[7], v2[2], v2[3], v2[4], v2[5], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

uint64_t partial apply for closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, partial apply for closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, partial apply for closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

uint64_t partial apply for closure #1 in static vForce.truncatingRemainder<A, B>(dividends:divisors:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, v2[6], v2[7], v2[2], v2[3], v2[4], v2[5], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

{
  return closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, v2[6], v2[7], v2[2], v2[3], v2[4], v2[5], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, partial apply for closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, partial apply for closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in static vForce.remainder<A, B>(dividends:divisors:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, v2[6], v2[7], v2[2], v2[3], v2[4], v2[5], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

{
  return closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, v2[6], v2[7], v2[2], v2[3], v2[4], v2[5], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, partial apply for closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, partial apply for closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in static vForce.trunc<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.trunc<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.trunc<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.trunc<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.trunc<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.trunc<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.nearestInteger<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.nearestInteger<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.nearestInteger<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.nearestInteger<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.nearestInteger<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.nearestInteger<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.rsqrt<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.rsqrt<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.rsqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.rsqrt<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.rsqrt<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.rsqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sqrt<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.sqrt<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.sqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sqrt<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.sqrt<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.sqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.reciprocal<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.reciprocal<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.reciprocal<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.reciprocal<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.reciprocal<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.reciprocal<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.exp<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.exp<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.exp<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.exp<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.expm1<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.expm1<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.expm1<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.expm1<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.expm1<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.expm1<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp2<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.exp2<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.exp2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp2<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.exp2<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.exp2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log2<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.log2<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.log2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log2<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.log2<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.log2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log10<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.log10<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.log10<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log10<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.log10<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.log10<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.logb<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.logb<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.logb<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.logb<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.logb<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.logb<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.pow<A, B>(bases:exponents:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.pow<A, B>(bases:exponents:)(a1, a2, v2[6], v2[7], v2[2], v2[3], v2[4], v2[5], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.pow<A, B, C>(bases:exponents:result:));
}

{
  return closure #1 in static vForce.pow<A, B>(bases:exponents:)(a1, a2, v2[6], v2[7], v2[2], v2[3], v2[4], v2[5], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.pow<A, B, C>(bases:exponents:result:));
}

uint64_t partial apply for closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

uint64_t partial apply for closure #1 in static vForce.sin<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.sin<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.sin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sin<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.sin<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.sin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinPi<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.sinPi<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.sinPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinPi<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.sinPi<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.sinPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cos<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.cos<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.cos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cos<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.cos<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.cos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosPi<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.cosPi<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.cosPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosPi<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.cosPi<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.cosPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1)
{
  return partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

{
  return partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

uint64_t partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2)
{
  v4 = *(v2 + 48);
  v3 = *(v2 + 56);
  v5 = *(v2 + 72);
  v6 = *(v2 + 80);
  v7 = *(v2 + 32);
  v9[1] = *(v2 + 16);
  v9[2] = v7;
  v10 = v4;
  v11 = v3;
  v12 = v5;
  v13 = a1;
  v14 = v6;
  return (*(v3 + 16))(a2, v9, MEMORY[0x1E69E7CA8] + 8, v7);
}

uint64_t partial apply for closure #1 in static vForce.tan<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.tan<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.tan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tan<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.tan<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.tan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanPi<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.tanPi<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.tanPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanPi<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.tanPi<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.tanPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asin<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.asin<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.asin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asin<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.asin<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.asin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acos<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.acos<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.acos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acos<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.acos<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.acos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.atan<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.atan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.atan<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.atan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.sinh<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.sinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.sinh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.sinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.cosh<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.cosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.cosh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.cosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.tanh<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.tanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.tanh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.tanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asinh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.asinh<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.asinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asinh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.asinh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.asinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acosh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.acosh<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.acosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acosh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.acosh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.acosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atanh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySfGMd, &_sSrySfGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vForce.atanh<A, B>(_:result:));
}

{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &_sSrySdGMd, &_sSrySdGMR, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vForce.atanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atanh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69592F8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959300]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.acosh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69592A8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69592B0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.asinh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69592C8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69592D0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.tanh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69594B8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69594C0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.cosh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959338]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959340]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sinh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959478]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959480]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atan<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69592D8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69592F0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.acos<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959298]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69592A0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.asin<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69592B8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69592C0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.tanPi<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69594C8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69594D0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.tan<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69594A8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69594B0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, partial apply for closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, partial apply for closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, *(v2 + 16), *(v2 + 24), *(v2 + 32), MEMORY[0x1E6959460]);
}

{
  return closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, *(v2 + 16), *(v2 + 24), *(v2 + 32), MEMORY[0x1E6959468]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2)
{
  v3 = v2[2];
  v4 = v2[5];
  v5 = v2[10];
  v7[2] = v2[9];
  v7[3] = a1;
  v7[4] = v5;
  return (*(v4 + 24))(a2, v7, MEMORY[0x1E69E7CA8] + 8, v3);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.cosPi<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959348]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959350]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.cos<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959328]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959330]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sinPi<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959488]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959490]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sin<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959458]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959470]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, partial apply for closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, partial apply for closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, *(v2 + 16), *(v2 + 24), *(v2 + 32), *(v2 + 40), MEMORY[0x1E6959418]);
}

{
  return closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, *(v2 + 16), *(v2 + 24), *(v2 + 32), *(v2 + 40), MEMORY[0x1E6959420]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.logb<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69593F0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69593F8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log10<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69593C0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69593C8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log2<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69593E0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69593E8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.exp2<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959360]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959368]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.expm1<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959378]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959380]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.exp<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959358]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959370]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.reciprocal<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959428]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959430]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sqrt<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959498]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69594A0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.rsqrt<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959448]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959450]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.nearestInteger<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959408]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959410]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.trunc<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69593A8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E69593B0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, partial apply for closure #1 in closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, partial apply for closure #1 in closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(v2 + 16), *(v2 + 24), *(v2 + 32), *(v2 + 40), MEMORY[0x1E6959438]);
}

{
  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(v2 + 16), *(v2 + 24), *(v2 + 32), *(v2 + 40), MEMORY[0x1E6959440]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, partial apply for closure #1 in closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, partial apply for closure #1 in closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(v2 + 16), *(v2 + 24), *(v2 + 32), *(v2 + 40), MEMORY[0x1E6959398]);
}

{
  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(v2 + 16), *(v2 + 24), *(v2 + 32), *(v2 + 40), MEMORY[0x1E69593A0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, partial apply for closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, partial apply for closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(v2 + 16), *(v2 + 24), *(v2 + 32), *(v2 + 40), MEMORY[0x1E6959318]);
}

{
  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(v2 + 16), *(v2 + 24), *(v2 + 32), *(v2 + 40), MEMORY[0x1E6959320]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  v4 = v3[3];
  v5 = v3[6];
  v7[2] = v3[9];
  v7[3] = a1;
  v7[4] = a2;
  return (*(v5 + 24))(a3, v7, MEMORY[0x1E69E7CA8] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.floor<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959388]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959390]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.ceil<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959308]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E6959310]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(void))
{
  result = **(v3 + 16);
  if (result)
  {
    if (a1)
    {
      return a3();
    }
  }

  else
  {
    __break(1u);
  }

  __break(1u);
  return result;
}

Swift::Int BNNS.SparsityType.hashValue.getter()
{
  Hasher.init(_seed:)();
  MEMORY[0x1B8CB1810](0);
  return Hasher._finalize()();
}

void BNNS.SparseParameters.init(type:ratio:targetSystem:)(int a1@<W1>, int a2@<W2>, int a3@<W3>, _DWORD *a4@<X8>)
{
  *a4 = a1;
  a4[1] = a2;
  a4[2] = a3;
}

uint64_t static BNNS.FullyConnectedLayer.sparsify(batchSize:inputLayout:inputDenseShape:inputValues:output:sparseParameters:workspace:filterParameters:)(size_t a1, const void *a2, _OWORD *a3, _OWORD *a4, BNNSNDArrayDescriptor *a5, uint64_t *a6, void *a7, uint64_t a8, char a9, int a10, unsigned __int32 a11, size_t a12, int (__cdecl *a13)(void **, size_t, size_t), void (__cdecl *a14)(void *))
{
  v127 = *MEMORY[0x1E69E9840];
  memcpy(__dst, a2, sizeof(__dst));
  v19 = *a6;
  v117 = *(a6 + 2);
  v20 = *(a6 + 12);
  if (a9 & 1 | (a7 == 0))
  {
    workspace_size = 0;
  }

  else
  {
    workspace_size = a8 - a7;
  }

  v22 = _s10Accelerate4BNNSO12SparseLayoutOWOg(__dst);
  v23 = destructiveProjectEnumData for BNNS.SparseLayout(__dst);
  if (v22 != 1)
  {
    if (a9)
    {
      v54 = 0;
    }

    else
    {
      v54 = a7;
    }

    memset(&filter_params, 0, 24);
    if (a13 == 1)
    {
      v55 = a3[9];
      *&in_dense_shape.stride[7] = a3[8];
      *&in_dense_shape.data_type = v55;
      *&in_dense_shape.table_data_type = a3[10];
      v56 = a3[5];
      *&in_dense_shape.size[7] = a3[4];
      *&in_dense_shape.stride[1] = v56;
      v57 = a3[7];
      *&in_dense_shape.stride[3] = a3[6];
      *&in_dense_shape.stride[5] = v57;
      v58 = a3[1];
      *&in_dense_shape.flags = *a3;
      *&in_dense_shape.size[1] = v58;
      v59 = a3[3];
      *&in_dense_shape.size[3] = a3[2];
      *&in_dense_shape.size[5] = v59;
      v60 = *v23;
      v61 = v23[2];
      *&in_column_indices.size[1] = v23[1];
      *&in_column_indices.size[3] = v61;
      *&in_column_indices.flags = v60;
      v62 = v23[3];
      v63 = v23[4];
      v64 = v23[6];
      *&in_column_indices.stride[1] = v23[5];
      *&in_column_indices.stride[3] = v64;
      *&in_column_indices.size[5] = v62;
      *&in_column_indices.size[7] = v63;
      v65 = v23[7];
      v66 = v23[8];
      v67 = v23[10];
      *&in_column_indices.data_type = v23[9];
      *&in_column_indices.table_data_type = v67;
      *&in_column_indices.stride[5] = v65;
      *&in_column_indices.stride[7] = v66;
      v68 = a4[9];
      *&in_row_starts.stride[7] = a4[8];
      *&in_row_starts.data_type = v68;
      *&in_row_starts.table_data_type = a4[10];
      v69 = a4[5];
      *&in_row_starts.size[7] = a4[4];
      *&in_row_starts.stride[1] = v69;
      v70 = a4[7];
      *&in_row_starts.stride[3] = a4[6];
      *&in_row_starts.stride[5] = v70;
      v71 = a4[1];
      *&in_row_starts.flags = *a4;
      *&in_row_starts.size[1] = v71;
      v72 = a4[3];
      *&in_row_starts.size[3] = a4[2];
      *&in_row_starts.size[5] = v72;
      result = BNNSNDArrayFullyConnectedSparsifySparseCOO(&in_dense_shape, &in_column_indices, &in_row_starts, a5, &filter_params, a1, v54, workspace_size, 0);
    }

    else
    {
      in_values.flags = a11;
      in_values.size[0] = a12;
      in_values.size[1] = a13;
      in_values.size[2] = a14;
      v98 = a3[9];
      *&in_dense_shape.stride[7] = a3[8];
      *&in_dense_shape.data_type = v98;
      *&in_dense_shape.table_data_type = a3[10];
      v99 = a3[5];
      *&in_dense_shape.size[7] = a3[4];
      *&in_dense_shape.stride[1] = v99;
      v100 = a3[7];
      *&in_dense_shape.stride[3] = a3[6];
      *&in_dense_shape.stride[5] = v100;
      v101 = a3[1];
      *&in_dense_shape.flags = *a3;
      *&in_dense_shape.size[1] = v101;
      v102 = a3[3];
      *&in_dense_shape.size[3] = a3[2];
      *&in_dense_shape.size[5] = v102;
      v103 = *v23;
      v104 = v23[2];
      *&in_column_indices.size[1] = v23[1];
      *&in_column_indices.size[3] = v104;
      *&in_column_indices.flags = v103;
      v105 = v23[3];
      v106 = v23[4];
      v107 = v23[6];
      *&in_column_indices.stride[1] = v23[5];
      *&in_column_indices.stride[3] = v107;
      *&in_column_indices.size[5] = v105;
      *&in_column_indices.size[7] = v106;
      v108 = v23[7];
      v109 = v23[8];
      v110 = v23[10];
      *&in_column_indices.data_type = v23[9];
      *&in_column_indices.table_data_type = v110;
      *&in_column_indices.stride[5] = v108;
      *&in_column_indices.stride[7] = v109;
      v111 = a4[9];
      *&in_row_starts.stride[7] = a4[8];
      *&in_row_starts.data_type = v111;
      *&in_row_starts.table_data_type = a4[10];
      v112 = a4[5];
      *&in_row_starts.size[7] = a4[4];
      *&in_row_starts.stride[1] = v112;
      v113 = a4[7];
      *&in_row_starts.stride[3] = a4[6];
      *&in_row_starts.stride[5] = v113;
      v114 = a4[1];
      *&in_row_starts.flags = *a4;
      *&in_row_starts.size[1] = v114;
      v115 = a4[3];
      *&in_row_starts.size[3] = a4[2];
      *&in_row_starts.size[5] = v115;
      result = BNNSNDArrayFullyConnectedSparsifySparseCOO(&in_dense_shape, &in_column_indices, &in_row_starts, a5, &filter_params, a1, v54, workspace_size, &in_values);
    }

    if (!result)
    {
      return result;
    }

LABEL_27:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v116 = 0;
    return swift_willThrow();
  }

  if (a9)
  {
    v24 = 0;
  }

  else
  {
    v24 = a7;
  }

  v25 = HIDWORD(v19);
  sparse_params.flags = 0;
  if (v20)
  {
    v26 = 0;
  }

  else
  {
    v26 = v19;
  }

  if (v20)
  {
    LODWORD(v25) = 0;
  }

  sparse_params.sparsity_ratio[0] = v26;
  sparse_params.sparsity_ratio[1] = v25;
  v27 = v117;
  if (v20)
  {
    v27 = BNNSTargetSystemGeneric;
  }

  sparse_params.sparsity_type = BNNSSparsityTypeUnstructured;
  sparse_params.target_system = v27;
  if (a13 == 1)
  {
    v28 = a3[9];
    *&in_dense_shape.stride[7] = a3[8];
    *&in_dense_shape.data_type = v28;
    *&in_dense_shape.table_data_type = a3[10];
    v29 = a3[5];
    *&in_dense_shape.size[7] = a3[4];
    *&in_dense_shape.stride[1] = v29;
    v30 = a3[7];
    *&in_dense_shape.stride[3] = a3[6];
    *&in_dense_shape.stride[5] = v30;
    v31 = a3[1];
    *&in_dense_shape.flags = *a3;
    *&in_dense_shape.size[1] = v31;
    v32 = a3[3];
    *&in_dense_shape.size[3] = a3[2];
    *&in_dense_shape.size[5] = v32;
    v33 = *v23;
    v34 = v23[2];
    *&in_column_indices.size[1] = v23[1];
    *&in_column_indices.size[3] = v34;
    *&in_column_indices.flags = v33;
    v35 = v23[3];
    v36 = v23[4];
    v37 = v23[6];
    *&in_column_indices.stride[1] = v23[5];
    *&in_column_indices.stride[3] = v37;
    *&in_column_indices.size[5] = v35;
    *&in_column_indices.size[7] = v36;
    v38 = v23[7];
    v39 = v23[8];
    v40 = v23[10];
    *&in_column_indices.data_type = v23[9];
    *&in_column_indices.table_data_type = v40;
    *&in_column_indices.stride[5] = v38;
    *&in_column_indices.stride[7] = v39;
    v41 = v23[20];
    *&in_row_starts.stride[7] = v23[19];
    *&in_row_starts.data_type = v41;
    *&in_row_starts.table_data_type = v23[21];
    v42 = v23[16];
    *&in_row_starts.size[7] = v23[15];
    *&in_row_starts.stride[1] = v42;
    v43 = v23[18];
    *&in_row_starts.stride[3] = v23[17];
    *&in_row_starts.stride[5] = v43;
    v44 = v23[12];
    *&in_row_starts.flags = v23[11];
    *&in_row_starts.size[1] = v44;
    v45 = v23[14];
    *&in_row_starts.size[3] = v23[13];
    *&in_row_starts.size[5] = v45;
    v46 = a4[8];
    v47 = a4[9];
    v48 = a4[6];
    *&in_values.stride[5] = a4[7];
    *&in_values.stride[7] = v46;
    v49 = a4[10];
    *&in_values.data_type = v47;
    *&in_values.table_data_type = v49;
    v50 = a4[4];
    *&in_values.stride[1] = a4[5];
    *&in_values.stride[3] = v48;
    v51 = a4[1];
    *&in_values.flags = *a4;
    *&in_values.size[1] = v51;
    v52 = a4[2];
    *&in_values.size[5] = a4[3];
    *&in_values.size[7] = v50;
    *&in_values.size[3] = v52;
    result = BNNSNDArrayFullyConnectedSparsifySparseCSR(&in_dense_shape, &in_column_indices, &in_row_starts, &in_values, a5, &sparse_params, a1, v24, workspace_size, 0);
  }

  else
  {
    filter_params.flags = a11;
    filter_params.n_threads = a12;
    filter_params.alloc_memory = a13;
    filter_params.free_memory = a14;
    v73 = a3[9];
    *&in_dense_shape.stride[7] = a3[8];
    *&in_dense_shape.data_type = v73;
    *&in_dense_shape.table_data_type = a3[10];
    v74 = a3[5];
    *&in_dense_shape.size[7] = a3[4];
    *&in_dense_shape.stride[1] = v74;
    v75 = a3[7];
    *&in_dense_shape.stride[3] = a3[6];
    *&in_dense_shape.stride[5] = v75;
    v76 = a3[1];
    *&in_dense_shape.flags = *a3;
    *&in_dense_shape.size[1] = v76;
    v77 = a3[3];
    *&in_dense_shape.size[3] = a3[2];
    *&in_dense_shape.size[5] = v77;
    v78 = *v23;
    v79 = v23[2];
    *&in_column_indices.size[1] = v23[1];
    *&in_column_indices.size[3] = v79;
    *&in_column_indices.flags = v78;
    v80 = v23[3];
    v81 = v23[4];
    v82 = v23[6];
    *&in_column_indices.stride[1] = v23[5];
    *&in_column_indices.stride[3] = v82;
    *&in_column_indices.size[5] = v80;
    *&in_column_indices.size[7] = v81;
    v83 = v23[7];
    v84 = v23[8];
    v85 = v23[10];
    *&in_column_indices.data_type = v23[9];
    *&in_column_indices.table_data_type = v85;
    *&in_column_indices.stride[5] = v83;
    *&in_column_indices.stride[7] = v84;
    v86 = v23[20];
    *&in_row_starts.stride[7] = v23[19];
    *&in_row_starts.data_type = v86;
    *&in_row_starts.table_data_type = v23[21];
    v87 = v23[16];
    *&in_row_starts.size[7] = v23[15];
    *&in_row_starts.stride[1] = v87;
    v88 = v23[18];
    *&in_row_starts.stride[3] = v23[17];
    *&in_row_starts.stride[5] = v88;
    v89 = v23[12];
    *&in_row_starts.flags = v23[11];
    *&in_row_starts.size[1] = v89;
    v90 = v23[14];
    *&in_row_starts.size[3] = v23[13];
    *&in_row_starts.size[5] = v90;
    v91 = a4[8];
    v92 = a4[9];
    v93 = a4[6];
    *&in_values.stride[5] = a4[7];
    *&in_values.stride[7] = v91;
    v94 = a4[10];
    *&in_values.data_type = v92;
    *&in_values.table_data_type = v94;
    v95 = a4[4];
    *&in_values.stride[1] = a4[5];
    *&in_values.stride[3] = v93;
    v96 = a4[1];
    *&in_values.flags = *a4;
    *&in_values.size[1] = v96;
    v97 = a4[2];
    *&in_values.size[5] = a4[3];
    *&in_values.size[7] = v95;
    *&in_values.size[3] = v97;
    result = BNNSNDArrayFullyConnectedSparsifySparseCSR(&in_dense_shape, &in_column_indices, &in_row_starts, &in_values, a5, &sparse_params, a1, v24, workspace_size, &filter_params);
  }

  if (result)
  {
    goto LABEL_27;
  }

  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.SparsityType and conformance BNNS.SparsityType()
{
  result = lazy protocol witness table cache variable for type BNNS.SparsityType and conformance BNNS.SparsityType;
  if (!lazy protocol witness table cache variable for type BNNS.SparsityType and conformance BNNS.SparsityType)
  {
    result = swift_getWitnessTable();
    atomic_store(result, &lazy protocol witness table cache variable for type BNNS.SparsityType and conformance BNNS.SparsityType);
  }

  return result;
}

uint64_t __swift_memcpy12_4(uint64_t result, uint64_t *a2)
{
  v2 = *a2;
  *(result + 8) = *(a2 + 2);
  *result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.SparseParameters(uint64_t a1, int a2)
{
  if (a2 && *(a1 + 12))
  {
    return (*a1 + 1);
  }

  else
  {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNS.SparseParameters(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(result + 8) = 0;
    *result = (a2 - 1);
    if (!a3)
    {
      return result;
    }

    v3 = 1;
  }

  else
  {
    if (!a3)
    {
      return result;
    }

    v3 = 0;
  }

  *(result + 12) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.SparseLayout(uint64_t a1, int a2)
{
  if (!a2)
  {
    return 0;
  }

  if (a2 < 0 && *(a1 + 352))
  {
    return *a1 + 0x80000000;
  }

  v2 = *(a1 + 144);
  if (v2 > 0x80000000FFFFFFFFLL)
  {
    v3 = ~HIDWORD(v2);
  }

  else
  {
    v3 = -1;
  }

  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.SparseLayout(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(result + 344) = 0;
    *(result + 248) = 0u;
    *(result + 232) = 0u;
    *(result + 216) = 0u;
    *(result + 200) = 0u;
    *(result + 184) = 0u;
    *(result + 168) = 0u;
    *(result + 152) = 0u;
    *(result + 136) = 0u;
    *(result + 120) = 0u;
    *(result + 104) = 0u;
    *(result + 88) = 0u;
    *(result + 72) = 0u;
    *(result + 56) = 0u;
    *(result + 40) = 0u;
    *(result + 24) = 0u;
    *(result + 8) = 0u;
    *(result + 328) = 0u;
    *(result + 312) = 0u;
    *(result + 296) = 0u;
    *(result + 280) = 0u;
    *(result + 264) = 0u;
    *result = a2 & 0x7FFFFFFF;
    if (a3 < 0)
    {
      *(result + 352) = 1;
    }
  }

  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2)
      {
        return result;
      }

LABEL_8:
      *(result + 112) = 0u;
      *(result + 128) = 0u;
      *(result + 80) = 0u;
      *(result + 96) = 0u;
      *(result + 48) = 0u;
      *(result + 64) = 0u;
      *(result + 16) = 0u;
      *(result + 32) = 0u;
      *result = 0u;
      *(result + 144) = -a2 << 32;
      *(result + 168) = 0u;
      *(result + 184) = 0u;
      *(result + 200) = 0u;
      *(result + 216) = 0u;
      *(result + 232) = 0u;
      *(result + 248) = 0u;
      *(result + 344) = 0;
      *(result + 152) = 0u;
      result += 152;
      *(result + 112) = 0u;
      *(result + 128) = 0u;
      *(result + 144) = 0u;
      *(result + 160) = 0u;
      *(result + 176) = 0u;
      return result;
    }

    *(result + 352) = 0;
    if (a2)
    {
      goto LABEL_8;
    }
  }

  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.SparseLayout(uint64_t result, int a2)
{
  v2 = *(result + 168);
  v3 = *(result + 320);
  *(result + 144) = *(result + 144);
  *(result + 168) = v2;
  *(result + 320) = v3;
  *(result + 348) = a2 << 31;
  return result;
}

double BNNS.FusedUnaryArithmeticParameters.layerParameters(input:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  *&v18[100] = a2[6];
  *&v18[116] = a2[7];
  *&v18[132] = a2[8];
  *&v18[148] = a2[9];
  *&v18[164] = a2[10];
  *&v18[52] = a2[3];
  *&v18[68] = a2[4];
  *&v18[84] = a2[5];
  *&v18[4] = *a2;
  *&v18[20] = a2[1];
  v6 = *(v3 + 8);
  v7 = *(v3 + 9);
  *&v18[36] = a2[2];
  v8 = swift_slowAlloc();
  v9 = a1[9];
  *(v8 + 128) = a1[8];
  *(v8 + 144) = v9;
  *(v8 + 160) = a1[10];
  v10 = a1[5];
  *(v8 + 64) = a1[4];
  *(v8 + 80) = v10;
  v11 = a1[7];
  *(v8 + 96) = a1[6];
  *(v8 + 112) = v11;
  v12 = a1[1];
  *v8 = *a1;
  *(v8 + 16) = v12;
  v13 = a1[3];
  *(v8 + 32) = a1[2];
  *(v8 + 48) = v13;
  *(v8 + 324) = *&v18[144];
  *(v8 + 340) = *&v18[160];
  *(v8 + 244) = *&v18[64];
  *(v8 + 260) = *&v18[80];
  *(v8 + 276) = *&v18[96];
  *(v8 + 292) = *&v18[112];
  *(v8 + 308) = *&v18[128];
  *(v8 + 180) = *v18;
  *(v8 + 196) = *&v18[16];
  *(v8 + 212) = *&v18[32];
  *v3 = v8;
  *(v8 + 176) = v6;
  *(v8 + 356) = *&v18[176];
  *(v8 + 228) = *&v18[48];
  *(v8 + 360) = v7;
  v14 = dword_1B7E7AC24[*(v3 + 10)];
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  a3[3] = v15;
  a3[4] = &protocol witness table for BNNSLayerParametersArithmetic;
  v16 = swift_allocObject();
  *a3 = v16;
  *(v16 + 16) = v14;
  *(v16 + 24) = v8;
  *(v16 + 32) = 0;
  *(v16 + 36) = vneg_f32(0x3F0000003FLL);
  *(v16 + 44) = 1;
  result = 0.0;
  *(v16 + 48) = 0u;
  *(v16 + 64) = 0u;
  return result;
}

char *BNNS.FusedUnaryArithmeticParameters.init(inputDescriptorType:outputDescriptorType:function:)@<X0>(char *result@<X0>, char *a2@<X1>, char *a3@<X2>, uint64_t a4@<X8>)
{
  v4 = *result;
  v5 = *a2;
  v6 = *a3;
  *a4 = 0;
  *(a4 + 8) = v4;
  *(a4 + 9) = v5;
  *(a4 + 10) = v6;
  return result;
}

uint64_t __swift_memcpy11_8(uint64_t result, uint64_t *a2)
{
  v2 = *a2;
  *(result + 7) = *(a2 + 7);
  *result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.FusedUnaryArithmeticParameters(uint64_t a1, unsigned int a2)
{
  if (!a2)
  {
    return 0;
  }

  if (a2 >= 0xFE && *(a1 + 11))
  {
    return (*a1 + 254);
  }

  v3 = *(a1 + 8);
  v4 = v3 >= 3;
  v5 = v3 - 3;
  if (!v4)
  {
    v5 = -1;
  }

  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedUnaryArithmeticParameters(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFD)
  {
    *(result + 10) = 0;
    *(result + 8) = 0;
    *result = a2 - 254;
    if (a3 >= 0xFE)
    {
      *(result + 11) = 1;
    }
  }

  else
  {
    if (a3 >= 0xFE)
    {
      *(result + 11) = 0;
    }

    if (a2)
    {
      *(result + 8) = a2 + 2;
    }
  }

  return result;
}

uint64_t _ss15withUnsafeBytes2of_q0_x_q0_SWq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  v11 = *(a5 - 8);
  v12 = MEMORY[0x1EEE9AC00](a1);
  v14 = &v18 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  result = v16(v12, v12 + *(*(v15 - 8) + 64), v14);
  if (v8)
  {
    return (*(v11 + 32))(a8, v14, a5);
  }

  return result;
}

uint64_t static vImage.Planar16F.makePixel(_:)(float a1)
{
  v6 = *MEMORY[0x1E69E9840];
  v3 = 0;
  v2 = a1;
  src.data = &v2;
  *&src.height = vdupq_n_s64(1uLL);
  src.rowBytes = 4;
  dest.data = &v3;
  *&dest.height = *&src.height;
  dest.rowBytes = 2;
  vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
  return v3;
}

uint64_t convert<A, B>(src:dest:convertFunc:channelCount:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  if (a5 < 0)
  {
    __break(1u);
  }

  else
  {
    v5 = MEMORY[0x1EEE9AC00](result);
    v15[2] = v6;
    v15[3] = v7;
    v15[4] = v8;
    v15[5] = v9;
    v15[6] = v5;
    v15[7] = v10;
    v15[8] = v11;
    v15[9] = v12;
    return _ss15withUnsafeBytes2of_q0_x_q0_SWq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(v5, partial apply for closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:), v15, v14, MEMORY[0x1E69E73E0], MEMORY[0x1E69E7CA8] + 8, MEMORY[0x1E69E7410], v13);
  }

  return result;
}

uint64_t static vImage.Interleaved16Fx2.makePixel(_:)(float a1, float a2)
{
  v7 = *MEMORY[0x1E69E9840];
  v4 = 0;
  *v3 = a1;
  *&v3[1] = a2;
  src.data = v3;
  *&src.height = xmmword_1B7E76D90;
  src.rowBytes = 8;
  dest.data = &v4;
  *&dest.height = xmmword_1B7E76D90;
  dest.rowBytes = 4;
  vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
  return v4;
}

uint64_t static vImage.Interleaved16Fx4.makePixel(_:)(float a1, float a2, float a3, float a4)
{
  v9 = *MEMORY[0x1E69E9840];
  *v5 = a1;
  *&v5[1] = a2;
  *&v5[2] = a3;
  *&v5[3] = a4;
  src.data = v5;
  *&src.height = xmmword_1B7E7AC90;
  src.rowBytes = 16;
  v6 = 0;
  dest.data = &v6;
  *&dest.height = xmmword_1B7E7AC90;
  dest.rowBytes = 8;
  vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
  return v6;
}

float static vImage.PlanarF.makePixel(_:)(__int16 a1)
{
  v6 = *MEMORY[0x1E69E9840];
  v3 = 0.0;
  v2 = a1;
  src.data = &v2;
  *&src.height = vdupq_n_s64(1uLL);
  src.rowBytes = 2;
  dest.data = &v3;
  *&dest.height = *&src.height;
  dest.rowBytes = 4;
  vImageConvert_Planar16FtoPlanarF(&src, &dest, 0);
  return v3;
}

float static vImage.InterleavedFx2.makePixel(_:)(__int16 a1, __int16 a2)
{
  v7 = *MEMORY[0x1E69E9840];
  v3[0] = a1;
  v3[1] = a2;
  src.data = v3;
  *&src.height = xmmword_1B7E76D90;
  src.rowBytes = 4;
  v4 = 0;
  dest.data = &v4;
  *&dest.height = xmmword_1B7E76D90;
  dest.rowBytes = 8;
  vImageConvert_Planar16FtoPlanarF(&src, &dest, 0);
  return *&v4;
}

float static vImage.InterleavedFx4.makePixel(_:)(__int16 a1, __int16 a2, __int16 a3, __int16 a4)
{
  v9 = *MEMORY[0x1E69E9840];
  v6[0] = 0;
  v6[1] = 0;
  v5[0] = a1;
  v5[1] = a2;
  v5[2] = a3;
  v5[3] = a4;
  src.data = v5;
  *&src.height = xmmword_1B7E7AC90;
  src.rowBytes = 8;
  dest.data = v6;
  *&dest.height = xmmword_1B7E7AC90;
  dest.rowBytes = 16;
  vImageConvert_Planar16FtoPlanarF(&src, &dest, 0);
  return *v6;
}

uint64_t partial apply for closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  v9 = *(v8 + 24);
  v10 = *(v8 + 32);
  v11 = *(v8 + 40);
  v12 = *(v8 + 56);
  v14[2] = *(v8 + 16);
  v14[3] = v9;
  v14[4] = a1;
  v14[5] = a2;
  v15 = v11;
  v16 = v12;
  v17 = *(v8 + 64);
  return _ss22withUnsafeMutableBytes2of_q0_xz_q0_Swq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(v10, partial apply for closure #1 in closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:), v14, v9, MEMORY[0x1E69E73E0], MEMORY[0x1E69E7CA8] + 8, MEMORY[0x1E69E7410], a8);
}

uint64_t partial apply for closure #1 in closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:)(uint64_t result)
{
  if (v1[4])
  {
    v2 = v1[6];
    v4 = v1[8];
    v3 = v1[9];
    v5 = *(*(v1[2] - 8) + 72);
    v7[0] = v1[4];
    v7[1] = 1;
    v7[2] = v2;
    v7[3] = v5;
    if (result)
    {
      v6[0] = result;
      v6[1] = 1;
      v6[2] = v2;
      v6[3] = v4;
      return v3(v7, v6, 0);
    }
  }

  else
  {
    __break(1u);
  }

  __break(1u);
  return result;
}

uint64_t _ss22withUnsafeMutableBytes2of_q0_xz_q0_Swq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  v11 = *(a5 - 8);
  v12 = MEMORY[0x1EEE9AC00](a1);
  v14 = &v18 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  result = v16(v12, v12 + *(*(v15 - 8) + 64), v14);
  if (v8)
  {
    return (*(v11 + 32))(a8, v14, a5);
  }

  return result;
}

uint64_t static BNNS.copy(_:to:filterParameters:)(_OWORD *a1, _OWORD *a2, uint32_t a3, size_t a4, int (__cdecl *a5)(void **, size_t, size_t), void (__cdecl *a6)(void *))
{
  v31 = *MEMORY[0x1E69E9840];
  if (a5 != 1)
  {
    v28.flags = a3;
    v28.n_threads = a4;
    v28.alloc_memory = a5;
    v28.free_memory = a6;
    v17 = a1[9];
    *&src.stride[7] = a1[8];
    *&src.data_type = v17;
    *&src.table_data_type = a1[10];
    v18 = a1[5];
    *&src.size[7] = a1[4];
    *&src.stride[1] = v18;
    v19 = a1[7];
    *&src.stride[3] = a1[6];
    *&src.stride[5] = v19;
    v20 = a1[1];
    *&src.flags = *a1;
    *&src.size[1] = v20;
    v21 = a1[3];
    *&src.size[3] = a1[2];
    *&src.size[5] = v21;
    v22 = a2[9];
    *&dest.stride[7] = a2[8];
    *&dest.data_type = v22;
    *&dest.table_data_type = a2[10];
    v23 = a2[5];
    *&dest.size[7] = a2[4];
    *&dest.stride[1] = v23;
    v24 = a2[7];
    *&dest.stride[3] = a2[6];
    *&dest.stride[5] = v24;
    v25 = a2[1];
    *&dest.flags = *a2;
    *&dest.size[1] = v25;
    v26 = a2[3];
    *&dest.size[3] = a2[2];
    *&dest.size[5] = v26;
    result = BNNSCopy(&dest, &src, &v28);
    if (!result)
    {
      return result;
    }

    goto LABEL_5;
  }

  v6 = a1[9];
  *&src.stride[7] = a1[8];
  *&src.data_type = v6;
  *&src.table_data_type = a1[10];
  v7 = a1[5];
  *&src.size[7] = a1[4];
  *&src.stride[1] = v7;
  v8 = a1[7];
  *&src.stride[3] = a1[6];
  *&src.stride[5] = v8;
  v9 = a1[1];
  *&src.flags = *a1;
  *&src.size[1] = v9;
  v10 = a1[3];
  *&src.size[3] = a1[2];
  *&src.size[5] = v10;
  v11 = a2[9];
  *&dest.stride[7] = a2[8];
  *&dest.data_type = v11;
  *&dest.table_data_type = a2[10];
  v12 = a2[5];
  *&dest.size[7] = a2[4];
  *&dest.stride[1] = v12;
  v13 = a2[7];
  *&dest.stride[3] = a2[6];
  *&dest.stride[5] = v13;
  v14 = a2[1];
  *&dest.flags = *a2;
  *&dest.size[1] = v14;
  v15 = a2[3];
  *&dest.size[3] = a2[2];
  *&dest.size[5] = v15;
  result = BNNSCopy(&dest, &src, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v27 = 0;
    return swift_willThrow();
  }

  return result;
}

uint64_t vImage.PixelBuffer<>.applyMorphology(operation:destination:)(uint64_t a1, uint64_t a2)
{
  v2 = MEMORY[0x1E69588B8];
  v3 = MEMORY[0x1E69588F8];
  v4 = MEMORY[0x1E6958948];
  v5 = MEMORY[0x1E6958968];

  return vImage.PixelBuffer<>.applyMorphology(operation:destination:)(a1, a2, v2, v3, v4, v5);
}

{
  v2 = MEMORY[0x1E69588C8];
  v3 = MEMORY[0x1E6958908];
  v4 = MEMORY[0x1E6958958];
  v5 = MEMORY[0x1E6958978];

  return vImage.PixelBuffer<>.applyMorphology(operation:destination:)(a1, a2, v2, v3, v4, v5);
}

{
  v2 = MEMORY[0x1E69588C0];
  v3 = MEMORY[0x1E6958900];
  v4 = MEMORY[0x1E6958950];
  v5 = MEMORY[0x1E6958970];

  return vImage.PixelBuffer<>.applyMorphology(operation:destination:)(a1, a2, v2, v3, v4, v5);
}

{
  v2 = MEMORY[0x1E69588D0];
  v3 = MEMORY[0x1E6958910];
  v4 = MEMORY[0x1E6958960];
  v5 = MEMORY[0x1E6958980];

  return vImage.PixelBuffer<>.applyMorphology(operation:destination:)(a1, a2, v2, v3, v4, v5);
}

uint64_t vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t *a2, void (*a3)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a4, void (*a5)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  v42 = *MEMORY[0x1E69E9840];
  v15 = *a1;
  v36 = *(a1 + 8);
  v32 = *(a1 + 16);
  v35 = *(a1 + 24);
  v16 = *a2;
  v17 = *v13;
  v38 = *v13;
  v18 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v37[0] = v16;
  type metadata accessor for vImage.PixelBuffer();
  v19 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v18)
  {
    if (v19)
    {
      goto LABEL_4;
    }

LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }

  if (v19 && v18 == v19)
  {
    goto LABEL_15;
  }

LABEL_4:
  v28 = a6;
  v37[3] = v17;
  vImage.PixelBuffer.size.getter(&v38);
  v21 = v38;
  v20 = v39;
  v37[2] = v16;
  vImage.PixelBuffer.size.getter(v37);
  if (v21 != v37[0] || v20 != v37[1])
  {
    goto LABEL_16;
  }

  if (v35 >= 2)
  {
    v22 = v15;
  }

  else
  {
    v22 = v15 & ~(v15 >> 63);
  }

  if (v35 >= 2)
  {
    v23 = v36;
  }

  else
  {
    v23 = v36 & ~(v36 >> 63);
  }

  if ((v23 & v22 & 1) == 0)
  {
LABEL_17:
    __break(1u);
  }

  v37[0] = v17;
  v38 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v39 = v24;
  v40 = v25;
  v41 = v26;
  return closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(&v38, v16, v15, v36, v32, v35, a9, a10, v23, v22, a7, a8, a5, v28, a3, a4, *(a11 + 16), a12, a13);
}

uint64_t vImage.PixelBuffer<>.applyMorphology(operation:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  result = (*(a4 + 32))(*(a3 + 16), a4);
  if (result < 0)
  {
    __break(1u);
  }

  else
  {
    v7 = result;
    if (result)
    {
      v8 = 0;
      do
      {
        v9 = v4;
        vImage.PixelBuffer<>.subscript.getter(v8, &v13);
        vImage.PixelBuffer<>.subscript.getter(v8, &v12);
        AssociatedTypeWitness = swift_getAssociatedTypeWitness();
        swift_getAssociatedConformanceWitness();
        v11 = type metadata accessor for vImage.PixelBuffer();
        vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(a1, &v12, @nonobjc vImageErode_Planar8(_:_:_:_:_:_:_:_:), 0, @nonobjc vImageDilate_Planar8(_:_:_:_:_:_:_:_:), 0, @nonobjc vImageMin_Planar8(_:_:_:_:_:_:_:_:), 0, @nonobjc vImageMax_Planar8(_:_:_:_:_:_:_:_:), 0, v11, AssociatedTypeWitness, MEMORY[0x1E69E7508]);
        v4 = v9;

        ++v8;
      }

      while (v7 != v8);
    }
  }

  return result;
}

{
  result = (*(a4 + 32))(*(a3 + 16), a4);
  if (result < 0)
  {
    __break(1u);
  }

  else
  {
    v7 = result;
    if (result)
    {
      v8 = 0;
      do
      {
        v9 = v4;
        vImage.PixelBuffer<>.subscript.getter(v8, &v13);
        vImage.PixelBuffer<>.subscript.getter(v8, &v12);
        AssociatedTypeWitness = swift_getAssociatedTypeWitness();
        swift_getAssociatedConformanceWitness();
        v11 = type metadata accessor for vImage.PixelBuffer();
        vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(a1, &v12, @nonobjc vImageErode_PlanarF(_:_:_:_:_:_:_:_:), 0, @nonobjc vImageDilate_PlanarF(_:_:_:_:_:_:_:_:), 0, @nonobjc vImageMin_PlanarF(_:_:_:_:_:_:_:_:), 0, @nonobjc vImageMax_PlanarF(_:_:_:_:_:_:_:_:), 0, v11, AssociatedTypeWitness, MEMORY[0x1E69E6448]);
        v4 = v9;

        ++v8;
      }

      while (v7 != v8);
    }
  }

  return result;
}

uint64_t vImage.PixelBuffer<>.applyMorphology(operation:destination:)(uint64_t result, uint64_t a2, uint64_t (*a3)(void *, void *, void, void, uint64_t, uint64_t), uint64_t (*a4)(void *, void *, void, void, uint64_t, uint64_t), uint64_t (*a5)(void *, void *, void, void, void, uint64_t, uint64_t, void), uint64_t (*a6)(void *, void *, void, void, void, uint64_t, uint64_t, void))
{
  v27[4] = *MEMORY[0x1E69E9840];
  v7 = *v6;
  if (!*(*v6 + 16))
  {
    __break(1u);
    goto LABEL_37;
  }

  v8 = *a2;
  if (!*(*a2 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }

  v9 = *result;
  v10 = *(result + 8);
  v11 = *(result + 16);
  v12 = *(result + 24);
  v13 = v7[4];
  v14 = v8[4];
  if (v13)
  {
    if (v14)
    {
      v15 = v13 == v14;
    }

    else
    {
      v15 = 0;
    }

    if (!v15)
    {
      goto LABEL_11;
    }

    __break(1u);
  }

  if (!v14)
  {
    __break(1u);
    return result;
  }

LABEL_11:
  v16 = v7[6];
  if (v16 < 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }

  v17 = v7[5];
  if (v17 < 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }

  if (!v16)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }

  if (!v17)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }

  v18 = v8[6];
  if (v18 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }

  v19 = v8[5];
  if (v19 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }

  if (!v18)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }

  if (!v19)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }

  if (v16 != v18)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }

  if (v17 != v19)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }

  v20 = v10 & ~(v10 >> 63);
  if (v12 >= 2)
  {
    v21 = v9;
  }

  else
  {
    v21 = v9 & ~(v9 >> 63);
  }

  if (v12 >= 2)
  {
    v22 = v10;
  }

  else
  {
    v22 = v10 & ~(v10 >> 63);
  }

  if ((v22 & v21 & 1) == 0)
  {
LABEL_48:
    __break(1u);
  }

  v23 = v7[7];
  v27[0] = v13;
  v27[1] = v17;
  v27[2] = v16;
  v27[3] = v23;
  v24 = v8[7];
  v26[0] = v14;
  v26[1] = v17;
  v26[2] = v16;
  v26[3] = v24;
  if (v12 > 1)
  {
    v25 = v11 + 32;
    if (v12 == 2)
    {
      return a3(v27, v26, 0, 0, v25, v10);
    }

    else
    {
      return a4(v27, v26, 0, 0, v25, v10);
    }
  }

  else if (v12)
  {
    return a6(v27, v26, 0, 0, 0, v20, v9 & ~(v9 >> 63), 0);
  }

  else
  {
    return a5(v27, v26, 0, 0, 0, v20, v9 & ~(v9 >> 63), 0);
  }
}

uint64_t vImage.MorphologyOperation.width.getter()
{
  if (v0[24] >= 2u)
  {
    return *v0;
  }

  else
  {
    return *v0 & ~(*v0 >> 63);
  }
}

uint64_t vImage.MorphologyOperation.height.getter()
{
  if (*(v0 + 24) >= 2u)
  {
    return *(v0 + 8);
  }

  else
  {
    return *(v0 + 8) & ~(*(v0 + 8) >> 63);
  }
}

uint64_t closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5, unsigned __int8 a6, uint64_t (*a7)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a12, void (*a13)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a14, void (*a15)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a16, uint64_t a17, uint64_t a18, uint64_t a19)
{
  v30[4] = *MEMORY[0x1E69E9840];
  type metadata accessor for vImage.PixelBuffer();
  v30[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v30[1] = v20;
  v30[2] = v21;
  v30[3] = v22;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(v30, a3, a4, a5, a6, a7, a8, a1, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18, a19);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, unint64_t a4, unsigned __int8 a5, uint64_t (*a6)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a12, void (*a13)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a14, void (*a15)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a16, uint64_t a17, uint64_t a18, uint64_t a19)
{
  if (a5 > 1u)
  {
    if (a5 == 2)
    {

      if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && a4 >> 62)
      {
        if (MEMORY[0x1B8CB13F0](a4, a19))
        {
          type metadata accessor for _ArrayBuffer();
          swift_getWitnessTable();
          Array.init<A>(_:)();
          v29 = swift_unknownObjectRetain();
          v30 = MEMORY[0x1B8CB1700](v29, a19);

          a13(a8, a1, 0, 0, v30, a9, a10, 0);
          return swift_unknownObjectRelease();
        }

        outlined consume of vImage.MorphologyOperation<B1><A><A1, B1>(a2, a3, a4, 2);
        v25 = 0;
      }

      else
      {
        outlined consume of vImage.MorphologyOperation<B1><A><A1, B1>(a2, a3, a4, 2);
        if (_swift_isClassOrObjCExistentialType())
        {
          v25 = ((*(*(a19 - 8) + 80) + 32) & ~*(*(a19 - 8) + 80)) + (a4 & 0xFFFFFFFFFFFFFF8);
        }

        else
        {
          v25 = a4 + ((*(*(a19 - 8) + 80) + 32) & ~*(*(a19 - 8) + 80));
        }
      }

      if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && a4 >> 62)
      {
        specialized _ArrayBuffer._nonNative.getter(a4);
        swift_unknownObjectRetain();
      }

      else
      {
        _swift_isClassOrObjCExistentialType();
      }

      v27 = a10;
      v28 = a13;
      if (v25)
      {
LABEL_20:
        v28(a8, a1, 0, 0, v25, a9, v27, 0);
        return swift_unknownObjectRelease();
      }
    }

    else
    {

      v26 = a4 >> 62;
      if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && v26)
      {
        if (MEMORY[0x1B8CB13F0](a4, a19))
        {
          type metadata accessor for _ArrayBuffer();
          swift_getWitnessTable();
          Array.init<A>(_:)();
          v31 = swift_unknownObjectRetain();
          v32 = MEMORY[0x1B8CB1700](v31, a19);

          a15(a8, a1, 0, 0, v32, a9, a10, 0);
          return swift_unknownObjectRelease();
        }

        outlined consume of vImage.MorphologyOperation<B1><A><A1, B1>(a2, a3, a4, 3);
        v25 = 0;
      }

      else
      {
        outlined consume of vImage.MorphologyOperation<B1><A><A1, B1>(a2, a3, a4, 3);
        if (_swift_isClassOrObjCExistentialType())
        {
          v25 = ((*(*(a19 - 8) + 80) + 32) & ~*(*(a19 - 8) + 80)) + (a4 & 0xFFFFFFFFFFFFFF8);
        }

        else
        {
          v25 = a4 + ((*(*(a19 - 8) + 80) + 32) & ~*(*(a19 - 8) + 80));
        }
      }

      if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && v26)
      {
        specialized _ArrayBuffer._nonNative.getter(a4);
        swift_unknownObjectRetain();
      }

      else
      {
        _swift_isClassOrObjCExistentialType();
      }

      v28 = a15;
      v27 = a10;
      if (v25)
      {
        goto LABEL_20;
      }
    }

    v25 = (~*(*(a19 - 8) + 80) | 0xFFFFFF00);
    goto LABEL_20;
  }

  if (a5)
  {
    return a11(a8, a1, 0, 0, 0, a9, a10, 0);
  }

  else
  {
    return a6(a8, a1, 0, 0, 0, a9, a10, 0);
  }
}

uint64_t vImage.MorphologyOperation.structuringElement.getter@<X0>(uint64_t *a1@<X8>)
{
  v3 = *(v1 + 24);
  if (v3 >= 2)
  {
    v5 = *(v1 + 8);
    v6 = *(v1 + 16);
    v4 = *v1;
    result = outlined copy of vImage.MorphologyOperation<A><A>(*v1, v5, v6, v3);
  }

  else
  {
    v4 = 0;
    v5 = 0;
    v6 = 0;
  }

  *a1 = v4;
  a1[1] = v5;
  a1[2] = v6;
  return result;
}

uint64_t outlined copy of vImage.MorphologyOperation<A><A>(uint64_t a1, uint64_t a2, uint64_t a3, char a4)
{
  if ((a4 & 0xFE) == 2)
  {
  }

  return result;
}

uint64_t type metadata instantiation function for vImage.MorphologyOperation()
{
  GenericValueMetadataWithLayoutString = swift_cvw_allocateGenericValueMetadataWithLayoutString();
  swift_cvw_instantiateLayoutString();
  return GenericValueMetadataWithLayoutString;
}

__n128 __swift_memcpy25_8(uint64_t a1, uint64_t a2)
{
  result = *a2;
  *(a1 + 9) = *(a2 + 9);
  *a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for vImage.MorphologyOperation(uint64_t a1, unsigned int a2)
{
  if (!a2)
  {
    return 0;
  }

  if (a2 >= 0xFD && *(a1 + 25))
  {
    return (*a1 + 253);
  }

  v3 = *(a1 + 24);
  if (v3 <= 3)
  {
    v4 = -1;
  }

  else
  {
    v4 = v3 ^ 0xFF;
  }

  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.MorphologyOperation(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFC)
  {
    *(result + 8) = 0;
    *(result + 16) = 0;
    *(result + 24) = 0;
    *result = a2 - 253;
    if (a3 >= 0xFD)
    {
      *(result + 25) = 1;
    }
  }

  else
  {
    if (a3 >= 0xFD)
    {
      *(result + 25) = 0;
    }

    if (a2)
    {
      *(result + 24) = -a2;
    }
  }

  return result;
}

uint64_t outlined consume of vImage.MorphologyOperation<B1><A><A1, B1>(uint64_t a1, uint64_t a2, uint64_t a3, char a4)
{
  if ((a4 & 0xFE) == 2)
  {
  }

  return result;
}

unint64_t static BNNS.matrixMultiplicationWorkspaceSize(inputA:transposed:inputB:transposed:output:alpha:filterParameters:)(_OWORD *a1, char a2, _OWORD *a3, char a4, _OWORD *a5, int a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  v51 = *MEMORY[0x1E69E9840];
  if (a8 == 1)
  {
    v10 = a1[9];
    *&inputA.stride[7] = a1[8];
    *&inputA.data_type = v10;
    *&inputA.table_data_type = a1[10];
    v11 = a1[5];
    *&inputA.size[7] = a1[4];
    *&inputA.stride[1] = v11;
    v12 = a1[7];
    *&inputA.stride[3] = a1[6];
    *&inputA.stride[5] = v12;
    v13 = a1[1];
    *&inputA.flags = *a1;
    *&inputA.size[1] = v13;
    v14 = a1[3];
    *&inputA.size[3] = a1[2];
    *&inputA.size[5] = v14;
    v15 = a3[9];
    *&inputB.stride[7] = a3[8];
    *&inputB.data_type = v15;
    *&inputB.table_data_type = a3[10];
    v16 = a3[5];
    *&inputB.size[7] = a3[4];
    *&inputB.stride[1] = v16;
    v17 = a3[7];
    *&inputB.stride[3] = a3[6];
    *&inputB.stride[5] = v17;
    v18 = a3[1];
    *&inputB.flags = *a3;
    *&inputB.size[1] = v18;
    v19 = a3[3];
    *&inputB.size[3] = a3[2];
    *&inputB.size[5] = v19;
    v20 = a5[9];
    *&output.stride[7] = a5[8];
    *&output.data_type = v20;
    *&output.table_data_type = a5[10];
    v21 = a5[5];
    *&output.size[7] = a5[4];
    *&output.stride[1] = v21;
    v22 = a5[7];
    *&output.stride[3] = a5[6];
    *&output.stride[5] = v22;
    v23 = a5[1];
    *&output.flags = *a5;
    *&output.size[1] = v23;
    v24 = a5[3];
    *&output.size[3] = a5[2];
    *&output.size[5] = v24;
    v25 = a2 & 1;
    v26 = a4 & 1;
    v27 = 0;
  }

  else
  {
    v44 = a6;
    v45 = a7;
    v46 = a8;
    v47 = a10;
    v28 = a1[9];
    *&inputA.stride[7] = a1[8];
    *&inputA.data_type = v28;
    *&inputA.table_data_type = a1[10];
    v29 = a1[5];
    *&inputA.size[7] = a1[4];
    *&inputA.stride[1] = v29;
    v30 = a1[7];
    *&inputA.stride[3] = a1[6];
    *&inputA.stride[5] = v30;
    v31 = a1[1];
    *&inputA.flags = *a1;
    *&inputA.size[1] = v31;
    v32 = a1[3];
    *&inputA.size[3] = a1[2];
    *&inputA.size[5] = v32;
    v33 = a3[9];
    *&inputB.stride[7] = a3[8];
    *&inputB.data_type = v33;
    *&inputB.table_data_type = a3[10];
    v34 = a3[5];
    *&inputB.size[7] = a3[4];
    *&inputB.stride[1] = v34;
    v35 = a3[7];
    *&inputB.stride[3] = a3[6];
    *&inputB.stride[5] = v35;
    v36 = a3[1];
    *&inputB.flags = *a3;
    *&inputB.size[1] = v36;
    v37 = a3[3];
    *&inputB.size[3] = a3[2];
    *&inputB.size[5] = v37;
    v38 = a5[9];
    *&output.stride[7] = a5[8];
    *&output.data_type = v38;
    *&output.table_data_type = a5[10];
    v39 = a5[5];
    *&output.size[7] = a5[4];
    *&output.stride[1] = v39;
    v40 = a5[7];
    *&output.stride[3] = a5[6];
    *&output.stride[5] = v40;
    v41 = a5[1];
    *&output.flags = *a5;
    *&output.size[1] = v41;
    v42 = a5[3];
    *&output.size[3] = a5[2];
    *&output.size[5] = v42;
    v25 = a2 & 1;
    v26 = a4 & 1;
    v27 = &v44;
  }

  result = BNNSMatMulWorkspaceSize(v25, v26, a9, &inputA, &inputB, &output, v27);
  if (result > 0x7FFFFFFFFFFFFFFELL)
  {
    return 0;
  }

  return result;
}

uint64_t static BNNS.applyMatrixMultiplication(inputA:transposed:inputB:transposed:output:alpha:workspace:filterParameters:)(_OWORD *a1, char a2, _OWORD *a3, char a4, _OWORD *a5, void *a6, float a7, uint64_t a8, char a9, uint32_t a10, size_t a11, int (__cdecl *a12)(void **, size_t, size_t), void (__cdecl *a13)(void *))
{
  v49 = *MEMORY[0x1E69E9840];
  if (a12 == 1)
  {
    v13 = a1[9];
    *&inputA.stride[7] = a1[8];
    *&inputA.data_type = v13;
    *&inputA.table_data_type = a1[10];
    v14 = a1[5];
    *&inputA.size[7] = a1[4];
    *&inputA.stride[1] = v14;
    v15 = a1[7];
    *&inputA.stride[3] = a1[6];
    *&inputA.stride[5] = v15;
    v16 = a1[1];
    *&inputA.flags = *a1;
    *&inputA.size[1] = v16;
    v17 = a1[3];
    *&inputA.size[3] = a1[2];
    *&inputA.size[5] = v17;
    v18 = a3[9];
    *&inputB.stride[7] = a3[8];
    *&inputB.data_type = v18;
    *&inputB.table_data_type = a3[10];
    v19 = a3[5];
    *&inputB.size[7] = a3[4];
    *&inputB.stride[1] = v19;
    v20 = a3[7];
    *&inputB.stride[3] = a3[6];
    *&inputB.stride[5] = v20;
    v21 = a3[1];
    *&inputB.flags = *a3;
    *&inputB.size[1] = v21;
    v22 = a3[3];
    *&inputB.size[3] = a3[2];
    *&inputB.size[5] = v22;
    v23 = a5[9];
    *&output.stride[7] = a5[8];
    *&output.data_type = v23;
    *&output.table_data_type = a5[10];
    v24 = a5[5];
    *&output.size[7] = a5[4];
    *&output.stride[1] = v24;
    v25 = a5[7];
    *&output.stride[3] = a5[6];
    *&output.stride[5] = v25;
    v26 = a5[1];
    *&output.flags = *a5;
    *&output.size[1] = v26;
    v27 = a5[3];
    if (a9)
    {
      a6 = 0;
    }

    *&output.size[3] = a5[2];
    *&output.size[5] = v27;
    result = BNNSMatMul(a2 & 1, a4 & 1, a7, &inputA, &inputB, &output, a6, 0);
    if (!result)
    {
      return result;
    }

LABEL_9:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *v44 = 0;
    return swift_willThrow();
  }

  v45.flags = a10;
  v45.n_threads = a11;
  v45.alloc_memory = a12;
  v45.free_memory = a13;
  v29 = a1[9];
  *&inputA.stride[7] = a1[8];
  *&inputA.data_type = v29;
  *&inputA.table_data_type = a1[10];
  v30 = a1[5];
  *&inputA.size[7] = a1[4];
  *&inputA.stride[1] = v30;
  v31 = a1[7];
  *&inputA.stride[3] = a1[6];
  *&inputA.stride[5] = v31;
  v32 = a1[1];
  *&inputA.flags = *a1;
  *&inputA.size[1] = v32;
  v33 = a1[3];
  *&inputA.size[3] = a1[2];
  *&inputA.size[5] = v33;
  v34 = a3[9];
  *&inputB.stride[7] = a3[8];
  *&inputB.data_type = v34;
  *&inputB.table_data_type = a3[10];
  v35 = a3[5];
  *&inputB.size[7] = a3[4];
  *&inputB.stride[1] = v35;
  v36 = a3[7];
  *&inputB.stride[3] = a3[6];
  *&inputB.stride[5] = v36;
  v37 = a3[1];
  *&inputB.flags = *a3;
  *&inputB.size[1] = v37;
  v38 = a3[3];
  *&inputB.size[3] = a3[2];
  *&inputB.size[5] = v38;
  v39 = a5[9];
  *&output.stride[7] = a5[8];
  *&output.data_type = v39;
  *&output.table_data_type = a5[10];
  v40 = a5[5];
  *&output.size[7] = a5[4];
  *&output.stride[1] = v40;
  v41 = a5[7];
  *&output.stride[3] = a5[6];
  *&output.stride[5] = v41;
  v42 = a5[1];
  *&output.flags = *a5;
  *&output.size[1] = v42;
  v43 = a5[3];
  if (a9)
  {
    a6 = 0;
  }

  *&output.size[3] = a5[2];
  *&output.size[5] = v43;
  result = BNNSMatMul(a2 & 1, a4 & 1, a7, &inputA, &inputB, &output, a6, &v45);
  if (result)
  {
    goto LABEL_9;
  }

  return result;
}

uint64_t BNNS.FusedQuantizationParameters.axis.setter(uint64_t result, char a2)
{
  *v2 = result;
  *(v2 + 8) = a2 & 1;
  return result;
}

__n128 BNNS.FusedQuantizationParameters.scale.getter@<Q0>(uint64_t a1@<X8>)
{
  v2 = *(v1 + 160);
  *(a1 + 128) = *(v1 + 144);
  *(a1 + 144) = v2;
  *(a1 + 160) = *(v1 + 176);
  *(a1 + 176) = *(v1 + 192);
  v3 = *(v1 + 96);
  *(a1 + 64) = *(v1 + 80);
  *(a1 + 80) = v3;
  v4 = *(v1 + 128);
  *(a1 + 96) = *(v1 + 112);
  *(a1 + 112) = v4;
  v5 = *(v1 + 32);
  *a1 = *(v1 + 16);
  *(a1 + 16) = v5;
  result = *(v1 + 48);
  v7 = *(v1 + 64);
  *(a1 + 32) = result;
  *(a1 + 48) = v7;
  return result;
}

__n128 BNNS.FusedQuantizationParameters.scale.setter(uint64_t a1)
{
  v2 = *(a1 + 144);
  *(v1 + 144) = *(a1 + 128);
  *(v1 + 160) = v2;
  *(v1 + 176) = *(a1 + 160);
  *(v1 + 192) = *(a1 + 176);
  v3 = *(a1 + 80);
  *(v1 + 80) = *(a1 + 64);
  *(v1 + 96) = v3;
  v4 = *(a1 + 112);
  *(v1 + 112) = *(a1 + 96);
  *(v1 + 128) = v4;
  v5 = *(a1 + 16);
  *(v1 + 16) = *a1;
  *(v1 + 32) = v5;
  result = *(a1 + 32);
  v7 = *(a1 + 48);
  *(v1 + 48) = result;
  *(v1 + 64) = v7;
  return result;
}

__n128 BNNS.FusedQuantizationParameters.bias.getter@<Q0>(uint64_t a1@<X8>)
{
  *(a1 + 176) = *(v1 + 376);
  v2 = *(v1 + 216);
  *a1 = *(v1 + 200);
  *(a1 + 16) = v2;
  v3 = *(v1 + 248);
  *(a1 + 32) = *(v1 + 232);
  *(a1 + 48) = v3;
  v4 = *(v1 + 344);
  *(a1 + 128) = *(v1 + 328);
  *(a1 + 144) = v4;
  *(a1 + 160) = *(v1 + 360);
  v5 = *(v1 + 280);
  *(a1 + 64) = *(v1 + 264);
  *(a1 + 80) = v5;
  result = *(v1 + 296);
  v7 = *(v1 + 312);
  *(a1 + 96) = result;
  *(a1 + 112) = v7;
  return result;
}

__n128 BNNS.FusedQuantizationParameters.bias.setter(uint64_t a1)
{
  v2 = *(a1 + 144);
  *(v1 + 328) = *(a1 + 128);
  *(v1 + 344) = v2;
  *(v1 + 360) = *(a1 + 160);
  v3 = *(a1 + 80);
  *(v1 + 264) = *(a1 + 64);
  *(v1 + 280) = v3;
  v4 = *(a1 + 112);
  *(v1 + 296) = *(a1 + 96);
  *(v1 + 312) = v4;
  v5 = *(a1 + 16);
  *(v1 + 200) = *a1;
  *(v1 + 216) = v5;
  result = *(a1 + 32);
  v7 = *(a1 + 48);
  *(v1 + 232) = result;
  *(v1 + 376) = *(a1 + 176);
  *(v1 + 248) = v7;
  return result;
}

__n128 BNNS.FusedQuantizationParameters.layerParameters(input:output:)@<Q0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, int a3@<W2>, uint64_t *a4@<X8>)
{
  v9 = *v4;
  v10 = *(v4 + 8);
  v11 = (v4 + 200);
  v12 = *(v4 + 160);
  v73[8] = *(v4 + 144);
  v73[9] = v12;
  v73[10] = *(v4 + 176);
  v74 = *(v4 + 192);
  v13 = *(v4 + 96);
  v73[4] = *(v4 + 80);
  v73[5] = v13;
  v14 = *(v4 + 128);
  v73[6] = *(v4 + 112);
  v73[7] = v14;
  v15 = *(v4 + 32);
  v73[0] = *(v4 + 16);
  v73[1] = v15;
  v16 = *(v4 + 64);
  v73[2] = *(v4 + 48);
  v73[3] = v16;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg(v73) == 1)
  {
    v71 = 0u;
    v72 = 0u;
    v69 = 0u;
    v70 = 0u;
    v67 = 0u;
    v68 = 0u;
    v65 = 0u;
    v66 = 0u;
    v63 = 0u;
    v64 = 0u;
    v62 = 0u;
  }

  else
  {
    v17 = *(v4 + 160);
    v70 = *(v4 + 144);
    v71 = v17;
    v72 = *(v4 + 176);
    v18 = *(v4 + 96);
    v66 = *(v4 + 80);
    v67 = v18;
    v19 = *(v4 + 128);
    v68 = *(v4 + 112);
    v69 = v19;
    v20 = *(v4 + 32);
    v62 = *(v4 + 16);
    v63 = v20;
    v21 = *(v4 + 64);
    v64 = *(v4 + 48);
    v65 = v21;
  }

  v22 = *(v4 + 344);
  v60[8] = *(v4 + 328);
  v60[9] = v22;
  v60[10] = *(v4 + 360);
  v61 = *(v4 + 376);
  v23 = *(v4 + 280);
  v60[4] = *(v4 + 264);
  v60[5] = v23;
  v24 = *(v4 + 312);
  v60[6] = *(v4 + 296);
  v60[7] = v24;
  v25 = *(v4 + 216);
  v60[0] = *v11;
  v60[1] = v25;
  v26 = *(v4 + 248);
  v60[2] = *(v4 + 232);
  v60[3] = v26;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg(v60) == 1)
  {
    v58 = 0u;
    v59 = 0u;
    v56 = 0u;
    v57 = 0u;
    v54 = 0u;
    v55 = 0u;
    v52 = 0u;
    v53 = 0u;
    v50 = 0u;
    v51 = 0u;
    v49 = 0u;
  }

  else
  {
    v57 = *(v4 + 328);
    v58 = *(v4 + 344);
    v59 = *(v4 + 360);
    v53 = *(v4 + 264);
    v54 = *(v4 + 280);
    v55 = *(v4 + 296);
    v56 = *(v4 + 312);
    v49 = *v11;
    v50 = *(v4 + 216);
    v51 = *(v4 + 232);
    v52 = *(v4 + 248);
  }

  *&v48[100] = a1[6];
  *&v48[116] = a1[7];
  *&v48[132] = a1[8];
  *&v48[148] = a1[9];
  *&v48[164] = a1[10];
  *&v48[52] = a1[3];
  *&v48[68] = a1[4];
  *&v48[84] = a1[5];
  *&v48[4] = *a1;
  *&v48[20] = a1[1];
  if (v9 > 0x40)
  {
    v27 = 1;
  }

  else
  {
    v27 = v10;
  }

  v28 = 1 << v9;
  if (v9 >= 0x40)
  {
    v28 = 0;
  }

  if (v27)
  {
    v29 = 0;
  }

  else
  {
    v29 = v28;
  }

  *&v48[36] = a1[2];
  type metadata accessor for BNNSLayerParametersQuantization(0);
  a4[3] = v30;
  a4[4] = &protocol witness table for BNNSLayerParametersQuantization;
  v31 = swift_allocObject();
  *(v31 + 156) = *&v48[128];
  *(v31 + 172) = *&v48[144];
  *(v31 + 188) = *&v48[160];
  *(v31 + 92) = *&v48[64];
  *(v31 + 108) = *&v48[80];
  *(v31 + 124) = *&v48[96];
  *(v31 + 140) = *&v48[112];
  *(v31 + 28) = *v48;
  *(v31 + 44) = *&v48[16];
  *(v31 + 60) = *&v48[32];
  *(v31 + 76) = *&v48[48];
  v32 = a2[9];
  *(v31 + 336) = a2[8];
  *(v31 + 352) = v32;
  v33 = a2[5];
  *(v31 + 272) = a2[4];
  *(v31 + 288) = v33;
  v34 = a2[7];
  *(v31 + 304) = a2[6];
  *(v31 + 320) = v34;
  v35 = a2[1];
  *(v31 + 208) = *a2;
  *(v31 + 224) = v35;
  v36 = a2[3];
  *(v31 + 240) = a2[2];
  *(v31 + 256) = v36;
  v37 = v70;
  v38 = v71;
  v39 = v68;
  *(v31 + 496) = v69;
  *(v31 + 512) = v37;
  v40 = v72;
  *(v31 + 528) = v38;
  *(v31 + 544) = v40;
  v41 = v66;
  v42 = v67;
  v43 = v64;
  *(v31 + 432) = v65;
  *(v31 + 448) = v41;
  v44 = a2[10];
  *(v31 + 464) = v42;
  *(v31 + 480) = v39;
  v45 = v62;
  v46 = v63;
  *(v31 + 368) = v44;
  *(v31 + 384) = v45;
  *a4 = v31;
  *(v31 + 16) = v29;
  *(v31 + 24) = a3;
  *(v31 + 204) = *&v48[176];
  *(v31 + 400) = v46;
  *(v31 + 416) = v43;
  *(v31 + 688) = v57;
  *(v31 + 704) = v58;
  *(v31 + 720) = v59;
  *(v31 + 624) = v53;
  *(v31 + 640) = v54;
  *(v31 + 656) = v55;
  *(v31 + 672) = v56;
  *(v31 + 560) = v49;
  *(v31 + 576) = v50;
  result = v52;
  *(v31 + 592) = v51;
  *(v31 + 608) = v52;
  return result;
}

double BNNS.FusedQuantizationParameters.init(scale:bias:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  _sSo21BNNSNDArrayDescriptoraSgWOi0_(v9);
  *&v8[135] = *(a1 + 128);
  *&v8[151] = *(a1 + 144);
  *&v8[167] = *(a1 + 160);
  v8[183] = *(a1 + 176);
  *&v8[71] = *(a1 + 64);
  *&v8[87] = *(a1 + 80);
  *&v8[103] = *(a1 + 96);
  *&v8[119] = *(a1 + 112);
  *&v8[7] = *a1;
  *&v8[23] = *(a1 + 16);
  *&v8[39] = *(a1 + 32);
  *&v8[55] = *(a1 + 48);
  *&v7[135] = *(a2 + 128);
  *&v7[151] = *(a2 + 144);
  *&v7[167] = *(a2 + 160);
  *&v7[71] = *(a2 + 64);
  *&v7[87] = *(a2 + 80);
  *&v7[103] = *(a2 + 96);
  *&v7[119] = *(a2 + 112);
  *&v7[7] = *a2;
  *&v7[23] = *(a2 + 16);
  *&v7[39] = *(a2 + 32);
  *&v7[55] = *(a2 + 48);
  *(a3 + 137) = *&v8[128];
  *(a3 + 153) = *&v8[144];
  *(a3 + 169) = *&v8[160];
  *(a3 + 73) = *&v8[64];
  *(a3 + 89) = *&v8[80];
  *(a3 + 105) = *&v8[96];
  *(a3 + 121) = *&v8[112];
  *(a3 + 9) = *v8;
  *(a3 + 25) = *&v8[16];
  *(a3 + 41) = *&v8[32];
  *(a3 + 57) = *&v8[48];
  *(a3 + 321) = *&v7[128];
  *(a3 + 337) = *&v7[144];
  *(a3 + 353) = *&v7[160];
  *(a3 + 257) = *&v7[64];
  *(a3 + 273) = *&v7[80];
  *(a3 + 289) = *&v7[96];
  *(a3 + 305) = *&v7[112];
  *(a3 + 193) = *v7;
  *(a3 + 209) = *&v7[16];
  result = *&v7[32];
  *(a3 + 225) = *&v7[32];
  v7[183] = *(a2 + 176);
  *a3 = 0;
  *(a3 + 8) = 1;
  *(a3 + 185) = *&v8[176];
  *(a3 + 369) = *&v7[176];
  *(a3 + 241) = *&v7[48];
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.argMin(axis:keepDimension:)@<X0>(uint64_t a1@<X8>)
{
  v3 = *v1;

  specialized static BNNSGraph.Builder.calculateStride(_:)(MEMORY[0x1E69E7CC0]);

  v5 = *(v1 + 1);
  result = BNNSGraph.Builder.Tensor.tensorData.getter();
  if (!result)
  {
    goto LABEL_5;
  }

  result = v5;
  if (v5)
  {
    result = _bnns_graph_builder_register_argmin();
    if (!result)
    {
      swift_beginAccess();

      MEMORY[0x1B8CB1340](0xD000000000000026, 0x80000001B7E7DA20);
      swift_endAccess();

      result = 0;
    }

LABEL_5:
    *a1 = v3;
    *(a1 + 8) = v5;
    *(a1 + 16) = xmmword_1B7E77980;
    *(a1 + 32) = result;
    return result;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.argSort(axis:sortOrder:)@<X0>(uint64_t a1@<X8>)
{
  v3 = *v1;

  specialized static BNNSGraph.Builder.calculateStride(_:)(MEMORY[0x1E69E7CC0]);

  v5 = *(v1 + 1);
  result = BNNSGraph.Builder.Tensor.tensorData.getter();
  if (!result)
  {
    goto LABEL_5;
  }

  result = v5;
  if (v5)
  {
    result = _bnns_graph_builder_register_argsort();
    if (!result)
    {
      swift_beginAccess();

      MEMORY[0x1B8CB1340](0xD000000000000023, 0x80000001B7E7DA80);
      swift_endAccess();

      result = 0;
    }

LABEL_5:
    *a1 = v3;
    *(a1 + 8) = v5;
    *(a1 + 16) = xmmword_1B7E77980;
    *(a1 + 32) = result;
    return result;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.cumulativeSum(axis:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X8>)
{
  v13 = *v2;
  v4 = v13;
  v5 = *(a1 + 16);
  v6 = *(a1 + 24);

  LOBYTE(v16) = 1;
  v11 = *(v2 + 8);
  v12 = *(v2 + 24);
  v7 = *(v2 + 8);
  *&v14 = v7;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], &v13, v5, v6, &v16);
  v13 = v4;
  v15 = v12;
  v14 = v11;
  result = BNNSGraph.Builder.Tensor.tensorData.getter();
  if (!result)
  {
    goto LABEL_4;
  }

  if (v7)
  {
    v9 = _bnns_graph_builder_register_cumsum();
    result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v9, 0xD000000000000014, 0x80000001B7E7DAB0);
LABEL_4:
    *a2 = v16;
    v10 = v18;
    *(a2 + 8) = v17;
    *(a2 + 24) = v10;
    return result;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.pad(_:padding:)@<X0>(int *a1@<X0>, uint64_t a2@<X2>, uint64_t a3@<X8>)
{
  v13 = *a1;
  v5 = *(a1 + 4);
  v16 = *v3;
  v6 = v16;
  v7 = *(a2 + 16);
  v8 = *(a2 + 24);

  LOBYTE(v19) = 1;
  v14 = *(v3 + 8);
  v15 = *(v3 + 24);
  v9 = *(v3 + 8);
  *&v17 = v9;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], &v16, v7, v8, &v19);
  v16 = v6;
  v18 = v15;
  v17 = v14;
  result = BNNSGraph.Builder.Tensor.tensorData.getter();
  if (!result)
  {
    goto LABEL_11;
  }

  if ((v5 & 1) == 0)
  {
    if (v9)
    {
      v11 = _bnns_graph_builder_register_constant_pad();
      goto LABEL_10;
    }

    __break(1u);
    goto LABEL_13;
  }

  if (v13)
  {
    if (v9)
    {
      v11 = _bnns_graph_builder_register_replication_pad();
LABEL_10:
      result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v11, 0x61703A5F28646170, 0xEF293A676E696464);
LABEL_11:
      *a3 = v19;
      v12 = v21;
      *(a3 + 8) = v20;
      *(a3 + 24) = v12;
      return result;
    }

LABEL_13:
    __break(1u);
    goto LABEL_14;
  }

  if (v9)
  {
    v11 = _bnns_graph_builder_register_reflection_pad();
    goto LABEL_10;
  }

LABEL_14:
  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.tensorShape()@<X0>(uint64_t a1@<X8>)
{
  v3 = *v1;

  specialized static BNNSGraph.Builder.calculateStride(_:)(MEMORY[0x1E69E7CC0]);

  v5 = *(v1 + 1);
  result = BNNSGraph.Builder.Tensor.tensorData.getter();
  if (!result)
  {
    goto LABEL_5;
  }

  result = v5;
  if (v5)
  {
    result = _bnns_graph_builder_register_shape();
    if (!result)
    {
      swift_beginAccess();

      MEMORY[0x1B8CB1340](0xD000000000000018, 0x80000001B7E7DAD0);
      swift_endAccess();

      result = 0;
    }

LABEL_5:
    *a1 = v3;
    *(a1 + 8) = v5;
    *(a1 + 16) = xmmword_1B7E77980;
    *(a1 + 32) = result;
    return result;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.reshape<A>(to:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t a4@<X8>)
{
  v19 = *v4;
  v8 = v19;
  v9 = *(a1 + 16);
  v10 = *(a1 + 24);
  swift_retain_n();
  LOBYTE(v22) = 1;
  v16 = *(v4 + 8);
  v17 = *(v4 + 24);
  v11 = *(v4 + 8);
  *&v20 = v11;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], &v19, v9, v10, &v22);
  v19 = v8;
  v21 = v17;
  v20 = v16;
  if (!BNNSGraph.Builder.Tensor.tensorData.getter())
  {

    goto LABEL_6;
  }

  v18[0] = v8;
  v18[1] = v11;
  (*(a3 + 24))(&v19, v18, a2, a3);

  v12 = *(&v21 + 1);

  if (!v12)
  {
LABEL_6:
    *a4 = v22;
    v15 = v24;
    *(a4 + 8) = v23;
    *(a4 + 24) = v15;
    return result;
  }

  if (v11)
  {
    v14 = _bnns_graph_builder_register_dynamic_reshape();
    result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v14, 0x2865706168736572, 0xEC000000293A6F74);
    goto LABEL_6;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.reshape(to:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X8>)
{
  v13 = *(v2 + 24);
  v5 = *(v2 + 8);
  v14 = *v2;
  v4 = v14;
  v6 = v5;
  *&v15 = v5;
  v7 = *(a1 + 16);
  v8 = *(a1 + 24);
  v12 = v5;

  LOBYTE(v17) = 1;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], &v14, v7, v8, &v17);
  v14 = v4;
  v16 = v13;
  v15 = v12;
  result = BNNSGraph.Builder.Tensor.tensorData.getter();
  if (!result)
  {
    goto LABEL_4;
  }

  if (v6)
  {
    v10 = _bnns_graph_builder_register_reshape();
    result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v10, 0x2865706168736572, 0xEC000000293A6F74);
LABEL_4:
    *a2 = v17;
    v11 = v19;
    *(a2 + 8) = v18;
    *(a2 + 24) = v11;
    return result;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.transpose(axes:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X8>)
{
  v4 = v2[1];
  v6 = v2[2];
  v7 = v2[3];
  v8 = v2[4];
  v14 = *v2;
  v5 = v14;
  v15 = v4;
  v9 = *(a1 + 16);
  v10 = *(a1 + 24);

  LOBYTE(v19) = 1;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], &v14, v9, v10, &v19);
  v14 = v5;
  v15 = v4;
  v16 = v6;
  v17 = v7;
  v18 = v8;
  result = BNNSGraph.Builder.Tensor.tensorData.getter();
  if (!result)
  {
    goto LABEL_4;
  }

  if (v4)
  {
    v14 = v5;
    v15 = v4;
    v16 = v6;
    v17 = v7;
    v18 = v8;
    BNNSGraph.Builder.Tensor.rank.getter();
    v12 = _bnns_graph_builder_register_transpose();
    result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v12, 0xD000000000000010, 0x80000001B7E7DAF0);
LABEL_4:
    *a2 = v19;
    v13 = v21;
    *(a2 + 8) = v20;
    *(a2 + 24) = v13;
    return result;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.topK(_:axis:findLargest:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  v21 = *v6;
  v9 = v21;
  v11 = *(a6 + 16);
  v10 = *(a6 + 24);
  swift_retain_n();
  LOBYTE(v24) = 1;
  v19 = *(v6 + 8);
  v20 = *(v6 + 24);
  v12 = *(v6 + 8);
  *&v22 = v12;
  v13 = MEMORY[0x1E69E7CC0];
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], &v21, v11, v10, &v24);
  specialized static BNNSGraph.Builder.calculateStride(_:)(v13);

  v21 = v9;
  v23 = v20;
  v22 = v19;
  result = BNNSGraph.Builder.Tensor.tensorData.getter();
  if (!result)
  {
    v17 = 0;
LABEL_6:
    *a1 = v24;
    v18 = v26;
    *(a1 + 8) = v25;
    *(a1 + 24) = v18;
    *a2 = v9;
    *(a2 + 8) = v12;
    *(a2 + 16) = xmmword_1B7E77980;
    *(a2 + 32) = v17;
    return result;
  }

  if (v12)
  {
    v15 = _bnns_graph_builder_register_topk();
    v17 = v16;
    result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v15, 0xD000000000000019, 0x80000001B7E7DB10);
    if (!v17)
    {
      swift_beginAccess();

      MEMORY[0x1B8CB1340](0xD000000000000024, 0x80000001B7E7DB30);
      swift_endAccess();
    }

    goto LABEL_6;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.concatenate<A>(_:axis:)@<X0>(uint64_t a1@<X0>, char *a2@<X2>, unint64_t a3@<X3>, uint64_t a4@<X8>)
{
  v9 = v4[1];
  v19[0] = *v4;
  v19[1] = v9;

  LOBYTE(v20) = 1;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], v19, a2, a3, &v20);
  v19[0] = a1;
  v18[2] = a2;
  v18[3] = a3;
  type metadata accessor for BNNSGraph.Builder.Tensor();
  v10 = type metadata accessor for Array();
  WitnessTable = swift_getWitnessTable();
  result = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF(partial apply for closure #1 in BNNSGraph.Builder.concatenate<A>(_:axis:), v18, v10, MEMORY[0x1E69E6878], MEMORY[0x1E69E73E0], WitnessTable, MEMORY[0x1E69E7410], v12);
  if (v9)
  {
    v14 = result;
    v15 = *(result + 16);
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    {
      specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, v15, 0, v14);
    }

    v16 = _bnns_graph_builder_register_concat();

    result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v16, 0xD000000000000014, 0x80000001B7E7DB60);
    *a4 = v20;
    v17 = v22;
    *(a4 + 8) = v21;
    *(a4 + 24) = v17;
  }

  else
  {
    __break(1u);
  }

  return result;
}

uint64_t closure #1 in BNNSGraph.Builder.concatenate<A>(_:axis:)@<X0>(uint64_t *a1@<X8>)
{
  type metadata accessor for BNNSGraph.Builder.Tensor();
  result = BNNSGraph.Builder.Tensor.tensorData.getter();
  if (result)
  {
    *a1 = result;
  }

  else
  {
    __break(1u);
  }

  return result;
}

uint64_t vImage.PixelBuffer<>.histogram()()
{
  v10 = *MEMORY[0x1E69E9840];
  v1 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v1 + 16) = 256;
  bzero((v1 + 32), 0x800uLL);
  v2 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v2 + 16) = 256;
  bzero((v2 + 32), 0x800uLL);
  v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v3 + 16) = 256;
  bzero((v3 + 32), 0x800uLL);
  v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v4 + 16) = 256;
  bzero((v4 + 32), 0x800uLL);
  v5 = *v0;
  if (!*(*v0 + 16))
  {
    __break(1u);
  }

  v6 = *(v5 + 48);
  *&src.data = *(v5 + 32);
  *&src.width = v6;
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCySpySuGSgGMd);
  inited = swift_initStackObject();
  *(inited + 32) = v1 + 32;
  *(inited + 40) = v2 + 32;
  *(inited + 48) = v3 + 32;
  *(inited + 56) = v4 + 32;
  vImageHistogramCalculation_ARGB8888(&src, (inited + 32), 0);
  swift_setDeallocating();
  return v1;
}

uint64_t vImage.PixelBuffer<>.histogram(binCount:)@<X0>(unint64_t a1@<X0>, unint64_t *a2@<X8>)
{
  v14 = *MEMORY[0x1E69E9840];
  if ((a1 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
  }

  if (a1)
  {
    v5 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(v5 + 16) = a1;
    bzero((v5 + 32), 8 * a1);
    v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(v6 + 16) = a1;
    bzero((v6 + 32), 8 * a1);
    v7 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(v7 + 16) = a1;
    bzero((v7 + 32), 8 * a1);
    v8 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(v8 + 16) = a1;
    bzero((v8 + 32), 8 * a1);
  }

  else
  {
    v8 = MEMORY[0x1E69E7CC0];
    v7 = MEMORY[0x1E69E7CC0];
    v6 = MEMORY[0x1E69E7CC0];
    v5 = MEMORY[0x1E69E7CC0];
  }

  v9 = *v2;
  if (!*(*v2 + 16))
  {
    goto LABEL_9;
  }

  v10 = *(v9 + 48);
  *&src.data = *(v9 + 32);
  *&src.width = v10;
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCySpySuGSgGMd);
  inited = swift_initStackObject();
  *(inited + 32) = v5 + 32;
  *(inited + 40) = v6 + 32;
  *(inited + 48) = v7 + 32;
  *(inited + 56) = v8 + 32;
  if (HIDWORD(a1))
  {
    goto LABEL_10;
  }

  vImageHistogramCalculation_ARGBFFFF(&src, (inited + 32), a1, 0.0, 1.0, 0);
  result = swift_setDeallocating();
  *a2 = a1;
  a2[1] = v5;
  a2[2] = v6;
  a2[3] = v7;
  a2[4] = v8;
  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  v26 = *MEMORY[0x1E69E9840];
  v7 = *v6;
  if (!*(*v6 + 16))
  {
    __break(1u);
    goto LABEL_17;
  }

  v8 = v7[6];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }

  v9 = v7[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }

  if (!v8)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }

  if (!v9)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }

  v10 = *a6;
  if (!*(*a6 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }

  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }

  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }

  if (!v11)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }

  if (!v12)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }

  if (v8 != v11)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }

  if (v9 != v12)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }

  v14 = v7[4];
  v15 = v7[7];
  src.data = v14;
  src.height = v9;
  src.width = v8;
  src.rowBytes = v15;
  v16 = v10[4];
  v17 = v10[7];
  dest.data = v16;
  dest.height = v9;
  v18 = a2 + 32;
  v19 = a3 + 32;
  v20 = a4 + 32;
  v21 = a5 + 32;
  dest.width = v8;
  dest.rowBytes = v17;
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCySPySuGSgGMd);
  inited = swift_initStackObject();
  *(inited + 32) = v18;
  *(inited + 40) = v19;
  *(inited + 48) = v20;
  *(inited + 56) = v21;
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
LABEL_29:
    __break(1u);
  }

  if (HIDWORD(a1))
  {
    goto LABEL_29;
  }

  vImageHistogramSpecification_ARGBFFFF(&src, &dest, 0, (inited + 32), a1, 0.0, 1.0, 0);
  return swift_setDeallocating();
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  v24 = *MEMORY[0x1E69E9840];
  v6 = *v5;
  if (!*(*v5 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }

  v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }

  v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }

  if (!v7)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }

  if (!v8)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }

  v9 = *a5;
  if (!*(*a5 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }

  v10 = v9[6];
  if (v10 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }

  v11 = v9[5];
  if (v11 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }

  if (!v10)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }

  if (!v11)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }

  if (v7 != v10)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }

  if (v8 != v11)
  {
    goto LABEL_25;
  }

  v12 = v6[4];
  v13 = v6[7];
  src.data = v12;
  src.height = v8;
  src.width = v7;
  src.rowBytes = v13;
  v14 = v9[4];
  v15 = v9[7];
  dest.data = v14;
  dest.height = v8;
  v16 = a1 + 32;
  v17 = a2 + 32;
  v18 = a3 + 32;
  v19 = a4 + 32;
  dest.width = v7;
  dest.rowBytes = v15;
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCySPySuGSgGMd);
  inited = swift_initStackObject();
  *(inited + 32) = v16;
  *(inited + 40) = v17;
  *(inited + 48) = v18;
  *(inited + 56) = v19;
  vImageHistogramSpecification_ARGB8888(&src, &dest, (inited + 32), 0);
  return swift_setDeallocating();
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1)
{
  v1 = MEMORY[0x1E69588E8];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, v1);
}

{
  v1 = MEMORY[0x1E69588D8];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, v1);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  v3 = MEMORY[0x1E69588E8];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, a2, a3, v3);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2)
{
  v2 = MEMORY[0x1E69588F0];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, v2);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v4 = MEMORY[0x1E69588F0];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, a3, a4, v4);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t *a2)
{
  v2 = MEMORY[0x1E69588E0];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, v2);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(destination:)(uint64_t a1)
{
  v1 = MEMORY[0x1E6958870];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, v1);
}

{
  v1 = MEMORY[0x1E6958860];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, v1);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(destination:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  v3 = MEMORY[0x1E6958870];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, a2, a3, v3);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, void))
{
  v18 = *MEMORY[0x1E69E9840];
  v8 = *(a2 + 16);
  result = (*(a3 + 32))(v8, a3);
  if (result < 0)
  {
    goto LABEL_8;
  }

  v10 = result;
  if (result)
  {
    v11 = 0;
    while (1)
    {
      v12 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v11 >= *(v12 + 16))
      {
        break;
      }

      v13 = v12 + 32 * v11;
      v15 = *(v13 + 48);
      v16 = *(v13 + 32);

      v17[1] = v15;
      v17[0] = v16;
      result = closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)(v17, a1, v11++, v4, v8, a3, v14, a4);
      if (v10 == v11)
      {
        return result;
      }
    }

    __break(1u);
LABEL_8:
    __break(1u);
  }

  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, void))
{
  v22 = *MEMORY[0x1E69E9840];
  type metadata accessor for vImage.PixelBuffer();
  v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }

  if (*(v15 + 16) <= a3)
  {
    goto LABEL_5;
  }

  v16 = v15 + 32 * a3;
  v19 = *(v16 + 48);
  v20 = *(v16 + 32);

  v21[1] = v19;
  v21[0] = v20;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)(v21, a4, a3, a2, a1, a5, a6, v17, a8);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, void))
{
  type metadata accessor for vImage.PixelBuffer();
  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_10;
  }

  if (*(result + 16) <= a3)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }

  v17 = a1;
  v13 = *(result + 32 * a3 + 48);

  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(result + 16) <= a3)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }

  v14 = *(result + 32 * a3 + 48);

  if (v13 != v14)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }

  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(result + 16) <= a3)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }

  v15 = *(result + 32 * a3 + 40);

  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(result + 16) <= a3)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }

  v16 = *(result + 32 * a3 + 40);

  if (v15 == v16)
  {
    return a9(a5, v17, 0);
  }

LABEL_15:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t (*a2)(void *, void *, void))
{
  v15[4] = *MEMORY[0x1E69E9840];
  v3 = *v2;
  if (!*(*v2 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }

  v4 = v3[6];
  if (v4 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }

  v5 = v3[5];
  if (v5 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }

  if (!v4)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }

  if (!v5)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }

  v6 = *a1;
  if (!*(*a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }

  v7 = v6[6];
  if (v7 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }

  v8 = v6[5];
  if (v8 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }

  if (!v7)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }

  if (!v8)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }

  if (v4 != v7)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }

  if (v5 != v8)
  {
    goto LABEL_25;
  }

  v9 = v3[4];
  v10 = v3[7];
  v15[0] = v9;
  v15[1] = v5;
  v15[2] = v4;
  v15[3] = v10;
  v11 = v6[4];
  v12 = v6[7];
  v14[0] = v11;
  v14[1] = v5;
  v14[2] = v4;
  v14[3] = v12;
  return a2(v15, v14, 0);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(binCount:destination:)(unint64_t a1, uint64_t a2)
{
  v2 = MEMORY[0x1E6958878];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, v2);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t (*a3)(void *, void *, void, unint64_t, void, double, float))
{
  v16[4] = *MEMORY[0x1E69E9840];
  v4 = *v3;
  if (!*(*v3 + 16))
  {
    __break(1u);
    goto LABEL_17;
  }

  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }

  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }

  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }

  if (!v6)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }

  v7 = *a2;
  if (!*(*a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }

  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }

  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }

  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }

  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }

  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }

  if (v6 != v9)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }

  v10 = v4[4];
  v11 = v4[7];
  v16[0] = v10;
  v16[1] = v6;
  v16[2] = v5;
  v16[3] = v11;
  v12 = v7[4];
  v13 = v7[7];
  v15[0] = v12;
  v15[1] = v6;
  v15[2] = v5;
  v15[3] = v13;
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
LABEL_29:
    __break(1u);
  }

  if (HIDWORD(a1))
  {
    goto LABEL_29;
  }

  return a3(v16, v15, 0, a1, 0, 0.0, 1.0);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v4 = MEMORY[0x1E6958878];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, a3, a4, v4);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void, unint64_t, void, double, float))
{
  v20 = *MEMORY[0x1E69E9840];
  v9 = *(a3 + 16);
  result = (*(a4 + 32))(v9, a4);
  if (result < 0)
  {
    goto LABEL_8;
  }

  v11 = result;
  if (result)
  {
    v12 = 0;
    while (1)
    {
      v13 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v12 >= *(v13 + 16))
      {
        break;
      }

      v14 = v13 + 32 * v12;
      v17 = *(v14 + 48);
      v18 = *(v14 + 32);

      v19[1] = v17;
      v19[0] = v18;
      result = closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(v19, a2, v12++, v5, a1, v9, a4, v15, a5);
      if (v11 == v12)
      {
        return result;
      }
    }

    __break(1u);
LABEL_8:
    __break(1u);
  }

  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, unint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, void, unint64_t, void, double, float))
{
  v23 = *MEMORY[0x1E69E9840];
  type metadata accessor for vImage.PixelBuffer();
  v16 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }

  if (*(v16 + 16) <= a3)
  {
    goto LABEL_5;
  }

  v17 = v16 + 32 * a3;
  v20 = *(v17 + 48);
  v21 = *(v17 + 32);

  v22[1] = v20;
  v22[0] = v21;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(v22, a4, a3, a2, a1, a5, a6, a7, v19, a9);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, unint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t (*a10)(uint64_t, uint64_t, void, unint64_t, void, double, float))
{
  type metadata accessor for vImage.PixelBuffer();
  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_12;
  }

  if (*(result + 16) <= a3)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }

  v18 = a5;
  v14 = *(result + 32 * a3 + 48);

  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(result + 16) <= a3)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }

  v15 = *(result + 32 * a3 + 48);

  if (v14 != v15)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }

  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(result + 16) <= a3)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }

  v16 = *(result + 32 * a3 + 40);

  result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(result + 16) <= a3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }

  v17 = *(result + 32 * a3 + 40);

  if (v16 != v17)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }

  if ((a6 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }

  if (!HIDWORD(a6))
  {
    return a10(v18, a1, 0, a6, 0, 0.0, 1.0);
  }

LABEL_19:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.contrastStretch(binCount:destination:)(unint64_t a1, uint64_t *a2)
{
  v2 = MEMORY[0x1E6958868];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, v2);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t *a2, uint64_t (*a3)(_OWORD *, _OWORD *, void, unint64_t, void, double, float))
{
  v11 = *MEMORY[0x1E69E9840];
  v4 = *v3;
  if (!*(*v3 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }

  v5 = *(v4 + 48);
  v10[0] = *(v4 + 32);
  v10[1] = v5;
  v6 = *a2;
  if (!*(*a2 + 16))
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }

  v7 = *(v6 + 48);
  v9[0] = *(v6 + 32);
  v9[1] = v7;
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }

  if (HIDWORD(a1))
  {
    goto LABEL_9;
  }

  return a3(v10, v9, 0, a1, 0, 0.0, 1.0);
}

void *vImage.PixelBuffer<>.histogram()()
{
  result = specialized vImage.PixelBuffer<>._calculateHistogram()(*v0);
  v2 = result[2];
  if (!v2)
  {
    __break(1u);
    goto LABEL_6;
  }

  if (v2 == 1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }

  if (v2 >= 3)
  {
    v3 = result[4];

    return v3;
  }

LABEL_7:
  __break(1u);
  return result;
}

{
  result = specialized vImage.PixelBuffer<>._calculateHistogram()(*v0);
  v2 = result[2];
  if (!v2)
  {
    __break(1u);
    goto LABEL_7;
  }

  if (v2 == 1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }

  if (v2 < 3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }

  if (v2 != 3)
  {
    v3 = result[4];

    return v3;
  }

LABEL_9:
  __break(1u);
  return result;
}

void *specialized vImage.PixelBuffer<>._calculateHistogram()(unint64_t a1)
{
  v32 = *MEMORY[0x1E69E9840];
  v2 = MEMORY[0x1E69E7CC0];
  src.data = MEMORY[0x1E69E7CC0];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 3, 0);
  data = src.data;
  v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v4 + 16) = 256;
  bzero((v4 + 32), 0x800uLL);
  v6 = data[2];
  v5 = data[3];
  v7 = v6 + 1;
  if (v6 >= v5 >> 1)
  {
LABEL_24:
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v5 > 1), v7, 1);
    data = src.data;
  }

  data[2] = v7;
  data[v6 + 4] = v4;
  v8 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v8 + 16) = 256;
  bzero((v8 + 32), 0x800uLL);
  src.data = data;
  v10 = data[2];
  v9 = data[3];
  if (v10 >= v9 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v9 > 1), v10 + 1, 1);
    data = src.data;
  }

  data[2] = v10 + 1;
  data[v10 + 4] = v8;
  v11 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v11 + 16) = 256;
  bzero((v11 + 32), 0x800uLL);
  src.data = data;
  v13 = data[2];
  v12 = data[3];
  if (v13 >= v12 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v12 > 1), v13 + 1, 1);
    data = src.data;
  }

  v6 = 0;
  data[2] = v13 + 1;
  v5 = &data[v13];
  *(v5 + 32) = v11;
  v4 = *(a1 + 16);
  v7 = a1 + 48;
  do
  {
    if (v4)
    {
      src.data = v2;
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
      v14 = src.data;
      a1 = *(src.data + 2);
      v15 = 4 * a1;
      v16 = v7;
      v17 = v4;
      do
      {
        v18 = *(v16 - 1);
        v19 = *v16;
        src.data = v14;
        v20 = v14[3];
        if (a1++ >= v20 >> 1)
        {
          v27 = v19;
          v29 = v18;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v20 > 1, a1, 1);
          v19 = v27;
          v18 = v29;
          v14 = src.data;
        }

        v14[2] = a1;
        v5 = &v14[v15];
        *(v5 + 32) = v18;
        *(v5 + 48) = v19;
        v15 += 4;
        v16 = (v16 + 40);
        --v17;
      }

      while (v17);
    }

    else
    {
      a1 = v2[2];
      v14 = v2;
    }

    if (v6 >= a1)
    {
      __break(1u);
LABEL_23:
      __break(1u);
      goto LABEL_24;
    }

    v22 = &v14[4 * v6];
    v28 = *(v22 + 3);
    v30 = *(v22 + 2);

    *&src.width = v28;
    *&src.data = v30;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    {
      data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    }

    v5 = data[2];
    if (v6 >= v5)
    {
      goto LABEL_23;
    }

    v23 = data + 4;
    v24 = data[v6 + 4];
    isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v6 + 4] = v24;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      v24 = specialized _ArrayBuffer._consumeAndCreateNew()(v24);
      v23[v6] = v24;
    }

    vImageHistogramCalculation_Planar8(&src, v24 + 4, 0);
    v23[v6++] = v24;
  }

  while (v6 != 3);
  return data;
}

{
  v35 = *MEMORY[0x1E69E9840];
  v2 = MEMORY[0x1E69E7CC0];
  src.data = MEMORY[0x1E69E7CC0];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
  data = src.data;
  v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v4 + 16) = 256;
  bzero((v4 + 32), 0x800uLL);
  v6 = data[2];
  v5 = data[3];
  v7 = v6 + 1;
  if (v6 >= v5 >> 1)
  {
LABEL_26:
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v5 > 1), v7, 1);
    data = src.data;
  }

  data[2] = v7;
  data[v6 + 4] = v4;
  v8 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v8 + 16) = 256;
  bzero((v8 + 32), 0x800uLL);
  src.data = data;
  v10 = data[2];
  v9 = data[3];
  if (v10 >= v9 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v9 > 1), v10 + 1, 1);
    data = src.data;
  }

  data[2] = v10 + 1;
  data[v10 + 4] = v8;
  v11 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v11 + 16) = 256;
  bzero((v11 + 32), 0x800uLL);
  src.data = data;
  v13 = data[2];
  v12 = data[3];
  if (v13 >= v12 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v12 > 1), v13 + 1, 1);
    data = src.data;
  }

  data[2] = v13 + 1;
  data[v13 + 4] = v11;
  v14 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(v14 + 16) = 256;
  bzero((v14 + 32), 0x800uLL);
  src.data = data;
  v16 = data[2];
  v15 = data[3];
  if (v16 >= v15 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v15 > 1), v16 + 1, 1);
    data = src.data;
  }

  v6 = 0;
  data[2] = v16 + 1;
  v5 = &data[v16];
  *(v5 + 32) = v14;
  v4 = *(a1 + 16);
  v7 = a1 + 48;
  do
  {
    if (v4)
    {
      src.data = v2;
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
      v17 = src.data;
      a1 = *(src.data + 2);
      v18 = 4 * a1;
      v19 = v7;
      v20 = v4;
      do
      {
        v21 = *(v19 - 1);
        v22 = *v19;
        src.data = v17;
        v23 = v17[3];
        if (a1++ >= v23 >> 1)
        {
          v30 = v22;
          v32 = v21;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v23 > 1, a1, 1);
          v22 = v30;
          v21 = v32;
          v17 = src.data;
        }

        v17[2] = a1;
        v5 = &v17[v18];
        *(v5 + 32) = v21;
        *(v5 + 48) = v22;
        v18 += 4;
        v19 = (v19 + 40);
        --v20;
      }

      while (v20);
    }

    else
    {
      a1 = v2[2];
      v17 = v2;
    }

    if (v6 >= a1)
    {
      __break(1u);
LABEL_25:
      __break(1u);
      goto LABEL_26;
    }

    v25 = &v17[4 * v6];
    v31 = *(v25 + 3);
    v33 = *(v25 + 2);

    *&src.width = v31;
    *&src.data = v33;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    {
      data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    }

    v5 = data[2];
    if (v6 >= v5)
    {
      goto LABEL_25;
    }

    v26 = data + 4;
    v27 = data[v6 + 4];
    isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v6 + 4] = v27;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      v27 = specialized _ArrayBuffer._consumeAndCreateNew()(v27);
      v26[v6] = v27;
    }

    vImageHistogramCalculation_Planar8(&src, v27 + 4, 0);
    v26[v6++] = v27;
  }

  while (v6 != 4);
  return data;
}

void *vImage.PixelBuffer<>._calculateHistogram()(uint64_t a1, uint64_t a2)
{
  v23 = *MEMORY[0x1E69E9840];
  v3 = *(a1 + 16);
  v4 = *(a2 + 32);
  v5 = v4(v3);
  if (v5 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }

  v6 = v5;
  data = MEMORY[0x1E69E7CC0];
  if (v5)
  {
    src.data = MEMORY[0x1E69E7CC0];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v5, 0);
    data = src.data;
    do
    {
      v8 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(v8 + 16) = 256;
      bzero((v8 + 32), 0x800uLL);
      src.data = data;
      v10 = data[2];
      v9 = data[3];
      if (v10 >= v9 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v9 > 1), v10 + 1, 1);
        data = src.data;
      }

      data[2] = v10 + 1;
      data[v10 + 4] = v8;
      --v6;
    }

    while (v6);
  }

  v11 = (v4)(v3, a2);
  if (v11 < 0)
  {
LABEL_21:
    __break(1u);
  }

  v12 = v11;
  if (v11)
  {
    v13 = 0;
    while (1)
    {
      v14 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v13 >= *(v14 + 16))
      {
        break;
      }

      v15 = v14 + 32 * v13;
      v20 = *(v15 + 48);
      v21 = *(v15 + 32);

      *&src.width = v20;
      *&src.data = v21;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
      }

      if (v13 >= data[2])
      {
        goto LABEL_19;
      }

      v16 = &data[v13];
      v17 = v16[4];
      isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
      v16[4] = v17;
      if ((isUniquelyReferenced_nonNull_native & 1) == 0)
      {
        v17 = specialized _ArrayBuffer._consumeAndCreateNew()(v17);
        v16[4] = v17;
      }

      ++v13;
      vImageHistogramCalculation_Planar8(&src, v17 + 4, 0);
      v16[4] = v17;
      if (v12 == v13)
      {
        return data;
      }
    }

    __break(1u);
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }

  return data;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, void **a4)
{
  v8 = *a4;
  v9 = *v4;
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCySaySuGGMd);
  inited = swift_initStackObject();
  *(inited + 16) = xmmword_1B7E76E00;
  *(inited + 32) = a1;
  *(inited + 40) = a2;
  *(inited + 48) = a3;

  specialized vImage.PixelBuffer<>._specifyHistogram(_:destination:)(inited, v8, v9);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledNameV2(&_sSaySuGMd);
  return swift_arrayDestroy();
}

uint64_t specialized vImage.PixelBuffer<>._specifyHistogram(_:destination:)(uint64_t a1, void *a2, void *a3)
{
  v51 = *MEMORY[0x1E69E9840];
  v3 = a3[2];
  if (!v3)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }

  v4 = a3[6];
  if (v4 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }

  v5 = a3[5];
  if (v5 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }

  if (!v4)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }

  if (!v5)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }

  v6 = a2[2];
  if (!v6)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }

  v7 = a2[6];
  if (v7 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }

  v8 = a2[5];
  if (v8 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }

  if (!v7)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }

  if (!v8)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }

  if (v4 != v7)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
  }

  if (v5 != v8)
  {
    goto LABEL_41;
  }

  v9 = 0;
  v37 = a1 + 32;
  v38 = *(a1 + 16);
  v39 = (a2 + 6);
  v40 = (a3 + 6);
  v10 = MEMORY[0x1E69E7CC0];
  do
  {
    src.data = v10;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v3, 0);
    data = src.data;
    v12 = *(src.data + 2);
    v13 = 4 * v12;
    v14 = v40;
    v15 = v3;
    do
    {
      v16 = *(v14 - 1);
      v17 = *v14;
      src.data = data;
      v18 = data[3];
      v19 = v12 + 1;
      if (v12 >= v18 >> 1)
      {
        v41 = v17;
        v45 = v16;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v18 > 1, v12 + 1, 1);
        v17 = v41;
        v16 = v45;
        data = src.data;
      }

      data[2] = v19;
      v20 = &data[v13];
      *(v20 + 2) = v16;
      *(v20 + 3) = v17;
      v13 += 4;
      v14 = (v14 + 40);
      v12 = v19;
      --v15;
    }

    while (v15);
    if (v9 > v19 - 1)
    {
      __break(1u);
LABEL_28:
      __break(1u);
LABEL_29:
      __break(1u);
      goto LABEL_30;
    }

    v21 = v9 + 1;
    v22 = &data[4 * v9 + 4];
    v42 = v22[1];
    v46 = *v22;

    *&src.width = v42;
    *&src.data = v46;
    v23 = v10;
    dest.data = v10;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v6, 0);
    v24 = dest.data;
    v25 = *(dest.data + 2);
    v26 = 4 * v25;
    v27 = v39;
    v28 = v6;
    do
    {
      v29 = *(v27 - 1);
      v30 = *v27;
      dest.data = v24;
      v31 = v24[3];
      v32 = v25 + 1;
      if (v25 >= v31 >> 1)
      {
        v43 = v30;
        v47 = v29;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v31 > 1, v25 + 1, 1);
        v30 = v43;
        v29 = v47;
        v24 = dest.data;
      }

      v24[2] = v32;
      v33 = &v24[v26];
      *(v33 + 2) = v29;
      *(v33 + 3) = v30;
      v26 += 4;
      v27 = (v27 + 40);
      v25 = v32;
      --v28;
    }

    while (v28);
    if (v9 > v32 - 1)
    {
      goto LABEL_28;
    }

    v34 = &v24[4 * v9 + 4];
    v44 = v34[1];
    v48 = *v34;

    *&dest.width = v44;
    *&dest.data = v48;
    if (v9 == v38)
    {
      goto LABEL_29;
    }

    v35 = *(v37 + 8 * v9);

    vImageHistogramSpecification_Planar8(&src, &dest, (v35 + 32), 0);

    ++v9;
    v10 = v23;
  }

  while (v21 != 3);
  return result;
}

{
  v51 = *MEMORY[0x1E69E9840];
  v3 = a3[2];
  if (!v3)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }

  v4 = a3[6];
  if (v4 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }

  v5 = a3[5];
  if (v5 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }

  if (!v4)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }

  if (!v5)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }

  v6 = a2[2];
  if (!v6)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }

  v7 = a2[6];
  if (v7 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }

  v8 = a2[5];
  if (v8 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }

  if (!v7)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }

  if (!v8)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }

  if (v4 != v7)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
  }

  if (v5 != v8)
  {
    goto LABEL_41;
  }

  v9 = 0;
  v37 = a1 + 32;
  v38 = *(a1 + 16);
  v39 = (a2 + 6);
  v40 = (a3 + 6);
  v10 = MEMORY[0x1E69E7CC0];
  do
  {
    src.data = v10;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v3, 0);
    data = src.data;
    v12 = *(src.data + 2);
    v13 = 4 * v12;
    v14 = v40;
    v15 = v3;
    do
    {
      v16 = *(v14 - 1);
      v17 = *v14;
      src.data = data;
      v18 = data[3];
      v19 = v12 + 1;
      if (v12 >= v18 >> 1)
      {
        v41 = v17;
        v45 = v16;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v18 > 1, v12 + 1, 1);
        v17 = v41;
        v16 = v45;
        data = src.data;
      }

      data[2] = v19;
      v20 = &data[v13];
      *(v20 + 2) = v16;
      *(v20 + 3) = v17;
      v13 += 4;
      v14 = (v14 + 40);
      v12 = v19;
      --v15;
    }

    while (v15);
    if (v9 > v19 - 1)
    {
      __break(1u);
LABEL_28:
      __break(1u);
LABEL_29:
      __break(1u);
      goto LABEL_30;
    }

    v21 = v9 + 1;
    v22 = &data[4 * v9 + 4];
    v42 = v22[1];
    v46 = *v22;

    *&src.width = v42;
    *&src.data = v46;
    v23 = v10;
    dest.data = v10;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v6, 0);
    v24 = dest.data;
    v25 = *(dest.data + 2);
    v26 = 4 * v25;
    v27 = v39;
    v28 = v6;
    do
    {
      v29 = *(v27 - 1);
      v30 = *v27;
      dest.data = v24;
      v31 = v24[3];
      v32 = v25 + 1;
      if (v25 >= v31 >> 1)
      {
        v43 = v30;
        v47 = v29;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v31 > 1, v25 + 1, 1);
        v30 = v43;
        v29 = v47;
        v24 = dest.data;
      }

      v24[2] = v32;
      v33 = &v24[v26];
      *(v33 + 2) = v29;
      *(v33 + 3) = v30;
      v26 += 4;
      v27 = (v27 + 40);
      v25 = v32;
      --v28;
    }

    while (v28);
    if (v9 > v32 - 1)
    {
      goto LABEL_28;
    }

    v34 = &v24[4 * v9 + 4];
    v44 = v34[1];
    v48 = *v34;

    *&dest.width = v44;
    *&dest.data = v48;
    if (v9 == v38)
    {
      goto LABEL_29;
    }

    v35 = *(v37 + 8 * v9);

    vImageHistogramSpecification_Planar8(&src, &dest, (v35 + 32), 0);

    ++v9;
    v10 = v23;
  }

  while (v21 != 4);
  return result;
}

uint64_t vImage.PixelBuffer<>._specifyHistogram(_:destination:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4)
{
  v21 = *MEMORY[0x1E69E9840];
  v8 = *a2;
  v9 = *v4;
  v19[3] = *v4;
  vImage.PixelBuffer.size.getter(&v20);
  v10 = *&v20.data;
  v19[2] = v8;
  vImage.PixelBuffer.size.getter(v19);
  if (v10 != v19[0] || *(&v10 + 1) != v19[1])
  {
LABEL_13:
    __break(1u);
LABEL_14:
    __break(1u);
  }

  result = (*(a4 + 32))(*(a3 + 16), a4);
  if (result < 0)
  {
    goto LABEL_14;
  }

  v13 = result;
  if (result)
  {
    v14 = 0;
    while (1)
    {
      v19[0] = v9;
      v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v14 >= *(v15 + 16))
      {
        break;
      }

      v16 = v15 + 32 * v14;
      v17 = *(v16 + 48);
      v18 = *(v16 + 32);

      *&v20.width = v17;
      *&v20.data = v18;
      result = closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:destination:)(&v20, v8, v14++, a1);
      if (v13 == v14)
      {
        return result;
      }
    }

    __break(1u);
    goto LABEL_13;
  }

  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, void **a5)
{
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCySaySuGGMd);
  inited = swift_initStackObject();
  *(inited + 32) = a1;
  *(inited + 16) = xmmword_1B7E770E0;
  *(inited + 40) = a2;
  *(inited + 48) = a3;
  *(inited + 56) = a4;
  v12 = *a5;
  v13 = *v5;

  specialized vImage.PixelBuffer<>._specifyHistogram(_:destination:)(inited, v12, v13);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledNameV2(&_sSaySuGMd);
  return swift_arrayDestroy();
}

uint64_t closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:destination:)(const vImage_Buffer *a1, uint64_t a2, unint64_t a3, uint64_t a4)
{
  v14 = *MEMORY[0x1E69E9840];
  type metadata accessor for vImage.PixelBuffer();
  v7 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_6;
  }

  if (*(v7 + 16) <= a3)
  {
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }

  v8 = v7 + 32 * a3;
  v11 = *(v8 + 48);
  v12 = *(v8 + 32);

  *&dest.width = v11;
  *&dest.data = v12;
  if (*(a4 + 16) <= a3)
  {
    goto LABEL_7;
  }

  v9 = *(a4 + 8 * a3 + 32);

  vImageHistogramSpecification_Planar8(a1, &dest, (v9 + 32), 0);
}

void *vImage.PixelBuffer<>.histogram(binCount:)(unint64_t a1)
{
  result = specialized vImage.PixelBuffer<>._calculateHistogram(binCount:)(a1, *v1);
  v4 = result[2];
  if (!v4)
  {
    __break(1u);
    goto LABEL_6;
  }

  if (v4 == 1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }

  if (v4 >= 3)
  {

    return a1;
  }

LABEL_7:
  __break(1u);
  return result;
}

void *specialized vImage.PixelBuffer<>._calculateHistogram(binCount:)(unint64_t a1, unint64_t a2)
{
  v34 = *MEMORY[0x1E69E9840];
  src.data = MEMORY[0x1E69E7CC0];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 3, 0);
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_35:
    __break(1u);
  }

  else
  {
    data = src.data;
    v2 = 8 * a1;
    if (a1)
    {
      v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(v4 + 16) = a1;
      bzero((v4 + 32), 8 * a1);
    }

    else
    {
      v4 = MEMORY[0x1E69E7CC0];
    }

    v5 = data[2];
    v8 = data[3];
    v3 = v5 + 1;
    if (v5 < v8 >> 1)
    {
      goto LABEL_6;
    }
  }

  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v8 > 1), v3, 1);
  data = src.data;
LABEL_6:
  data[2] = v3;
  data[v5 + 4] = v4;
  if (a1)
  {
    v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(v10 + 16) = a1;
    bzero((v10 + 32), v2);
    v3 = data[2];
  }

  else
  {
    v10 = MEMORY[0x1E69E7CC0];
  }

  src.data = data;
  v11 = data[3];
  v5 = v3 + 1;
  if (v3 >= v11 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v11 > 1), v3 + 1, 1);
    data = src.data;
  }

  data[2] = v5;
  data[v3 + 4] = v10;
  v12 = MEMORY[0x1E69E7CC0];
  if (a1)
  {
    v13 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(v13 + 16) = a1;
    bzero((v13 + 32), v2);
  }

  else
  {
    v13 = MEMORY[0x1E69E7CC0];
  }

  src.data = data;
  v15 = data[2];
  v14 = data[3];
  v2 = v15 + 1;
  if (v15 >= v14 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v14 > 1), v15 + 1, 1);
    data = src.data;
  }

  data[2] = v2;
  data[v15 + 4] = v13;
  if (HIDWORD(a1))
  {
    __break(1u);
  }

  v3 = 0;
  v4 = *(a2 + 16);
  v8 = a2 + 48;
  v28 = (a2 + 48);
  do
  {
    if (v4)
    {
      v5 = v12;
      src.data = v12;
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
      v16 = src.data;
      a2 = *(src.data + 2);
      v17 = 4 * a2;
      v18 = v28;
      v2 = v4;
      do
      {
        v19 = *(v18 - 1);
        v20 = *v18;
        src.data = v16;
        v21 = v16[3];
        if (a2++ >= v21 >> 1)
        {
          v29 = v20;
          v31 = v19;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v21 > 1, a2, 1);
          v20 = v29;
          v19 = v31;
          v16 = src.data;
        }

        v16[2] = a2;
        v8 = &v16[v17];
        *(v8 + 32) = v19;
        *(v8 + 48) = v20;
        v17 += 4;
        v18 = (v18 + 40);
        --v2;
      }

      while (v2);
      v12 = v5;
      if (v3 >= a2)
      {
LABEL_33:
        __break(1u);
LABEL_34:
        __break(1u);
        goto LABEL_35;
      }
    }

    else
    {
      a2 = v12[2];
      v16 = v12;
      if (v3 >= a2)
      {
        goto LABEL_33;
      }
    }

    v23 = &v16[4 * v3];
    v30 = *(v23 + 3);
    v32 = *(v23 + 2);

    *&src.width = v30;
    *&src.data = v32;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    {
      data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    }

    v8 = data[2];
    if (v3 >= v8)
    {
      goto LABEL_34;
    }

    v24 = data + 4;
    v25 = data[v3 + 4];
    isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v3 + 4] = v25;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      v25 = specialized _ArrayBuffer._consumeAndCreateNew()(v25);
      v24[v3] = v25;
    }

    vImageHistogramCalculation_PlanarF(&src, v25 + 4, a1, 0.0, 1.0, 0);
    v24[v3++] = v25;
    v2 = v3;
  }

  while (v3 != 3);
  return data;
}

{
  v37 = *MEMORY[0x1E69E9840];
  src.data = MEMORY[0x1E69E7CC0];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
  }

  else
  {
    data = src.data;
    v2 = 8 * a1;
    if (a1)
    {
      v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(v4 + 16) = a1;
      bzero((v4 + 32), 8 * a1);
    }

    else
    {
      v4 = MEMORY[0x1E69E7CC0];
    }

    v5 = data[2];
    v8 = data[3];
    v3 = v5 + 1;
    if (v5 < v8 >> 1)
    {
      goto LABEL_6;
    }
  }

  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v8 > 1), v3, 1);
  data = src.data;
LABEL_6:
  data[2] = v3;
  data[v5 + 4] = v4;
  if (a1)
  {
    v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(v10 + 16) = a1;
    bzero((v10 + 32), v2);
    v3 = data[2];
  }

  else
  {
    v10 = MEMORY[0x1E69E7CC0];
  }

  src.data = data;
  v11 = data[3];
  if (v3 >= v11 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v11 > 1), v3 + 1, 1);
    data = src.data;
  }

  data[2] = v3 + 1;
  data[v3 + 4] = v10;
  if (a1)
  {
    v12 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(v12 + 16) = a1;
    bzero((v12 + 32), v2);
  }

  else
  {
    v12 = MEMORY[0x1E69E7CC0];
  }

  src.data = data;
  v14 = data[2];
  v13 = data[3];
  v15 = v14 + 1;
  if (v14 >= v13 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v13 > 1), v14 + 1, 1);
    data = src.data;
  }

  data[2] = v15;
  data[v14 + 4] = v12;
  v16 = MEMORY[0x1E69E7CC0];
  if (a1)
  {
    v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(v4 + 16) = a1;
    bzero((v4 + 32), v2);
    v15 = data[2];
  }

  else
  {
    v4 = MEMORY[0x1E69E7CC0];
  }

  src.data = data;
  v17 = data[3];
  v2 = v15 + 1;
  if (v15 >= v17 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v17 > 1), v15 + 1, 1);
    data = src.data;
  }

  data[2] = v2;
  data[v15 + 4] = v4;
  if (HIDWORD(a1))
  {
    __break(1u);
  }

  v3 = 0;
  v5 = *(a2 + 16);
  v8 = a2 + 48;
  v31 = (a2 + 48);
  do
  {
    if (v5)
    {
      v4 = a1;
      v18 = v16;
      src.data = v16;
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v5, 0);
      v19 = src.data;
      a2 = *(src.data + 2);
      v20 = 4 * a2;
      v21 = v31;
      v2 = v5;
      do
      {
        v22 = *(v21 - 1);
        v23 = *v21;
        src.data = v19;
        v24 = v19[3];
        if (a2++ >= v24 >> 1)
        {
          v32 = v23;
          v34 = v22;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v24 > 1, a2, 1);
          v23 = v32;
          v22 = v34;
          v19 = src.data;
        }

        v19[2] = a2;
        v8 = &v19[v20];
        *(v8 + 32) = v22;
        *(v8 + 48) = v23;
        v20 += 4;
        v21 = (v21 + 40);
        --v2;
      }

      while (v2);
      v16 = v18;
      a1 = v4;
      if (v3 >= a2)
      {
LABEL_38:
        __break(1u);
LABEL_39:
        __break(1u);
        goto LABEL_40;
      }
    }

    else
    {
      a2 = v16[2];
      v19 = v16;
      if (v3 >= a2)
      {
        goto LABEL_38;
      }
    }

    v26 = &v19[4 * v3];
    v33 = *(v26 + 3);
    v35 = *(v26 + 2);

    *&src.width = v33;
    *&src.data = v35;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    {
      data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    }

    v8 = data[2];
    if (v3 >= v8)
    {
      goto LABEL_39;
    }

    v27 = data + 4;
    v28 = data[v3 + 4];
    isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v3 + 4] = v28;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      v28 = specialized _ArrayBuffer._consumeAndCreateNew()(v28);
      v27[v3] = v28;
    }

    vImageHistogramCalculation_PlanarF(&src, v28 + 4, a1, 0.0, 1.0, 0);
    v27[v3++] = v28;
    v2 = v3;
  }

  while (v3 != 4);
  return data;
}

void *vImage.PixelBuffer<>._calculateHistogram(binCount:)(unint64_t a1, uint64_t a2, uint64_t a3)
{
  v27 = *MEMORY[0x1E69E9840];
  v5 = *(a2 + 16);
  v6 = *(a3 + 32);
  v7 = v6(v5, a3);
  if (v7 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }

  v8 = v7;
  data = MEMORY[0x1E69E7CC0];
  if (v7)
  {
    src.data = MEMORY[0x1E69E7CC0];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
    if ((a1 & 0x8000000000000000) != 0)
    {
LABEL_29:
      __break(1u);
      goto LABEL_30;
    }

    v23 = v6;
    data = src.data;
    v10 = MEMORY[0x1E69E7CC0];
    do
    {
      if (a1)
      {
        v11 = static Array._allocateBufferUninitialized(minimumCapacity:)();
        *(v11 + 16) = a1;
        bzero((v11 + 32), 8 * a1);
      }

      else
      {
        v11 = v10;
      }

      src.data = data;
      v13 = data[2];
      v12 = data[3];
      if (v13 >= v12 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v12 > 1), v13 + 1, 1);
        data = src.data;
      }

      data[2] = v13 + 1;
      data[v13 + 4] = v11;
      --v8;
    }

    while (v8);
    v6 = v23;
  }

  v14 = v6(v5, a3);
  if (v14 < 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }

  v15 = v14;
  if (v14)
  {
    if ((a1 & 0x8000000000000000) != 0)
    {
LABEL_30:
      __break(1u);
LABEL_31:
      __break(1u);
    }

    if (HIDWORD(a1))
    {
      goto LABEL_31;
    }

    v16 = 0;
    while (1)
    {
      v17 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v16 >= *(v17 + 16))
      {
        break;
      }

      v18 = v17 + 32 * v16;
      v24 = *(v18 + 48);
      v25 = *(v18 + 32);

      *&src.width = v24;
      *&src.data = v25;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
      }

      if (v16 >= data[2])
      {
        goto LABEL_26;
      }

      v19 = &data[v16];
      v20 = v19[4];
      isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
      v19[4] = v20;
      if ((isUniquelyReferenced_nonNull_native & 1) == 0)
      {
        v20 = specialized _ArrayBuffer._consumeAndCreateNew()(v20);
        v19[4] = v20;
      }

      ++v16;
      vImageHistogramCalculation_PlanarF(&src, v20 + 4, a1, 0.0, 1.0, 0);
      v19[4] = v20;
      if (v15 == v16)
      {
        return data;
      }
    }

    __break(1u);
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }

  return data;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, void **a5)
{
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCySaySuGGMd);
  inited = swift_initStackObject();
  *(inited + 16) = xmmword_1B7E76E00;
  *(inited + 32) = a2;
  *(inited + 40) = a3;
  *(inited + 48) = a4;
  v12 = *a5;
  v13 = *v5;

  specialized vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(inited, a1, v12, v13);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledNameV2(&_sSaySuGMd);
  return swift_arrayDestroy();
}

uint64_t specialized vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(uint64_t a1, unint64_t a2, void *a3, void *a4)
{
  v51 = *MEMORY[0x1E69E9840];
  v4 = a4[2];
  if (!v4)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }

  v5 = a4[6];
  if (v5 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }

  v6 = a4[5];
  if (v6 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }

  if (!v5)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }

  if (!v6)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }

  v7 = a3[2];
  if (!v7)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }

  v8 = a3[6];
  if (v8 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }

  if (!v8)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }

  v9 = a3[5];
  if (v9 < 1)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }

  if (v5 != v8)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }

  if (v6 != v9)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }

  v10 = a2;
  if ((a2 & 0x8000000000000000) != 0)
  {
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
  }

  if (HIDWORD(a2))
  {
    goto LABEL_43;
  }

  v11 = 0;
  v37 = a1 + 32;
  v38 = *(a1 + 16);
  v39 = (a3 + 6);
  v40 = (a4 + 6);
  do
  {
    src.data = MEMORY[0x1E69E7CC0];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
    data = src.data;
    v13 = *(src.data + 2);
    v14 = 4 * v13;
    v15 = v40;
    v16 = v4;
    do
    {
      v17 = *(v15 - 1);
      v18 = *v15;
      src.data = data;
      v19 = data[3];
      v20 = v13 + 1;
      if (v13 >= v19 >> 1)
      {
        v41 = v18;
        v45 = v17;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v19 > 1, v13 + 1, 1);
        v18 = v41;
        v17 = v45;
        data = src.data;
      }

      data[2] = v20;
      v21 = &data[v14];
      *(v21 + 2) = v17;
      *(v21 + 3) = v18;
      v14 += 4;
      v15 = (v15 + 40);
      v13 = v20;
      --v16;
    }

    while (v16);
    if (v11 > v20 - 1)
    {
      __break(1u);
LABEL_29:
      __break(1u);
LABEL_30:
      __break(1u);
      goto LABEL_31;
    }

    v22 = v11 + 1;
    v23 = &data[4 * v11 + 4];
    v42 = v23[1];
    v46 = *v23;

    *&src.width = v42;
    *&src.data = v46;
    dest.data = MEMORY[0x1E69E7CC0];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
    v24 = dest.data;
    v25 = *(dest.data + 2);
    v26 = 4 * v25;
    v27 = v39;
    v28 = v7;
    do
    {
      v29 = *(v27 - 1);
      v30 = *v27;
      dest.data = v24;
      v31 = v24[3];
      v32 = v25 + 1;
      if (v25 >= v31 >> 1)
      {
        v43 = v30;
        v47 = v29;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v31 > 1, v25 + 1, 1);
        v30 = v43;
        v29 = v47;
        v24 = dest.data;
      }

      v24[2] = v32;
      v33 = &v24[v26];
      *(v33 + 2) = v29;
      *(v33 + 3) = v30;
      v26 += 4;
      v27 = (v27 + 40);
      v25 = v32;
      --v28;
    }

    while (v28);
    if (v11 > v32 - 1)
    {
      goto LABEL_29;
    }

    v34 = &v24[4 * v11 + 4];
    v44 = v34[1];
    v48 = *v34;

    *&dest.width = v44;
    *&dest.data = v48;
    if (v11 == v38)
    {
      goto LABEL_30;
    }

    v35 = *(v37 + 8 * v11);

    vImageHistogramSpecification_PlanarF(&src, &dest, 0, (v35 + 32), v10, 0.0, 1.0, 0);

    ++v11;
  }

  while (v22 != 3);
  return result;
}

{
  v51 = *MEMORY[0x1E69E9840];
  v4 = a4[2];
  if (!v4)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }

  v5 = a4[6];
  if (v5 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }

  v6 = a4[5];
  if (v6 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }

  if (!v5)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }

  if (!v6)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }

  v7 = a3[2];
  if (!v7)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }

  v8 = a3[6];
  if (v8 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }

  if (!v8)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }

  v9 = a3[5];
  if (v9 < 1)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }

  if (v5 != v8)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }

  if (v6 != v9)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }

  v10 = a2;
  if ((a2 & 0x8000000000000000) != 0)
  {
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
  }

  if (HIDWORD(a2))
  {
    goto LABEL_43;
  }

  v11 = 0;
  v37 = a1 + 32;
  v38 = *(a1 + 16);
  v39 = (a3 + 6);
  v40 = (a4 + 6);
  do
  {
    src.data = MEMORY[0x1E69E7CC0];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
    data = src.data;
    v13 = *(src.data + 2);
    v14 = 4 * v13;
    v15 = v40;
    v16 = v4;
    do
    {
      v17 = *(v15 - 1);
      v18 = *v15;
      src.data = data;
      v19 = data[3];
      v20 = v13 + 1;
      if (v13 >= v19 >> 1)
      {
        v41 = v18;
        v45 = v17;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v19 > 1, v13 + 1, 1);
        v18 = v41;
        v17 = v45;
        data = src.data;
      }

      data[2] = v20;
      v21 = &data[v14];
      *(v21 + 2) = v17;
      *(v21 + 3) = v18;
      v14 += 4;
      v15 = (v15 + 40);
      v13 = v20;
      --v16;
    }

    while (v16);
    if (v11 > v20 - 1)
    {
      __break(1u);
LABEL_29:
      __break(1u);
LABEL_30:
      __break(1u);
      goto LABEL_31;
    }

    v22 = v11 + 1;
    v23 = &data[4 * v11 + 4];
    v42 = v23[1];
    v46 = *v23;

    *&src.width = v42;
    *&src.data = v46;
    dest.data = MEMORY[0x1E69E7CC0];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
    v24 = dest.data;
    v25 = *(dest.data + 2);
    v26 = 4 * v25;
    v27 = v39;
    v28 = v7;
    do
    {
      v29 = *(v27 - 1);
      v30 = *v27;
      dest.data = v24;
      v31 = v24[3];
      v32 = v25 + 1;
      if (v25 >= v31 >> 1)
      {
        v43 = v30;
        v47 = v29;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v31 > 1, v25 + 1, 1);
        v30 = v43;
        v29 = v47;
        v24 = dest.data;
      }

      v24[2] = v32;
      v33 = &v24[v26];
      *(v33 + 2) = v29;
      *(v33 + 3) = v30;
      v26 += 4;
      v27 = (v27 + 40);
      v25 = v32;
      --v28;
    }

    while (v28);
    if (v11 > v32 - 1)
    {
      goto LABEL_29;
    }

    v34 = &v24[4 * v11 + 4];
    v44 = v34[1];
    v48 = *v34;

    *&dest.width = v44;
    *&dest.data = v48;
    if (v11 == v38)
    {
      goto LABEL_30;
    }

    v35 = *(v37 + 8 * v11);

    vImageHistogramSpecification_PlanarF(&src, &dest, 0, (v35 + 32), v10, 0.0, 1.0, 0);

    ++v11;
  }

  while (v22 != 4);
  return result;
}

uint64_t vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(uint64_t a1, unint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5)
{
  v23 = *MEMORY[0x1E69E9840];
  v9 = *a3;
  v10 = *v5;
  v21[3] = *v5;
  vImage.PixelBuffer.size.getter(&v22);
  v11 = *&v22.data;
  v21[2] = v9;
  vImage.PixelBuffer.size.getter(v21);
  if (v11 != v21[0] || *(&v11 + 1) != v21[1])
  {
LABEL_13:
    __break(1u);
LABEL_14:
    __break(1u);
  }

  result = (*(a5 + 32))(*(a4 + 16), a5);
  if (result < 0)
  {
    goto LABEL_14;
  }

  v14 = result;
  if (result)
  {
    v15 = 0;
    while (1)
    {
      v21[0] = v10;
      v16 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v15 >= *(v16 + 16))
      {
        break;
      }

      v17 = v16 + 32 * v15;
      v19 = *(v17 + 48);
      v20 = *(v17 + 32);

      *&v22.width = v19;
      *&v22.data = v20;
      result = closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(&v22, v9, v15++, a1, a2);
      if (v14 == v15)
      {
        return result;
      }
    }

    __break(1u);
    goto LABEL_13;
  }

  return result;
}

void *vImage.PixelBuffer<>.histogram(binCount:)@<X0>(unint64_t a1@<X0>, unint64_t *a2@<X8>)
{
  result = specialized vImage.PixelBuffer<>._calculateHistogram(binCount:)(a1, *v2);
  v6 = result[2];
  if (!v6)
  {
    __break(1u);
    goto LABEL_7;
  }

  if (v6 == 1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }

  if (v6 < 3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }

  if (v6 != 3)
  {
    v7 = result[4];
    v8 = result[5];
    v9 = result[6];
    v10 = result[7];

    *a2 = a1;
    a2[1] = v7;
    a2[2] = v8;
    a2[3] = v9;
    a2[4] = v10;
    return result;
  }

LABEL_9:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, void **a6)
{
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCySaySuGGMd);
  inited = swift_initStackObject();
  *(inited + 32) = a2;
  *(inited + 16) = xmmword_1B7E770E0;
  *(inited + 40) = a3;
  *(inited + 48) = a4;
  *(inited + 56) = a5;
  v14 = *a6;
  v15 = *v6;

  specialized vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(inited, a1, v14, v15);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledNameV2(&_sSaySuGMd);
  return swift_arrayDestroy();
}

uint64_t closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(const vImage_Buffer *a1, uint64_t a2, unint64_t a3, uint64_t a4, unint64_t a5)
{
  v16 = *MEMORY[0x1E69E9840];
  type metadata accessor for vImage.PixelBuffer();
  v9 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_8;
  }

  if (*(v9 + 16) <= a3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }

  v10 = v9 + 32 * a3;
  v13 = *(v10 + 48);
  v14 = *(v10 + 32);

  *&dest.width = v13;
  *&dest.data = v14;
  if (*(a4 + 16) <= a3)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }

  if ((a5 & 0x8000000000000000) != 0)
  {
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
  }

  if (HIDWORD(a5))
  {
    goto LABEL_11;
  }

  v11 = *(a4 + 8 * a3 + 32);

  vImageHistogramSpecification_PlanarF(a1, &dest, 0, (v11 + 32), a5, 0.0, 1.0, 0);
}

uint64_t BNNSGraph.Builder.Tensor.rnn(initialHiddenStates:inputHiddenWeight:hiddenHiddenWeight:bias:direction:activation:outputSequence:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  v30 = *v10;
  v12 = v30;
  v14 = *(a10 + 16);
  v13 = *(a10 + 24);
  destructiveProjectEnumData for vImage(v14);
  swift_retain_n();
  LOBYTE(v33) = 1;
  v25 = *(v10 + 24);
  v26 = *(v10 + 8);
  v15 = *(v10 + 8);
  *&v31 = v15;
  v16 = MEMORY[0x1E69E7CC0];
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], &v30, v14, v13, &v33);
  v27 = v12;
  *&v28 = v26;
  LOBYTE(v30) = 1;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, v16, &v27, v14, v13, &v30);
  v27 = v12;
  v29 = v25;
  v28 = v26;
  if (!BNNSGraph.Builder.Tensor.tensorData.getter())
  {

    goto LABEL_11;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v27);

  v17 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v17 || (, BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v27), , v18 = BNNSGraph.Builder.Tensor.tensorData.getter(), , !v18))
  {

LABEL_10:

    goto LABEL_11;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v27);

  v19 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v19)
  {

    goto LABEL_10;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v27);

  v20 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v20)
  {
LABEL_11:
    *a1 = v33;
    *(a1 + 8) = v34;
    *(a1 + 24) = v35;
    *a2 = v30;
    v23 = v32;
    *(a2 + 8) = v31;
    *(a2 + 24) = v23;
    return result;
  }

  if (v15)
  {
    _bnns_graph_builder_register_rnn();
    v22 = v28;
    BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v27, 0xD000000000000067, 0x80000001B7E7DB80);
    result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v22, 0xD000000000000067, 0x80000001B7E7DB80);
    goto LABEL_11;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.gru(initialHiddenStates:inputHiddenWeight:hiddenHiddenWeight:bias:inputBias:direction:activation:recurrentActivation:applyResetGateAfterMatMul:outputSequence:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  v33 = *v12;
  v14 = v33;
  v16 = *(a12 + 16);
  v15 = *(a12 + 24);
  destructiveProjectEnumData for vImage(v16);
  swift_retain_n();
  LOBYTE(v36) = 1;
  v27 = *(v12 + 24);
  v29 = *(v12 + 8);
  v17 = *(v12 + 8);
  *&v34 = v17;
  v18 = MEMORY[0x1E69E7CC0];
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], &v33, v16, v15, &v36);
  v30 = v14;
  *&v31 = v29;
  LOBYTE(v33) = 1;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, v18, &v30, v16, v15, &v33);
  v30 = v14;
  v32 = v27;
  v31 = v29;
  if (!BNNSGraph.Builder.Tensor.tensorData.getter())
  {

    goto LABEL_12;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v30);

  v19 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v19 || (, BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v30), , v20 = BNNSGraph.Builder.Tensor.tensorData.getter(), , !v20) || (, BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v30), , v21 = BNNSGraph.Builder.Tensor.tensorData.getter(), , !v21))
  {

LABEL_11:

    goto LABEL_12;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v30);

  v22 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v22)
  {

    goto LABEL_11;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v30);

  v23 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v23)
  {
LABEL_12:
    *a1 = v36;
    *(a1 + 8) = v37;
    *(a1 + 24) = v38;
    *a2 = v33;
    v26 = v35;
    *(a2 + 8) = v34;
    *(a2 + 24) = v26;
    return result;
  }

  if (v17)
  {
    _bnns_graph_builder_register_gru();
    v25 = v31;
    BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v30, 0xD00000000000009FLL, 0x80000001B7E7DBF0);
    result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v25, 0xD00000000000009FLL, 0x80000001B7E7DBF0);
    goto LABEL_12;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.lstm(initialHiddenStates:initialCellStates:inputHiddenWeight:hiddenHiddenWeight:bias:direction:activation:recurrentActivation:cellActivation:outputSequence:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  v39 = *v14;
  v15 = v39;
  v16 = *(a14 + 16);
  v17 = *(a14 + 24);
  destructiveProjectEnumData for vImage(v16);
  swift_retain_n();
  LOBYTE(v42) = 1;
  v28 = *(v14 + 24);
  v32 = *(v14 + 8);
  v18 = *(v14 + 8);
  *&v40 = v18;
  v19 = MEMORY[0x1E69E7CC0];
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], &v39, v16, v17, &v42);
  v36 = v15;
  *&v37 = v32;
  LOBYTE(v39) = 1;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, v19, &v36, v16, v17, &v39);
  v33 = v15;
  *&v34 = v32;
  LOBYTE(v36) = 1;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, v19, &v33, v16, v17, &v36);
  v33 = v15;
  v34 = v32;
  v35 = v28;
  if (!BNNSGraph.Builder.Tensor.tensorData.getter())
  {

    goto LABEL_12;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v33);

  v20 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v20 || (, BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v33), , v21 = BNNSGraph.Builder.Tensor.tensorData.getter(), , !v21) || (, BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v33), , v22 = BNNSGraph.Builder.Tensor.tensorData.getter(), , !v22))
  {

LABEL_11:

    goto LABEL_12;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v33);

  v23 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v23)
  {

    goto LABEL_11;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v33);

  v24 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v24)
  {
LABEL_12:
    v27 = v44;
    *(a1 + 8) = v43;
    *a1 = v42;
    *(a1 + 24) = v27;
    *a2 = v39;
    *(a2 + 8) = v40;
    *(a2 + 24) = v41;
    *a3 = v36;
    *(a3 + 8) = v37;
    *(a3 + 24) = v38;
    return result;
  }

  if (v18)
  {
    _bnns_graph_builder_register_lstm();
    v26 = v34;
    BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v33, 0xD00000000000009DLL, 0x80000001B7E7DC90);
    BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v26, 0xD00000000000009DLL, 0x80000001B7E7DC90);
    result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(*(&v26 + 1), 0xD00000000000009DLL, 0x80000001B7E7DC90);
    goto LABEL_12;
  }

  __break(1u);
  return result;
}

uint64_t BNNSGraph.Builder.Tensor.bidirectionalLSTM(initialHiddenStates:initialCellStates:inputHiddenWeight:hiddenHiddenWeight:bias:inputHiddenWeightBack:hiddenHiddenWeightBack:biasBack:activation:recurrentActivation:cellActivation:outputSequence:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  v44 = *v16;
  v17 = v44;
  v18 = *(a16 + 16);
  v19 = *(a16 + 24);
  destructiveProjectEnumData for vImage(v18);
  swift_retain_n();
  LOBYTE(v47) = 1;
  v33 = *(v16 + 24);
  v37 = *(v16 + 8);
  v20 = *(v16 + 8);
  *&v45 = v20;
  v21 = MEMORY[0x1E69E7CC0];
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, MEMORY[0x1E69E7CC0], &v44, v18, v19, &v47);
  v41 = v17;
  *&v42 = v37;
  LOBYTE(v44) = 1;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, v21, &v41, v18, v19, &v44);
  v38 = v17;
  *&v39 = v37;
  LOBYTE(v41) = 1;
  BNNSGraph.Builder.Tensor.init(_:intent:dataType:shape:builder:)(0, 0xE000000000000000, 0x100000000uLL, v21, &v38, v18, v19, &v41);
  v38 = v17;
  v39 = v37;
  v40 = v33;
  if (!BNNSGraph.Builder.Tensor.tensorData.getter())
  {

    goto LABEL_15;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v38);

  v22 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v22 || (, BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v38), , v23 = BNNSGraph.Builder.Tensor.tensorData.getter(), , !v23) || (, BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v38), , v24 = BNNSGraph.Builder.Tensor.tensorData.getter(), , !v24) || (, BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v38), , v25 = BNNSGraph.Builder.Tensor.tensorData.getter(), , !v25) || (, BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v38), , v26 = BNNSGraph.Builder.Tensor.tensorData.getter(), , !v26) || (, BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v38), , v27 = BNNSGraph.Builder.Tensor.tensorData.getter(), , !v27))
  {

LABEL_14:

    goto LABEL_15;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v38);

  v28 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v28)
  {

    goto LABEL_14;
  }

  BNNSGraph.Builder.Tensor.graphBuilderTensor(_:)(&v38);

  v29 = BNNSGraph.Builder.Tensor.tensorData.getter();

  if (!v29)
  {
LABEL_15:
    v32 = v49;
    *(a1 + 8) = v48;
    *a1 = v47;
    *(a1 + 24) = v32;
    *a2 = v44;
    *(a2 + 8) = v45;
    *(a2 + 24) = v46;
    *a3 = v41;
    *(a3 + 8) = v42;
    *(a3 + 24) = v43;
    return result;
  }

  if (v20)
  {
    _bnns_graph_builder_register_bidirectional_lstm();
    v31 = v39;
    BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v38, 0xD0000000000000D6, 0x80000001B7E7DD30);
    BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(v31, 0xD0000000000000D6, 0x80000001B7E7DD30);
    result = BNNSGraph.Builder.Tensor.setTensorData(_:operationName:)(*(&v31 + 1), 0xD0000000000000D6, 0x80000001B7E7DD30);
    goto LABEL_15;
  }

  __break(1u);
  return result;
}

void vImage.PixelBuffer.size.getter(void *a1@<X8>)
{
  v2 = *v1;
  if (!*(*v1 + 16))
  {
    __break(1u);
    goto LABEL_10;
  }

  v3 = *(v2 + 48);
  if (v3 < 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }

  v4 = *(v2 + 40);
  if (v4 < 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }

  if (v3)
  {
    v5 = v4 == 0;
  }

  else
  {
    v5 = 1;
  }

  if (!v5)
  {
    *a1 = v3;
    a1[1] = v4;
    return;
  }

LABEL_12:
  __break(1u);
}

void *vImage.PixelBuffer<>.init(size:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t *a4@<X8>)
{
  v7 = *a1;
  v8 = *(a1 + 8);
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
  v9 = swift_allocObject();
  *(v9 + 16) = xmmword_1B7E76D90;
  v10 = (*(a3 + 16))(a2, a3);
  if (v10 < 1)
  {
    __break(1u);
  }

  else if (!HIDWORD(v10))
  {
    v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v7, v8);
    v13 = v12;
    v15 = v14;
    v17 = v16;
    type metadata accessor for vImage.BufferReference();
    result = swift_allocObject();
    result[2] = v11;
    result[3] = v13;
    result[4] = v15;
    result[5] = v17;
    *(v9 + 32) = v11;
    *(v9 + 40) = v13;
    *(v9 + 48) = v15;
    *(v9 + 56) = v17;
    *(v9 + 64) = result;
    *a4 = v9;
    return result;
  }

  __break(1u);

  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.vImageBuffer.getter()
{
  if (*(*v0 + 16))
  {
    return *(*v0 + 32);
  }

  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer.width.getter()
{
  if (!*(*v0 + 16))
  {
    __break(1u);
    goto LABEL_5;
  }

  result = *(*v0 + 48);
  if (result < 0)
  {
LABEL_5:
    __break(1u);
  }

  return result;
}

uint64_t vImage.PixelBuffer.height.getter()
{
  if (!*(*v0 + 16))
  {
    __break(1u);
    goto LABEL_5;
  }

  result = *(*v0 + 40);
  if (result < 0)
  {
LABEL_5:
    __break(1u);
  }

  return result;
}

uint64_t _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  v11 = *(a5 - 8);
  v12 = MEMORY[0x1EEE9AC00](a1);
  v14 = &v17 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  result = v15(v12, v14);
  if (v8)
  {
    return (*(v11 + 32))(a8, v14, a5);
  }

  return result;
}

uint64_t vImage.PixelBuffer<>.vImageBuffers.getter()
{
  v1 = *v0;
  v2 = *(*v0 + 16);
  result = MEMORY[0x1E69E7CC0];
  if (v2)
  {
    v14 = MEMORY[0x1E69E7CC0];
    _ss15ContiguousArrayV16_createNewBuffer14bufferIsUnique15minimumCapacity13growForAppendySb_SiSbtFSo07vImage_E0V_Tgq5_0(0, v2, 0);
    result = v14;
    v4 = *(v14 + 16);
    v5 = 32 * v4;
    v6 = (v1 + 48);
    do
    {
      v7 = *(v6 - 1);
      v8 = *v6;
      v15 = result;
      v9 = *(result + 24);
      v10 = v4 + 1;
      if (v4 >= v9 >> 1)
      {
        v12 = *v6;
        v13 = *(v6 - 1);
        _ss15ContiguousArrayV16_createNewBuffer14bufferIsUnique15minimumCapacity13growForAppendySb_SiSbtFSo07vImage_E0V_Tgq5_0((v9 > 1), v4 + 1, 1);
        v8 = v12;
        v7 = v13;
        result = v15;
      }

      *(result + 16) = v10;
      v11 = result + v5;
      *(v11 + 32) = v7;
      *(v11 + 48) = v8;
      v5 += 32;
      v6 = (v6 + 40);
      v4 = v10;
      --v2;
    }

    while (v2);
  }

  return result;
}

uint64_t vImage.PixelBuffer<>.withUnsafePixelBuffers<A>(_:)(void (*a1)(void), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  vImage.PixelBuffer<>.pixelBuffers.getter(a3, a5);
  a1();
}

uint64_t vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t *a5@<X8>)
{
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
  result = swift_allocObject();
  *(result + 16) = xmmword_1B7E76D90;
  if ((a3 | a2) < 0)
  {
    __break(1u);
  }

  else
  {
    *(result + 32) = a1;
    *(result + 40) = a3;
    *(result + 48) = a2;
    *(result + 56) = a4;
    *(result + 64) = 0;
    *a5 = result;
  }

  return result;
}

uint64_t vImage.PixelBuffer<>.withUnsafePointerToVImageBuffer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v20 = *MEMORY[0x1E69E9840];
  v5 = *v4;
  if (!*(*v4 + 16))
  {
    __break(1u);
  }

  v7 = *(v5 + 48);
  v19[0] = *(v5 + 32);
  v19[1] = v7;
  v8 = MEMORY[0x1EEE9AC00](a1);
  v17[2] = *(v9 + 16);
  v17[3] = v10;
  v17[4] = v11;
  v17[5] = v8;
  v17[6] = v12;
  type metadata accessor for vImage_Buffer(0);
  v14 = v13;
  v15 = __swift_instantiateConcreteTypeFromMangledNameV2(&_ss5Error_pMd);
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(v19, partial apply for closure #1 in vImage.PixelBuffer<>.withUnsafePointerToVImageBuffer<A>(_:), v17, v14, v15, a4, MEMORY[0x1E69E7288], &v18);
}

unint64_t vImage.PixelBuffer<>.subscript.getter@<X0>(unint64_t result@<X0>, uint64_t *a2@<X8>)
{
  if ((result & 0x8000000000000000) != 0)
  {
    __break(1u);
  }

  else if (*(*v2 + 16) > result)
  {
    v4 = *v2 + 40 * result;
    v5 = *(v4 + 32);
    v6 = *(v4 + 48);
    v13 = *(v4 + 64);
    v11 = v5;
    v12 = v6;
    __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
    v7 = swift_allocObject();
    v8 = v11;
    v9 = v12;
    *(v7 + 16) = xmmword_1B7E76D90;
    *(v7 + 32) = v8;
    *(v7 + 48) = v9;
    *(v7 + 64) = v13;
    *a2 = v7;
    return outlined init with copy of vImage.BufferWrapper(&v11, v10);
  }

  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.count.getter(uint64_t a1, uint64_t a2)
{
  v4 = *v2;
  result = vImage.PixelBuffer<>.rowStride.getter(a1, a2);
  if (!*(v4 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }

  v6 = *(v4 + 40);
  if (v6 < 0)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }

  v7 = result * v6;
  if ((result * v6) >> 64 != (result * v6) >> 63)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }

  v8 = (*(a2 + 24))();
  result = v7 * v8;
  if ((v7 * v8) >> 64 != (v7 * v8) >> 63)
  {
LABEL_9:
    __break(1u);
  }

  return result;
}

uint64_t vImage.PixelBuffer<>.rowStride.getter(uint64_t result, uint64_t a2)
{
  if (!*(*v2 + 16))
  {
    __break(1u);
    goto LABEL_14;
  }

  v4 = *(*v2 + 56);
  v5 = *(result + 16);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v7 = *(AssociatedTypeWitness - 8);
  result = AssociatedTypeWitness - 8;
  v8 = *(v7 + 72);
  if (!v8)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }

  if (v4 == 0x8000000000000000 && v8 == -1)
  {
    goto LABEL_16;
  }

  result = (*(a2 + 24))(v5, a2);
  if (!result)
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }

  if (v4 / v8 != 0x8000000000000000 || result != -1)
  {
    return v4 / v8 / result;
  }

LABEL_17:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.withUnsafeBufferPointer<A>(_:)(uint64_t (*a1)(uint64_t), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  v9 = *v5;
  result = vImage.PixelBuffer<>.rowStride.getter(a3, a5);
  if (!v9[2])
  {
    __break(1u);
    goto LABEL_9;
  }

  v11 = v9[5];
  if (v11 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }

  v12 = result * v11;
  if ((result * v11) >> 64 != (result * v11) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }

  result = (*(a5 + 24))(*(a3 + 16), a5);
  if ((v12 * result) >> 64 != (v12 * result) >> 63)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }

  if (!v9[2])
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }

  if (v9[4])
  {
    swift_getAssociatedTypeWitness();
    v13 = UnsafeBufferPointer.init(start:count:)();
    return a1(v13);
  }

LABEL_13:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.withUnsafeMutableBufferPointer<A>(_:)(uint64_t (*a1)(void *), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  v13[0] = *v5;
  result = vImage.PixelBuffer<>.rowStride.getter(a3, a5);
  if (!*(v13[0] + 16))
  {
    __break(1u);
    goto LABEL_9;
  }

  v10 = *(v13[0] + 40);
  if (v10 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }

  v11 = result * v10;
  if ((result * v10) >> 64 != (result * v10) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }

  result = (*(a5 + 24))(*(a3 + 16), a5);
  if ((v11 * result) >> 64 != (v11 * result) >> 63)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }

  if (!*(v13[0] + 16))
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }

  if (*(v13[0] + 32))
  {
    swift_getAssociatedTypeWitness();
    v13[0] = UnsafeMutableBufferPointer.init(start:count:)();
    v13[1] = v12;
    return a1(v13);
  }

LABEL_13:
  __break(1u);
  return result;
}

uint64_t vImage.Size.init(width:height:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, uint64_t *a3@<X8>)
{
  if (result < 1 || a2 < 1)
  {
    __break(1u);
  }

  else
  {
    *a3 = result;
    a3[1] = a2;
  }

  return result;
}

uint64_t (*vImage.PixelBuffer<>.withUnsafeVImageBuffer<A>(_:)(uint64_t (*result)(void, void, void, void)))(void, void, void, void)
{
  if (*(*v1 + 16))
  {
    return result(*(*v1 + 32), *(*v1 + 40), *(*v1 + 48), *(*v1 + 56));
  }

  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.withUnsafeVImageBuffers<A>(_:)(void (*a1)(void))
{
  vImage.PixelBuffer<>.vImageBuffers.getter();
  a1();
}

uint64_t vImage.PixelBuffer<>.pixelBuffers.getter(uint64_t a1, uint64_t a2)
{
  v9[5] = *v2;
  v9[2] = *(a1 + 16);
  v9[3] = a2;

  v3 = __swift_instantiateConcreteTypeFromMangledNameV2(&_sSay10Accelerate6vImageO13BufferWrapperVGMd);
  swift_getAssociatedTypeWitness();
  swift_getAssociatedConformanceWitness();
  v4 = type metadata accessor for vImage.PixelBuffer();
  v5 = lazy protocol witness table accessor for type [vImage.BufferWrapper] and conformance [A]();
  v7 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF(partial apply for closure #1 in vImage.PixelBuffer<>.pixelBuffers.getter, v9, v3, v4, MEMORY[0x1E69E73E0], v5, MEMORY[0x1E69E7410], v6);

  return v7;
}

uint64_t vImage.PixelBuffer<>.withUnsafePixelBuffer<A>(at:_:)(unint64_t a1, void (*a2)(uint64_t *))
{
  v5[2] = *v2;
  vImage.PixelBuffer<>.subscript.getter(a1, v5);
  a2(v5);
}

uint64_t closure #1 in vImage.PixelBuffer<>.pixelBuffers.getter@<X0>(__int128 *a1@<X0>, uint64_t *a2@<X8>)
{
  v4 = a1[1];
  v10[0] = *a1;
  v10[1] = v4;
  v11 = *(a1 + 4);
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
  v5 = swift_allocObject();
  v6 = *a1;
  v7 = a1[1];
  *(v5 + 16) = xmmword_1B7E76D90;
  *(v5 + 32) = v6;
  *(v5 + 48) = v7;
  *(v5 + 64) = *(a1 + 4);
  *a2 = v5;
  return outlined init with copy of vImage.BufferWrapper(v10, v9);
}

double vImage.PixelBuffer<>.init(bufferWrapper:pixelFormat:)@<D0>(__int128 *a1@<X0>, uint64_t *a2@<X8>)
{
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
  v4 = swift_allocObject();
  *&result = 1;
  v6 = *a1;
  v7 = a1[1];
  *(v4 + 16) = xmmword_1B7E76D90;
  *(v4 + 32) = v6;
  *(v4 + 48) = v7;
  *(v4 + 64) = *(a1 + 4);
  *a2 = v4;
  return result;
}

uint64_t vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, char a5@<W4>, uint64_t a6@<X6>, uint64_t a7@<X7>, uint64_t *a8@<X8>)
{
  if ((a5 & 1) == 0)
  {
    goto LABEL_4;
  }

  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v17 = *(AssociatedTypeWitness - 8);
  result = AssociatedTypeWitness - 8;
  v18 = *(v17 + 72);
  v19 = a2 * v18;
  if ((a2 * v18) >> 64 != (a2 * v18) >> 63)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }

  result = (*(a7 + 24))(a6, a7);
  a4 = v19 * result;
  if ((v19 * result) >> 64 == (v19 * result) >> 63)
  {
LABEL_4:
    __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
    result = swift_allocObject();
    *(result + 16) = xmmword_1B7E76D90;
    if (((a3 | a2) & 0x8000000000000000) == 0)
    {
      *(result + 32) = a1;
      *(result + 40) = a3;
      *(result + 48) = a2;
      *(result + 56) = a4;
      *(result + 64) = 0;
      *a8 = result;
      return result;
    }

    __break(1u);
    goto LABEL_7;
  }

LABEL_8:
  __break(1u);
  return result;
}

int64_t vImage.PixelBuffer<>.init(size:pixelFormat:)@<X0>(uint64_t *a1@<X0>, void *a2@<X2>, uint64_t a3@<X3>, void *a4@<X8>)
{
  v7 = *a1;
  v8 = a1[1];
  result = (*(a3 + 32))(a2, a3);
  if (result < 0)
  {
    __break(1u);
  }

  else
  {
    v10 = result;
    v11 = MEMORY[0x1E69E7CC0];
    if (result)
    {
      v20 = a4;
      v24 = MEMORY[0x1E69E7CC0];
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, result, 0);
      v11 = v24;
      do
      {
        result = closure #1 in vImage.PixelBuffer<>.init(size:pixelFormat:)(v7, v8, a2, a3, &v21);
        v12 = v21;
        v13 = v22;
        v14 = v23;
        v24 = v11;
        v16 = *(v11 + 16);
        v15 = *(v11 + 24);
        if (v16 >= v15 >> 1)
        {
          v18 = v22;
          v19 = v21;
          result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v15 > 1), v16 + 1, 1);
          v13 = v18;
          v12 = v19;
          v11 = v24;
        }

        *(v11 + 16) = v16 + 1;
        v17 = v11 + 40 * v16;
        *(v17 + 32) = v12;
        *(v17 + 48) = v13;
        *(v17 + 64) = v14;
        --v10;
      }

      while (v10);
      a4 = v20;
    }

    *a4 = v11;
  }

  return result;
}

void *closure #1 in vImage.PixelBuffer<>.init(size:pixelFormat:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X2>, void *a3@<X3>, uint64_t a4@<X4>, uint64_t *a5@<X8>)
{
  v9 = a3;
  v10 = (*(a4 + 40))(a3, a4);
  if (v10 < 1)
  {
    __break(1u);
    goto LABEL_6;
  }

  if (HIDWORD(v10))
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }

  v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(a1, a2);
  v9 = v5;
  if (!v5)
  {
    v15 = v11;
    v16 = v12;
    v17 = v13;
    v18 = v14;
    type metadata accessor for vImage.BufferReference();
    result = swift_allocObject();
    result[2] = v15;
    result[3] = v16;
    result[4] = v17;
    result[5] = v18;
    *a5 = v15;
    a5[1] = v16;
    a5[2] = v17;
    a5[3] = v18;
    a5[4] = result;
    return result;
  }

LABEL_7:

  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X2>, uint64_t a3@<X3>, void *a4@<X8>)
{
  swift_getAssociatedTypeWitness();
  swift_getAssociatedConformanceWitness();
  v8 = type metadata accessor for vImage.PixelBuffer();
  v9 = MEMORY[0x1B8CB13E0](a1, v8);
  v10 = *(a3 + 32);
  v11 = v10(a2, a3);
  if (v9 != v11)
  {
    __break(1u);
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }

  v40 = v10;
  *&v41 = a3 + 32;
  v39 = a4;
  *&v45 = a1;
  MEMORY[0x1EEE9AC00](v11);
  v37 = a2;
  v38 = a3;
  v12 = type metadata accessor for Array();
  *&v42 = swift_getWitnessTable();
  v14 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF(partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:), v36, v12, MEMORY[0x1E69E6530], MEMORY[0x1E69E73E0], v42, MEMORY[0x1E69E7410], v13);
  v15 = _sShyShyxGqd__nc7ElementQyd__RszSTRd__lufCSi_SaySiGTt0g5Tf4g_n(v14);

  v43 = a1;
  *&v45 = a1;
  MEMORY[0x1EEE9AC00](v16);
  v17 = a2;
  v37 = a2;
  v38 = a3;
  v18 = a3;
  v20 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF(partial apply for closure #2 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:), v36, v12, MEMORY[0x1E69E6530], MEMORY[0x1E69E73E0], v42, MEMORY[0x1E69E7410], v19);
  v21 = _sShyShyxGqd__nc7ElementQyd__RszSTRd__lufCSi_SaySiGTt0g5Tf4g_n(v20);

  v22 = *(v15 + 16);

  if (v22 != 1)
  {
LABEL_16:

    __break(1u);
    return result;
  }

  v23 = *(v21 + 16);

  if (v23 != 1)
  {
    goto LABEL_14;
  }

  v24 = v40(a2, a3);
  if (v24 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }

  v25 = v24;
  if (v24)
  {
    v48 = MEMORY[0x1E69E7CC0];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v24, 0);
    v26 = 0;
    v27 = v48;
    v28 = v17;
    do
    {
      v44 = v26;
      closure #3 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)(v28, v18, &v45);
      v29 = v45;
      v30 = v46;
      v31 = v47;
      v48 = v27;
      v33 = *(v27 + 16);
      v32 = *(v27 + 24);
      if (v33 >= v32 >> 1)
      {
        v41 = v46;
        v42 = v45;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((v32 > 1), v33 + 1, 1);
        v30 = v41;
        v29 = v42;
        v27 = v48;
      }

      ++v26;
      *(v27 + 16) = v33 + 1;
      v34 = v27 + 40 * v33;
      *(v34 + 32) = v29;
      *(v34 + 48) = v30;
      *(v34 + 64) = v31;
      v28 = v17;
    }

    while (v25 != v26);
  }

  else
  {

    v27 = MEMORY[0x1E69E7CC0];
  }

  *v39 = v27;
  return result;
}

void *closure #3 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t a1@<X2>, uint64_t a2@<X3>, uint64_t a3@<X8>)
{
  swift_getAssociatedTypeWitness();
  swift_getAssociatedConformanceWitness();
  type metadata accessor for vImage.PixelBuffer();
  Array.subscript.getter();
  result = v13;
  if (v13[2])
  {
    v7 = v13[4];
    v8 = v13[5];
    v9 = v13[6];
    v10 = v13[7];

    v11 = (*(a2 + 40))(a1, a2);
    result = specialized vImage.BufferWrapper.init(copying:bitsPerPixel:)(v7, v8, v9, v10, v11, v14);
    v12 = v14[1];
    *a3 = v14[0];
    *(a3 + 16) = v12;
    *(a3 + 32) = v15;
  }

  else
  {
    __break(1u);
  }

  return result;
}

void *vImage.PixelBuffer<>.init(width:height:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t *a5@<X8>)
{
  if (a1 < 1 || (v5 = a2, a2 < 1))
  {
    __break(1u);
    goto LABEL_7;
  }

  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
  v10 = swift_allocObject();
  *(v10 + 16) = xmmword_1B7E76D90;
  v11 = (*(a4 + 16))(a3, a4);
  if (v11 < 1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }

  if (!HIDWORD(v11))
  {
    v12 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(a1, v5);
    v14 = v13;
    v16 = v15;
    v18 = v17;
    type metadata accessor for vImage.BufferReference();
    result = swift_allocObject();
    result[2] = v12;
    result[3] = v14;
    result[4] = v16;
    result[5] = v18;
    *(v10 + 32) = v12;
    *(v10 + 40) = v14;
    *(v10 + 48) = v16;
    *(v10 + 56) = v18;
    *(v10 + 64) = result;
    *a5 = v10;
    return result;
  }

LABEL_8:
  __break(1u);

  result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.copy(to:)(uint64_t *a1, uint64_t a2, uint64_t a3)
{
  v10 = *MEMORY[0x1E69E9840];
  v5 = *v3;
  if (!*(*v3 + 16))
  {
    __break(1u);
  }

  v6 = *a1;
  v7 = *(v5 + 48);
  *&v9.data = *(v5 + 32);
  *&v9.width = v7;
  return closure #1 in vImage.PixelBuffer<>.copy(to:)(&v9, v6, v5, *(a2 + 16), a3);
}

vImage_Error closure #1 in vImage.PixelBuffer<>.copy(to:)(const vImage_Buffer *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  v10 = *MEMORY[0x1E69E9840];
  if (!*(a2 + 16))
  {
    __break(1u);
  }

  v6 = *(a2 + 48);
  *&v9.data = *(a2 + 32);
  *&v9.width = v6;
  v7 = (*(a5 + 16))(a4, a5);
  return vImageCopyBuffer(a1, &v9, v7 / 8, 0);
}

unint64_t vImage.PixelBuffer<>.cropped(to:)@<X0>(unint64_t result@<X0>, uint64_t a2@<X1>, uint64_t *a3@<X8>, double a4@<D0>)
{
  v9 = *v7;
  if (!*(*v7 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }

  v10 = *(v9 + 48);
  if ((v10 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }

  v11 = *(v9 + 40);
  if ((v11 & 0x8000000000000000) != 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }

  v13 = result;
  v14 = v10;
  v15 = v11;
  v22 = CGRectIntegral(*&a4);
  v20.origin.x = 0.0;
  v20.origin.y = 0.0;
  v20.size.width = v14;
  v20.size.height = v15;
  v21 = CGRectIntersection(v20, v22);
  x = v21.origin.x;
  y = v21.origin.y;
  result = vImage.Size.init(exactly:)(v18, v21.size.width, v21.size.height);
  if (v18[16])
  {
LABEL_11:
    __break(1u);
    return result;
  }

  vImage.PixelBuffer<>.init(size:pixelFormat:)(v18, *(v13 + 16), a2, a3);

  return vImage.PixelBuffer<>.crop(at:destination:)(a3, v13, a2, x, y);
}

unint64_t vImage.Size.init(exactly:)@<X0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>)
{
  result = _ss17FixedWidthIntegerPsE8_convert4fromxSg5value_Sb5exacttqd___tSBRd__lFZSi_12CoreGraphics7CGFloatVTt1g5(&v8, a2);
  if ((result & 1) == 0 || v9 == 1 || (v6 = v8, v8 < 1) || (result = _ss17FixedWidthIntegerPsE8_convert4fromxSg5value_Sb5exacttqd___tSBRd__lFZSi_12CoreGraphics7CGFloatVTt1g5(&v8, a3), (result & 1) == 0) || v9 == 1 || (v7 = v8, v8 < 1))
  {
    *a1 = 0;
    *(a1 + 8) = 0;
    *(a1 + 16) = 1;
  }

  else
  {
    *a1 = v6;
    *(a1 + 8) = v7;
    *(a1 + 16) = 0;
  }

  return result;
}

uint64_t vImage.PixelBuffer<>.crop(at:destination:)(uint64_t *a1, uint64_t a2, uint64_t a3, double a4, double a5)
{
  v6 = v5;
  v38 = *MEMORY[0x1E69E9840];
  v7 = *v5;
  if (!*(*v5 + 16))
  {
    __break(1u);
    goto LABEL_30;
  }

  v8 = v7[6];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }

  v9 = v7[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }

  v11 = *a1;
  if (!*(*a1 + 16))
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }

  v12 = *(v11 + 48);
  if ((v12 & 0x8000000000000000) != 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }

  v13 = *(v11 + 40);
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }

  v16 = v8;
  v17 = v9;
  v18 = v12;
  v19 = v13;
  v43 = CGRectIntegral(*&a4);
  v39.origin.x = 0.0;
  v39.origin.y = 0.0;
  v39.size.width = v16;
  v39.size.height = v17;
  v40 = CGRectIntersection(v39, v43);
  x = v40.origin.x;
  y = v40.origin.y;
  width = v40.size.width;
  height = v40.size.height;
  if (CGRectIsEmpty(v40))
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }

  if ((*&y & 0x7FFFFFFFFFFFFFFFuLL) > 0x7FEFFFFFFFFFFFFFLL)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }

  if (y <= -9.22337204e18)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }

  if (y >= 9.22337204e18)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }

  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }

  v24 = v7[7];
  v25 = y * v24;
  if ((y * v24) >> 64 != v25 >> 63)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }

  if ((*&x & 0x7FFFFFFFFFFFFFFFuLL) > 0x7FEFFFFFFFFFFFFFLL)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }

  if (x <= -9.22337204e18)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }

  if (x >= 9.22337204e18)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }

  v26 = *(a2 + 16);
  v27 = (*(a3 + 16))(v26, a3);
  v28 = x * (v27 / 8);
  if ((x * (v27 / 8)) >> 64 != v28 >> 63)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }

  v29 = __OFADD__(v25, v28);
  v30 = v25 + v28;
  if (v29)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }

  if (!v7[2])
  {
LABEL_46:
    __break(1u);
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }

  v31 = v7[4];
  if (!v31)
  {
    goto LABEL_55;
  }

  v41.origin.x = x;
  v41.origin.y = y;
  v41.size.width = width;
  v41.size.height = height;
  v32 = CGRectGetWidth(v41);
  if ((*&v32 & 0x7FFFFFFFFFFFFFFFuLL) > 0x7FEFFFFFFFFFFFFFLL)
  {
    goto LABEL_47;
  }

  if (v32 <= -9.22337204e18)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }

  if (v32 >= 9.22337204e18)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }

  v42.origin.x = x;
  v42.origin.y = y;
  v42.size.width = width;
  v42.size.height = height;
  v33 = CGRectGetHeight(v42);
  if ((*&v33 & 0x7FFFFFFFFFFFFFFFuLL) > 0x7FEFFFFFFFFFFFFFLL)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }

  if (v33 <= -9.22337204e18)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }

  if (v33 >= 9.22337204e18)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }

  if (!v7[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }

  vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)(v31 + v30, v32, v33, v7[7], &v36);
  if (!*(v36 + 16))
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }

  v34 = *(v36 + 48);
  *&v37.data = *(v36 + 32);
  *&v37.width = v34;
  closure #1 in vImage.PixelBuffer<>.crop(at:destination:)(&v37, a1, v6, v26, a3);
}

vImage_Error closure #1 in vImage.PixelBuffer<>.crop(at:destination:)(const vImage_Buffer *a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  v11 = *MEMORY[0x1E69E9840];
  v5 = *a2;
  if (!*(*a2 + 16))
  {
    __break(1u);
  }

  v7 = *(v5 + 48);
  *&v10.data = *(v5 + 32);
  *&v10.width = v7;
  v8 = (*(a5 + 16))(a4, a5);
  return vImageCopyBuffer(a1, &v10, v8 / 8, 0);
}

void vImage.PixelBuffer<>.withUnsafeRegionOfInterest<A>(_:_:)(void (*a1)(uint64_t *), double a2, double a3, double a4, double a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  v10 = *v9;
  if (!*(*v9 + 16))
  {
    __break(1u);
    goto LABEL_26;
  }

  v11 = v10[6];
  if ((v11 & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }

  v12 = v10[5];
  if ((v12 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }

  v15 = v11;
  v16 = v12;
  v34 = CGRectIntegral(*&a2);
  v30.origin.x = 0.0;
  v30.origin.y = 0.0;
  v30.size.width = v15;
  v30.size.height = v16;
  v31 = CGRectIntersection(v30, v34);
  x = v31.origin.x;
  y = v31.origin.y;
  width = v31.size.width;
  height = v31.size.height;
  if (CGRectIsEmpty(v31))
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }

  if ((*&y & 0x7FFFFFFFFFFFFFFFuLL) > 0x7FEFFFFFFFFFFFFFLL)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }

  if (y <= -9.22337204e18)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }

  if (y >= 9.22337204e18)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }

  if (!v10[2])
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }

  v21 = v10[7];
  v22 = y * v21;
  if ((y * v21) >> 64 != v22 >> 63)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }

  if ((*&x & 0x7FFFFFFFFFFFFFFFuLL) > 0x7FEFFFFFFFFFFFFFLL)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }

  if (x <= -9.22337204e18)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }

  if (x >= 9.22337204e18)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }

  v23 = (*(a9 + 16))();
  v24 = x * (v23 / 8);
  if ((x * (v23 / 8)) >> 64 != v24 >> 63)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }

  v25 = v22 + v24;
  if (__OFADD__(v22, v24))
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }

  if (!v10[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }

  v26 = v10[4];
  if (v26)
  {
    v32.origin.x = x;
    v32.origin.y = y;
    v32.size.width = width;
    v32.size.height = height;
    v27 = CGRectGetWidth(v32);
    if ((*&v27 & 0x7FFFFFFFFFFFFFFFuLL) <= 0x7FEFFFFFFFFFFFFFLL)
    {
      if (v27 > -9.22337204e18)
      {
        if (v27 < 9.22337204e18)
        {
          v33.origin.x = x;
          v33.origin.y = y;
          v33.size.width = width;
          v33.size.height = height;
          v28 = CGRectGetHeight(v33);
          if ((*&v28 & 0x7FFFFFFFFFFFFFFFuLL) <= 0x7FEFFFFFFFFFFFFFLL)
          {
            if (v28 > -9.22337204e18)
            {
              if (v28 < 9.22337204e18)
              {
                if (v10[2])
                {
                  vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)(v26 + v25, v27, v28, v10[7], &v29);
                  a1(&v29);

                  return;
                }

                goto LABEL_46;
              }

LABEL_45:
              __break(1u);
LABEL_46:
              __break(1u);
              goto LABEL_47;
            }

LABEL_44:
            __break(1u);
            goto LABEL_45;
          }

LABEL_43:
          __break(1u);
          goto LABEL_44;
        }

LABEL_42:
        __break(1u);
        goto LABEL_43;
      }

LABEL_41:
      __break(1u);
      goto LABEL_42;
    }

LABEL_40:
    __break(1u);
    goto LABEL_41;
  }

LABEL_47:
  __break(1u);
}

double static vImage.PixelBuffer<>.makeDynamicPixelBufferAndCGImageFormat(cgImage:)@<D0>(uint64_t *a1@<X0>, void *a2@<X1>, uint64_t a3@<X8>)
{
  v19 = *MEMORY[0x1E69E9840];
  v16 = 0;
  v14 = 0u;
  v15 = 0u;
  v6 = a2;
  specialized vImage.BufferWrapper.init(cgImage:format:)(v6, &v14, v17);

  if (!v3)
  {
    __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
    v8 = swift_allocObject();
    v9 = v17[0];
    v10 = v17[1];
    *(v8 + 16) = xmmword_1B7E76D90;
    *(v8 + 32) = v9;
    *(v8 + 48) = v10;
    *(v8 + 64) = v18;
    *a1 = v8;
    v11 = *(&v14 + 1);
    v12 = *(&v15 + 1);
    v13 = v16;
    result = *&v15;
    *a3 = v14;
    *(a3 + 8) = v11;
    *(a3 + 16) = result;
    *(a3 + 24) = v12;
    *(a3 + 32) = v13;
  }

  return result;
}

double static vImage.PixelBuffer<>.makePixelBufferAndCGImageFormat(cgImage:pixelFormat:)@<D0>(void *a1@<X0>, void *a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, size_t a5@<X5>, uint64_t a6@<X8>)
{
  v17 = *MEMORY[0x1E69E9840];
  v16 = 0;
  v14 = 0u;
  v15 = 0u;
  vImage.PixelBuffer<>.init(cgImage:cgImageFormat:pixelFormat:)(a2, &v14, a3, a4, a5, &v13);
  if (!v6)
  {
    *a1 = v13;
    v10 = *(&v14 + 1);
    v11 = *(&v15 + 1);
    v12 = v16;
    result = *&v15;
    *a6 = v14;
    *(a6 + 8) = v10;
    *(a6 + 16) = result;
    *(a6 + 24) = v11;
    *(a6 + 32) = v12;
  }

  return result;
}

void vImage.PixelBuffer<>.init(cgImage:cgImageFormat:pixelFormat:)(CGImage *a1@<X0>, unsigned int *a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, size_t BitsPerComponent@<X5>, uint64_t *a6@<X8>)
{
  v31 = *MEMORY[0x1E69E9840];
  BitsPerPixel = *a2;
  v14 = a2[1];
  v15 = *(a2 + 1);
  v16 = *(a2 + 3);
  v17 = a2[8];
  v24[0] = *a2;
  v24[1] = v14;
  v25 = v15;
  v26 = *(a2 + 2);
  v27 = v16;
  v28 = v17;
  v23 = 0;
  memset(v22, 0, sizeof(v22));
  if (MEMORY[0x1B8CB3230](v24, v22))
  {
    BitsPerPixel = CGImageGetBitsPerPixel(a1);
    if (BitsPerPixel != (*(BitsPerComponent + 16))(a3, BitsPerComponent))
    {
      __break(1u);
      goto LABEL_12;
    }

    BitsPerComponent = CGImageGetBitsPerComponent(a1);
    if (BitsPerComponent == (*(a4 + 16))(a3, a4))
    {
      goto LABEL_7;
    }

    __break(1u);
  }

  if ((*(BitsPerComponent + 16))(a3, BitsPerComponent) != v14)
  {
LABEL_12:
    __break(1u);
LABEL_13:
    __break(1u);
  }

  if ((*(a4 + 16))(a3, a4) != BitsPerPixel)
  {
    goto LABEL_13;
  }

LABEL_7:
  v18 = a1;
  specialized vImage.BufferWrapper.init(cgImage:format:)(v18, a2, v29);

  if (v6)
  {
  }

  else
  {
    __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
    v19 = swift_allocObject();
    v20 = v29[0];
    v21 = v29[1];
    *(v19 + 16) = xmmword_1B7E76D90;
    *(v19 + 32) = v20;
    *(v19 + 48) = v21;
    *(v19 + 64) = v30;

    *a6 = v19;
  }
}

void *vImage.PixelBuffer<>.makeCGImage(cgImageFormat:)(void *result)
{
  v2 = *v1;
  if (*(*v1 + 16))
  {
    v3 = v2[4];
    v4 = v2[5];
    v5 = v2[6];
    v6 = v2[7];
    v7 = 0;
    return vImage_Buffer.createCGImage(format:flags:)(result, &v7, v3, v4, v5, v6);
  }

  else
  {
    __break(1u);
  }

  return result;
}

void vImage.PixelBuffer<>.init(referencing:planeIndex:overrideSize:pixelFormat:)(__CVBuffer *a1@<X0>, size_t a2@<X1>, uint64_t a3@<X2>, void *a4@<X8>)
{
  WidthOfPlane = *a3;
  HeightOfPlane = *(a3 + 8);
  v9 = *(a3 + 16);
  BaseAddressOfPlane = CVPixelBufferGetBaseAddressOfPlane(a1, a2);
  BytesPerRowOfPlane = CVPixelBufferGetBytesPerRowOfPlane(a1, a2);
  if (v9 == 1)
  {
    HeightOfPlane = CVPixelBufferGetHeightOfPlane(a1, a2);
    WidthOfPlane = CVPixelBufferGetWidthOfPlane(a1, a2);
  }

  if (BaseAddressOfPlane)
  {
    vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)(BaseAddressOfPlane, WidthOfPlane, HeightOfPlane, BytesPerRowOfPlane, &v12);

    *a4 = v12;
  }

  else
  {
    __break(1u);
  }
}

void vImage.PixelBuffer<>.init(copying:cvImageFormat:cgImageFormat:pixelFormat:)(void *a1@<X0>, void *a2@<X1>, uint64_t a3@<X2>, uint64_t *a4@<X8>)
{
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
  v9 = swift_allocObject();
  *(v9 + 16) = xmmword_1B7E76D90;
  specialized vImage.BufferWrapper.init(cvPixelBuffer:cvImageFormat:cgImageFormat:)(a1, a2, a3, v11);
  if (v4)
  {

    *(v9 + 16) = 0;
  }

  else
  {
    v10 = v11[1];
    *(v9 + 32) = v11[0];
    *(v9 + 48) = v10;
    *(v9 + 64) = v12;

    *a4 = v9;
  }
}

void vImage.PixelBuffer<>.init(referencing:converter:destinationPixelFormat:)(void *a1@<X0>, void *a2@<X1>, uint64_t *a3@<X8>)
{
  v10 = *MEMORY[0x1E69E9840];
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
  v6 = swift_allocObject();
  *(v6 + 16) = xmmword_1B7E76D90;
  v8 = 0u;
  v9 = 0u;
  MEMORY[0x1B8CB31F0](&v8, a2, a1, 512);
  v7 = v9;
  *(v6 + 32) = v8;
  *(v6 + 48) = v7;
  *(v6 + 64) = 0;

  *a3 = v6;
}

uint64_t vImage.PixelBuffer<>.copy(to:cvImageFormat:cgImageFormat:)(__CVBuffer *a1, uint64_t a2, uint64_t a3)
{
  v22 = *MEMORY[0x1E69E9840];
  v7 = *v3;
  if (CVPixelBufferGetPlaneCount(a1))
  {
    __break(1u);
    goto LABEL_14;
  }

  Width = CVPixelBufferGetWidth(a1);
  if (!v7[2])
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }

  v9 = v7[6];
  if (v9 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }

  if (Width != v9)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }

  Height = CVPixelBufferGetHeight(a1);
  if (!v7[2])
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }

  v11 = v7[5];
  if (v11 < 0)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }

  if (Height != v11)
  {
    goto LABEL_19;
  }

  v20[0] = v7[4];
  v20[1] = Height;
  v21 = *(v7 + 3);
  v12 = *(a3 + 16);
  v18[0] = *a3;
  v18[1] = v12;
  v19 = *(a3 + 32);
  result = MEMORY[0x1B8CB31C0](v20, v18, a1, a2, 0, 0);
  if (result)
  {
    v14 = result;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    v16 = v15;
    vImage.Error.init(rawValue:)(v14, v18);
    v17 = v18[0];
    if (LOBYTE(v18[0]) == 20)
    {
      v17 = 11;
    }

    *v16 = v17;
    return swift_willThrow();
  }

  return result;
}

uint64_t vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t *a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t a5@<X5>, uint64_t a6@<X6>, uint64_t *a7@<X8>)
{
  v12 = *a2;
  v13 = a2[1];
  v14 = (*(a6 + 16))(a4, a6);
  v15 = v12 * v13;
  if ((v12 * v13) >> 64 != (v12 * v13) >> 63)
  {
    __break(1u);
    goto LABEL_10;
  }

  v16 = v14;
  v29 = a4;
  v17 = *(a5 + 24);
  v18 = v17(a3, a5);
  if ((v15 * v18) >> 64 != (v15 * v18) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }

  if (v16 != v15 * v18)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }

  v19 = *(*(swift_getAssociatedTypeWitness() - 8) + 72);
  if ((v19 - 0x1000000000000000) >> 61 != 7)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }

  v20 = 8 * v19;
  v21 = v17(a3, a5);
  v22 = v20 * v21;
  if ((v20 * v21) >> 64 != (v20 * v21) >> 63)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }

  if ((v22 & 0x8000000000000000) != 0)
  {
LABEL_14:
    __break(1u);
LABEL_15:
    __break(1u);
  }

  if (HIDWORD(v22))
  {
    goto LABEL_15;
  }

  v31 = specialized vImage_Buffer.init(size:bitsPerPixel:)(v12, v13);
  v32 = v23;
  v33 = v24;
  v34 = v25;
  MEMORY[0x1EEE9AC00](v31);
  (*(a6 + 24))(partial apply for closure #1 in vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:));
  __swift_instantiateConcreteTypeFromMangledNameV2(&_ss23_ContiguousArrayStorageCy10Accelerate6vImageO13BufferWrapperVGMd);
  v26 = swift_allocObject();
  *(v26 + 16) = xmmword_1B7E76D90;
  type metadata accessor for vImage.BufferReference();
  v27 = swift_allocObject();
  v27[2] = v31;
  v27[3] = v32;
  v27[4] = v33;
  v27[5] = v34;
  *(v26 + 32) = v31;
  *(v26 + 40) = v32;
  *(v26 + 48) = v33;
  *(v26 + 56) = v34;
  *(v26 + 64) = v27;
  result = (*(*(v29 - 8) + 8))(a1, v29);
  *a7 = v26;
  return result;
}

vImage_Error closure #1 in vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:)(uint64_t a1, uint64_t a2, vImagePixelCount a3, int64_t a4, vImage_Buffer *a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  v24 = *MEMORY[0x1E69E9840];
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v14 = UnsafeBufferPointer.baseAddress.getter();
  if (!v14)
  {
LABEL_11:
    __break(1u);
  }

  if (((a4 | a3) & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_8;
  }

  dest = a5;
  v15 = *(*(AssociatedTypeWitness - 8) + 72);
  v16 = a4 * v15;
  if ((a4 * v15) >> 64 != (a4 * v15) >> 63)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }

  v17 = v14;
  v18 = *(a8 + 24);
  v19 = v18(a6, a8);
  if ((v16 * v19) >> 64 != (v16 * v19) >> 63)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }

  src.data = v17;
  src.height = a3;
  src.width = a4;
  src.rowBytes = v16 * v19;
  v20 = v18(a6, a8);
  if ((v15 * v20) >> 64 != (v15 * v20) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }

  return vImageCopyBuffer(&src, dest, v15 * v20, 0);
}

uint64_t vImage.PixelBuffer<>.array.getter(uint64_t result, uint64_t a2)
{
  v3 = *v2;
  if (!*(*v2 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }

  v4 = *(v3 + 48);
  if (v4 < 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }

  v5 = *(v3 + 40);
  if (v5 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }

  v6 = v4 * v5;
  if ((v4 * v5) >> 64 != (v4 * v5) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }

  result = (*(a2 + 24))(*(result + 16));
  if ((v6 * result) >> 64 == (v6 * result) >> 63)
  {
    MEMORY[0x1EEE9AC00](result);
    swift_getAssociatedTypeWitness();
    return Array.init(unsafeUninitializedCapacity:initializingWith:)();
  }

LABEL_11:
  __break(1u);
  return result;
}

vImage_Error closure #1 in vImage.PixelBuffer<>.array.getter(uint64_t a1, void *a2, void *a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  v30 = *MEMORY[0x1E69E9840];
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v12 = UnsafeMutableBufferPointer.baseAddress.getter();
  if (!a3[2])
  {
    __break(1u);
    goto LABEL_11;
  }

  v13 = a3[5];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }

  v14 = a3[6];
  if (v14 < 0)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }

  v26 = a2;
  v15 = *(*(AssociatedTypeWitness - 8) + 72);
  v16 = v14 * v15;
  if ((v14 * v15) >> 64 != (v14 * v15) >> 63)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }

  v17 = v12;
  v18 = *(a6 + 24);
  v19 = v18(a5);
  if ((v16 * v19) >> 64 != (v16 * v19) >> 63)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }

  dest.data = v17;
  dest.height = v13;
  dest.width = v14;
  dest.rowBytes = v16 * v19;
  if (!a3[2])
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
  }

  v20 = a3[4];
  v21 = a3[5];
  v23 = a3[6];
  v22 = a3[7];
  v24 = (v18)(a5, a6);
  if ((v15 * v24) >> 64 != (v15 * v24) >> 63)
  {
    goto LABEL_16;
  }

  v28 = 0;
  result = vImage_Buffer.copy(destinationBuffer:pixelSize:flags:)(&dest, v15 * v24, &v28, v20, v21, v23, v22);
  if (v6)
  {
    result = swift_unexpectedError();
    __break(1u);
  }

  else
  {
    *v26 = a4;
  }

  return result;
}

uint64_t vImage.Size.init<A>(exactWidth:height:)@<X0>(char *a1@<X0>, char *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t *a5@<X8>)
{
  v56 = a5;
  v57 = a2;
  v7 = *(*(a4 + 24) + 16);
  AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  v8 = MEMORY[0x1EEE9AC00](AssociatedTypeWitness);
  v51 = &v49 - v9;
  v10 = *(a3 - 8);
  v11 = MEMORY[0x1EEE9AC00](v8);
  v13 = &v49 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  v14 = MEMORY[0x1EEE9AC00](v11);
  v54 = &v49 - v15;
  MEMORY[0x1EEE9AC00](v14);
  v17 = &v49 - v16;
  v18 = *(v10 + 16);
  v55 = a1;
  v53 = v18;
  v18(&v49 - v16, a1, a3);
  if ((dispatch thunk of static BinaryInteger.isSigned.getter() & 1) == 0 || dispatch thunk of BinaryInteger.bitWidth.getter() < 65)
  {
    goto LABEL_15;
  }

  v58 = 0x8000000000000000;
  if (dispatch thunk of static BinaryInteger.isSigned.getter())
  {
    if (dispatch thunk of BinaryInteger.bitWidth.getter() >= 64)
    {
      lazy protocol witness table accessor for type Int and conformance Int();
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      v19 = dispatch thunk of static Comparable.< infix(_:_:)();
      v20 = *(v10 + 8);
      v20(v54, a3);
      goto LABEL_9;
    }

LABEL_14:
    dispatch thunk of BinaryInteger._lowWord.getter();
    goto LABEL_15;
  }

  v21 = dispatch thunk of static BinaryInteger.isSigned.getter();
  v22 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if ((v21 & 1) == 0)
  {
    if (v22 >= 64)
    {
      goto LABEL_15;
    }

    goto LABEL_14;
  }

  if (v22 <= 64)
  {
    swift_getAssociatedConformanceWitness();
    dispatch thunk of _ExpressibleByBuiltinIntegerLiteral.init(_builtinIntegerLiteral:)();
    v25 = v7;
    v26 = v54;
    dispatch thunk of ExpressibleByIntegerLiteral.init(integerLiteral:)();
    LODWORD(v50) = dispatch thunk of static Comparable.< infix(_:_:)();
    v20 = *(v10 + 8);
    v27 = v26;
    v7 = v25;
    v20(v27, a3);
    if (v50)
    {
      goto LABEL_19;
    }

    goto LABEL_14;
  }

  lazy protocol witness table accessor for type Int and conformance Int();
  v50 = v7;
  v23 = v54;
  dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
  v19 = dispatch thunk of static Comparable.< infix(_:_:)();
  v20 = *(v10 + 8);
  v24 = v23;
  v7 = v50;
  v20(v24, a3);
LABEL_9:
  if (v19)
  {
    goto LABEL_19;
  }

LABEL_15:
  if (dispatch thunk of BinaryInteger.bitWidth.getter() > 64 || dispatch thunk of BinaryInteger.bitWidth.getter() == 64 && (dispatch thunk of static BinaryInteger.isSigned.getter() & 1) == 0)
  {
    v58 = 0x7FFFFFFFFFFFFFFFLL;
    v28 = dispatch thunk of static BinaryInteger.isSigned.getter();
    v29 = dispatch thunk of BinaryInteger.bitWidth.getter();
    if (v28)
    {
      if (v29 > 64)
      {
        goto LABEL_18;
      }
    }

    else if (v29 > 63)
    {
LABEL_18:
      lazy protocol witness table accessor for type Int and conformance Int();
      v50 = v7;
      v30 = v54;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      v31 = dispatch thunk of static Comparable.< infix(_:_:)();
      v20 = *(v10 + 8);
      v20(v30, a3);
      if ((v31 & 1) == 0)
      {
        goto LABEL_25;
      }

LABEL_19:
      v20(v17, a3);
      v20(v57, a3);
      result = (v20)(v55, a3);
      goto LABEL_53;
    }

    dispatch thunk of BinaryInteger._lowWord.getter();
  }

LABEL_25:
  v33 = dispatch thunk of BinaryInteger._lowWord.getter();
  v34 = *(v10 + 8);
  v34(v17, a3);
  if (v33 <= 0)
  {
    v34(v57, a3);
    v35 = v55;
LABEL_45:
    result = (v34)(v35, a3);
    goto LABEL_53;
  }

  v53(v13, v57, a3);
  if ((dispatch thunk of static BinaryInteger.isSigned.getter() & 1) != 0 && dispatch thunk of BinaryInteger.bitWidth.getter() >= 65)
  {
    v58 = 0x8000000000000000;
    if (dispatch thunk of static BinaryInteger.isSigned.getter())
    {
      if (dispatch thunk of BinaryInteger.bitWidth.getter() < 64)
      {
        goto LABEL_39;
      }
    }

    else
    {
      v36 = dispatch thunk of static BinaryInteger.isSigned.getter();
      v37 = dispatch thunk of BinaryInteger.bitWidth.getter();
      if ((v36 & 1) == 0)
      {
        if (v37 >= 64)
        {
          goto LABEL_40;
        }

        goto LABEL_39;
      }

      if (v37 <= 64)
      {
        swift_getAssociatedConformanceWitness();
        dispatch thunk of _ExpressibleByBuiltinIntegerLiteral.init(_builtinIntegerLiteral:)();
        v40 = v54;
        dispatch thunk of ExpressibleByIntegerLiteral.init(integerLiteral:)();
        v41 = dispatch thunk of static Comparable.< infix(_:_:)();
        v34(v40, a3);
        if ((v41 & 1) == 0)
        {
LABEL_39:
          dispatch thunk of BinaryInteger._lowWord.getter();
          goto LABEL_40;
        }

LABEL_44:
        v34(v57, a3);
        v34(v55, a3);
        v35 = v13;
        goto LABEL_45;
      }
    }

    lazy protocol witness table accessor for type Int and conformance Int();
    v38 = v54;
    dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
    v39 = dispatch thunk of static Comparable.< infix(_:_:)();
    v34(v38, a3);
    if (v39)
    {
      goto LABEL_44;
    }
  }

LABEL_40:
  if (dispatch thunk of BinaryInteger.bitWidth.getter() > 64 || dispatch thunk of BinaryInteger.bitWidth.getter() == 64 && (dispatch thunk of static BinaryInteger.isSigned.getter() & 1) == 0)
  {
    v58 = 0x7FFFFFFFFFFFFFFFLL;
    v42 = dispatch thunk of static BinaryInteger.isSigned.getter();
    v43 = dispatch thunk of BinaryInteger.bitWidth.getter();
    if (v42)
    {
      if (v43 > 64)
      {
        goto LABEL_43;
      }
    }

    else if (v43 > 63)
    {
LABEL_43:
      lazy protocol witness table accessor for type Int and conformance Int();
      v44 = v54;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      v45 = dispatch thunk of static Comparable.< infix(_:_:)();
      v34(v44, a3);
      if ((v45 & 1) == 0)
      {
        goto LABEL_51;
      }

      goto LABEL_44;
    }

    dispatch thunk of BinaryInteger._lowWord.getter();
  }

LABEL_51:
  v46 = dispatch thunk of BinaryInteger._lowWord.getter();
  v34(v57, a3);
  v34(v55, a3);
  result = (v34)(v13, a3);
  if (v46 > 0)
  {
    v47 = v56;
    *v56 = v33;
    v47[1] = v46;
    *(v47 + 16) = 0;
    return result;
  }

LABEL_53:
  v48 = v56;
  *v56 = 0;
  v48[1] = 0;
  *(v48 + 16) = 1;
  return result;
}

uint64_t vImage.Size.init<A>(exactWidth:height:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X8>)
{
  v10 = *(a3 - 8);
  MEMORY[0x1EEE9AC00](a1);
  v12 = v18 - ((v11 + 15) & 0xFFFFFFFFFFFFFFF0);
  v13 = *(v10 + 16);
  v13(v12, a1);
  lazy protocol witness table accessor for type Int and conformance Int();
  v18[1] = a4;
  FixedWidthInteger.init<A>(exactly:)();
  if (v20 == 1 || v19 < 1)
  {
    v17 = *(v10 + 8);
    v17(a2, a3);
    result = (v17)(a1, a3);
  }

  else
  {
    v18[0] = v19;
    (v13)(v12, a2, a3);
    FixedWidthInteger.init<A>(exactly:)();
    v14 = *(v10 + 8);
    v14(a2, a3);
    result = (v14)(a1, a3);
    if ((v20 & 1) == 0)
    {
      v16 = v19;
      if (v19 >= 1)
      {
        *a5 = v18[0];
        *(a5 + 8) = v16;
        *(a5 + 16) = 0;
        return result;
      }
    }
  }

  *a5 = 0;
  *(a5 + 8) = 0;
  *(a5 + 16) = 1;
  return result;
}

uint64_t vImage.Size.init(width:height:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, void *a3@<X8>)
{
  if (result < 0)
  {
    __break(1u);
  }

  else if ((a2 & 0x8000000000000000) == 0)
  {
    *a3 = result;
    a3[1] = a2;
    return result;
  }

  __break(1u);
  return result;
}

void vImage.Size.init(cvPixelBuffer:)(__CVBuffer *a1@<X0>, int64_t *a2@<X8>)
{
  Width = CVPixelBufferGetWidth(a1);
  Height = CVPixelBufferGetHeight(a1);

  if (Width < 1 || Height < 1)
  {
    __break(1u);
  }

  else
  {
    *a2 = Width;
    a2[1] = Height;
  }
}