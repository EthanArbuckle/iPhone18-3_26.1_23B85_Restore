int16x8_t *vpx_highbd_idct32x32_34_add_neon(uint64_t a1, int16x8_t *a2, int a3, int a4)
{
  v803 = *MEMORY[0x277D85DE8];
  if (a4 == 8)
  {
    vpx_idct32_6_neon(a1, v799);
    vpx_idct32_8_neon(v799, a2, a3, 1);
    vpx_idct32_8_neon(v800, a2 + 1, a3, 1);
    vpx_idct32_8_neon(v801, a2 + 2, a3, 1);
    return vpx_idct32_8_neon(v802, a2 + 3, a3, 1);
  }

  else
  {
    v7 = 0;
    v8 = *(a1 + 16);
    v9 = *(a1 + 128);
    v10 = *(a1 + 144);
    v11 = vtrn1q_s32(*a1, v9);
    v12 = vtrn2q_s32(*a1, v9);
    v786 = *v12.i8;
    v13 = *(a1 + 256);
    v14 = *(a1 + 272);
    v15 = *(a1 + 384);
    v16 = *(a1 + 400);
    v17 = vtrn1q_s32(v13, v15);
    v18 = vtrn2q_s32(v13, v15);
    v781 = *v18.i8;
    v19 = *(a1 + 512);
    v20 = *(a1 + 528);
    v21 = *(a1 + 640);
    v22 = *(a1 + 656);
    v23 = vtrn1q_s32(v19, v21);
    v767 = vtrn2q_s32(v19, v21);
    v24 = *(a1 + 768);
    v25 = *(a1 + 784);
    v26 = *(a1 + 896);
    v27 = *(a1 + 912);
    v28 = vtrn1q_s32(v24, v26);
    v791 = vtrn2q_s32(v24, v26);
    v709 = vzip2q_s64(v11, v17);
    v29 = vzip2q_s64(v12, v18);
    v693 = vzip2q_s64(v23, v28);
    v30 = vzip2_s32(*&v8, *&v10);
    v31 = vdup_n_s32(0xF8Du);
    *v12.i8 = vzip2_s32(*&v14, *&v16);
    v32 = vmull_s32(*v12.i8, v31);
    *v26.i8 = vzip2_s32(*&v20, *&v22);
    v793 = vzip2_s32(*&v25, *&v27);
    *v24.i8 = vdup_n_s32(0x3E15u);
    v33 = vrshrn_n_s64(vmull_s32(v30, v31), 0xEuLL);
    v34 = vrshrn_n_s64(vmull_s32(v30, *v24.i8), 0xEuLL);
    v35 = vdupq_n_s32(0xFFFFF69C);
    v36 = vrshrn_n_s64(vmull_s32(*v29.i8, *v35.i8), 0xEuLL);
    v745 = vzip1_s32(*&v8, *&v10);
    v750 = vzip1_s32(*&v14, *&v16);
    v759 = vzip1_s32(*&v20, *&v22);
    v37 = vdup_n_s32(0xFFFFDC72);
    v38 = vdup_n_s32(0x3537u);
    v39 = vmlal_s32(vmull_s32(v34, v38), v33, v37);
    v40 = vdup_n_s32(0x238Eu);
    v740 = vmlal_s32(vmull_s32(v34, v40), v33, v38);
    v734 = vsub_s32(v36, v33);
    v41 = vrshrn_high_n_s64(v33, v32, 0xEuLL);
    v42 = vrshrn_n_s64(v32, 0xEuLL);
    v43 = vmull_s32(*v12.i8, *v24.i8);
    v44 = vzip1_s32(*&v25, *&v27);
    v45 = vdupq_n_s32(0x2D41u);
    v46 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v11.i8, *v45.i8), 0xEuLL), vmull_s32(*v17.i8, *v45.i8), 0xEuLL);
    v47 = vdupq_n_s32(0x3F4Fu);
    v536 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v23.i8, *v45.i8), 0xEuLL), vmull_s32(*v28.i8, *v45.i8), 0xEuLL);
    *v17.i8 = vrshrn_n_s64(vmull_s32(*v29.i8, *v47.i8), 0xEuLL);
    v729 = vsub_s32(*v17.i8, v34);
    v48 = vrshrn_high_n_s64(v34, v43, 0xEuLL);
    v49 = vdup_n_s32(0xFFFFCAC9);
    v50 = vmlal_s32(vmull_s32(*v17.i8, v37), v36, v49);
    v51 = vmlal_s32(vmull_s32(*v17.i8, v38), v36, v37);
    v52 = vmull_high_s32(v29, v35);
    v53 = vrshrn_high_n_s64(v36, v52, 0xEuLL);
    *&v25 = vrshrn_n_s64(v52, 0xEuLL);
    v54 = vmull_high_s32(v29, v47);
    v55 = vrshrn_high_n_s64(*v17.i8, v54, 0xEuLL);
    v561 = vrshrn_n_s64(v43, 0xEuLL);
    v713 = vmlal_s32(vmull_s32(v561, v38), v42, v37);
    v555 = v42;
    *v39.i8 = vrshrn_n_s64(v39, 0xEuLL);
    *v23.i8 = vrshrn_n_s64(v50, 0xEuLL);
    v718 = vsub_s32(*v23.i8, *v39.i8);
    v549 = vrshrn_n_s64(v54, 0xEuLL);
    v703 = vmlal_s32(vmull_s32(v549, v37), *&v25, v49);
    v542 = v25;
    v763 = vaddq_s32(vrshrn_high_n_s64(*v23.i8, v703, 0xEuLL), vrshrn_high_n_s64(*v39.i8, v713, 0xEuLL));
    *v29.i8 = vrshrn_n_s64(vmull_s32(*v26.i8, v31), 0xEuLL);
    *v26.i8 = vrshrn_n_s64(vmull_s32(*v26.i8, *v24.i8), 0xEuLL);
    v56 = vzip2q_s64(v767, v791);
    *v50.i8 = vrshrn_n_s64(vmull_s32(*v56.i8, *v35.i8), 0xEuLL);
    v57 = vmlal_s32(vmull_s32(*v26.i8, v38), *v29.i8, v37);
    v58 = vmlal_s32(vmull_s32(*v26.i8, v40), *v29.i8, v38);
    v678 = vsub_s32(*v50.i8, *v29.i8);
    v465 = *v24.i8;
    v466 = v31;
    v59 = vmull_s32(v793, v31);
    v60 = vrshrn_high_n_s64(*v29.i8, v59, 0xEuLL);
    v61 = vrshrn_n_s64(v59, 0xEuLL);
    v62 = vmull_s32(v793, *v24.i8);
    *v59.i8 = vrshrn_n_s64(vmull_s32(*v56.i8, *v47.i8), 0xEuLL);
    v627 = vsub_s32(*v59.i8, *v26.i8);
    v63 = vrshrn_high_n_s64(*v26.i8, v62, 0xEuLL);
    v64 = vmlal_s32(vmull_s32(*v59.i8, v38), *v50.i8, v37);
    v464 = v35;
    v65 = vmull_high_s32(v56, v35);
    v462 = v47;
    v66 = vmull_high_s32(v56, v47);
    v67 = vrshrn_high_n_s64(*v59.i8, v66, 0xEuLL);
    *&v8 = vrshrn_n_s64(v66, 0xEuLL);
    v531 = vrshrn_n_s64(v62, 0xEuLL);
    v526 = v61;
    v613 = vmlal_s32(vmull_s32(v531, v38), v61, v37);
    *v66.i8 = vrshrn_n_s64(v57, 0xEuLL);
    *v62.i8 = vrshrn_n_s64(vmlal_s32(vmull_s32(*v59.i8, v37), *v50.i8, v49), 0xEuLL);
    v683 = vsub_s32(*v62.i8, *v66.i8);
    v521 = v8;
    v795 = v49;
    *v59.i8 = vrshrn_n_s64(v65, 0xEuLL);
    v597 = vmlal_s32(vmull_s32(*&v8, v37), *v59.i8, v49);
    v755 = vaddq_s32(vrshrn_high_n_s64(*v62.i8, v597, 0xEuLL), vrshrn_high_n_s64(*v66.i8, v613, 0xEuLL));
    v777 = vaddq_s32(v53, v41);
    v772 = vaddq_s32(vrshrn_high_n_s64(*v50.i8, v65, 0xEuLL), v60);
    v669 = vaddq_s32(v55, v48);
    v674 = vaddq_s32(v67, v63);
    v574 = vmlal_s32(vmull_s32(v561, v40), v42, v38);
    *v62.i8 = vrshrn_n_s64(v740, 0xEuLL);
    *v26.i8 = vrshrn_n_s64(v51, 0xEuLL);
    v607 = vsub_s32(*v26.i8, *v62.i8);
    v567 = vmlal_s32(vmull_s32(v549, v38), *&v25, v37);
    v68 = vaddq_s32(vrshrn_high_n_s64(*v26.i8, v567, 0xEuLL), vrshrn_high_n_s64(*v62.i8, v574, 0xEuLL));
    v796 = v40;
    v69 = vmlal_s32(vmull_s32(v531, v40), v61, v38);
    *v62.i8 = vrshrn_n_s64(v58, 0xEuLL);
    *v26.i8 = vrshrn_n_s64(v64, 0xEuLL);
    v580 = vsub_s32(*v26.i8, *v62.i8);
    v797 = v38;
    v798 = v37;
    v70 = vmlal_s32(vmull_s32(*&v8, v38), *v59.i8, v37);
    v48.i64[0] = v59.i64[0];
    v659 = vaddq_s32(vrshrn_high_n_s64(*v26.i8, v70, 0xEuLL), vrshrn_high_n_s64(*v62.i8, v69, 0xEuLL));
    v664 = v68;
    *v60.i8 = vdup_n_s32(0xC7Cu);
    *v58.i8 = vrshrn_n_s64(vmull_s32(v745, *v60.i8), 0xEuLL);
    *v50.i8 = vdup_n_s32(0x3EC5u);
    *v62.i8 = vrshrn_n_s64(vmull_s32(v745, *v50.i8), 0xEuLL);
    *v26.i8 = vsub_s32(*v62.i8, *v58.i8);
    *&v25 = vadd_s32(*v62.i8, *v58.i8);
    v71 = vmull_s32(v750, *v60.i8);
    v72 = vrshrn_high_n_s64(*v58.i8, v71, 0xEuLL);
    *v64.i8 = vrshrn_n_s64(v71, 0xEuLL);
    v73 = vmull_s32(v750, *v50.i8);
    v617 = vrshrn_high_n_s64(*v62.i8, v73, 0xEuLL);
    v622 = v72;
    *v62.i8 = vrshrn_n_s64(v73, 0xEuLL);
    v592 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v26.i8, *v45.i8), 0xEuLL), vmull_s32(vsub_s32(*v62.i8, *v64.i8), *v45.i8), 0xEuLL);
    v74 = vrshrn_n_s64(vmull_s32(v759, *v60.i8), 0xEuLL);
    *v66.i8 = vrshrn_n_s64(vmull_s32(v759, *v50.i8), 0xEuLL);
    *v26.i8 = vsub_s32(*v66.i8, v74);
    *v41.i8 = vadd_s32(*v66.i8, v74);
    v75 = vmull_s32(v44, *v60.i8);
    v76 = vrshrn_high_n_s64(v74, v75, 0xEuLL);
    *v67.i8 = vrshrn_n_s64(v75, 0xEuLL);
    v77 = vmull_s32(v44, *v50.i8);
    v602 = vrshrn_high_n_s64(*v66.i8, v77, 0xEuLL);
    v78 = vrshrn_n_s64(v77, 0xEuLL);
    v79 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v26.i8, *v45.i8), 0xEuLL), vmull_s32(vsub_s32(v78, *v67.i8), *v45.i8), 0xEuLL);
    v80 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*&v25, *v45.i8), 0xEuLL), vmull_s32(vadd_s32(*v62.i8, *v64.i8), *v45.i8), 0xEuLL);
    v81 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v41.i8, *v45.i8), 0xEuLL), vmull_s32(vadd_s32(v78, *v67.i8), *v45.i8), 0xEuLL);
    *v26.i8 = vdup_n_s32(0x324u);
    *v41.i8 = vrshrn_n_s64(vmull_s32(v786, *v26.i8), 0xEuLL);
    *v68.i8 = vdup_n_s32(0x3FECu);
    v82 = vrshrn_n_s64(vmull_s32(v786, *v68.i8), 0xEuLL);
    *&v16 = vdup_n_s32(0xFFFFC13B);
    v83 = vmlal_s32(vmull_s32(v82, *v60.i8), *v41.i8, *&v16);
    v84 = vmlal_s32(vmull_s32(v82, *v50.i8), *v41.i8, *v60.i8);
    *v55.i8 = vdup_n_s32(0xFFFFC4DF);
    *&v25 = vdup_n_s32(0x187Eu);
    v85 = vmlal_s32(vmull_s32(v82, *&v25), *v41.i8, *v55.i8);
    *v72.i8 = vdup_n_s32(0x3B21u);
    v86 = vmlal_s32(vmull_s32(v82, *v72.i8), *v41.i8, *&v25);
    v62.i64[0] = v41.i64[0];
    v87 = vmull_s32(v781, *v26.i8);
    v787 = vrshrn_high_n_s64(*v62.i8, v87, 0xEuLL);
    *v67.i8 = vrshrn_n_s64(v87, 0xEuLL);
    v88 = vmull_s32(v781, *v68.i8);
    v654 = vrshrn_high_n_s64(v82, v88, 0xEuLL);
    v89 = vrshrn_n_s64(v88, 0xEuLL);
    v90 = vmlal_s32(vmull_s32(v89, *v60.i8), *v67.i8, *&v16);
    v91 = vrshrn_n_s64(v83, 0xEuLL);
    *v84.i8 = vrshrn_n_s64(v84, 0xEuLL);
    v92 = vmlal_s32(vmull_s32(*v84.i8, *&v25), v91, *v55.i8);
    v93 = vmlal_s32(vmull_s32(*v84.i8, *v72.i8), v91, *&v25);
    v782 = vrshrn_high_n_s64(v91, v90, 0xEuLL);
    *v90.i8 = vrshrn_n_s64(v90, 0xEuLL);
    v94 = vmlal_s32(vmull_s32(v89, *v50.i8), *v67.i8, *v60.i8);
    v650 = vrshrn_high_n_s64(*v84.i8, v94, 0xEuLL);
    *v84.i8 = vrshrn_n_s64(v94, 0xEuLL);
    v699 = vrshrn_high_n_s64(vrshrn_n_s64(v92, 0xEuLL), vmlal_s32(vmull_s32(*v84.i8, *&v25), *v90.i8, *v55.i8), 0xEuLL);
    *v92.i8 = vrshrn_n_s64(vmull_s32(*v767.i8, *v26.i8), 0xEuLL);
    v95 = vrshrn_n_s64(vmull_s32(*v767.i8, *v68.i8), 0xEuLL);
    v96 = vmlal_s32(vmull_s32(v95, *v60.i8), *v92.i8, *&v16);
    v97 = vmlal_s32(vmull_s32(v95, *v50.i8), *v92.i8, *v60.i8);
    v98 = vmlal_s32(vmull_s32(v95, *&v25), *v92.i8, *v55.i8);
    v99 = vmlal_s32(vmull_s32(v95, *v72.i8), *v92.i8, *&v25);
    *&v14 = v92.i64[0];
    v458 = *v68.i8;
    v459 = *v26.i8;
    v100 = vmull_s32(*v791.i8, *v26.i8);
    v724 = vrshrn_high_n_s64(*&v14, v100, 0xEuLL);
    *&v14 = vrshrn_n_s64(v100, 0xEuLL);
    v101 = vmull_s32(*v791.i8, *v68.i8);
    v644 = vrshrn_high_n_s64(v95, v101, 0xEuLL);
    *v94.i8 = vrshrn_n_s64(v101, 0xEuLL);
    v457 = v16;
    v102 = vmlal_s32(vmull_s32(*v94.i8, *v60.i8), *&v14, *&v16);
    *v96.i8 = vrshrn_n_s64(v96, 0xEuLL);
    v103 = vrshrn_n_s64(v97, 0xEuLL);
    v104 = vmlal_s32(vmull_s32(v103, *&v25), *v96.i8, *v55.i8);
    v105 = vmlal_s32(vmull_s32(v103, *v72.i8), *v96.i8, *&v25);
    v768 = vrshrn_high_n_s64(*v96.i8, v102, 0xEuLL);
    *v96.i8 = vrshrn_n_s64(v102, 0xEuLL);
    v460 = *v50.i8;
    v461 = *v60.i8;
    v106 = vmlal_s32(vmull_s32(*v94.i8, *v50.i8), *&v14, *v60.i8);
    v107 = vrshrn_high_n_s64(v103, v106, 0xEuLL);
    *v106.i8 = vrshrn_n_s64(v106, 0xEuLL);
    v689 = vrshrn_high_n_s64(vrshrn_n_s64(v104, 0xEuLL), vmlal_s32(vmull_s32(*v106.i8, *&v25), *v96.i8, *v55.i8), 0xEuLL);
    v586 = vrshrn_high_n_s64(vrshrn_n_s64(v93, 0xEuLL), vmlal_s32(vmull_s32(*v84.i8, *v72.i8), *v90.i8, *&v25), 0xEuLL);
    v632 = vrshrn_high_n_s64(vrshrn_n_s64(v105, 0xEuLL), vmlal_s32(vmull_s32(*v106.i8, *v72.i8), *v96.i8, *&v25), 0xEuLL);
    v638 = v107;
    v760 = vrshrn_high_n_s64(vrshrn_n_s64(v85, 0xEuLL), vmlal_s32(vmull_s32(v89, *&v25), *v67.i8, *v55.i8), 0xEuLL);
    v751 = vrshrn_high_n_s64(vrshrn_n_s64(v98, 0xEuLL), vmlal_s32(vmull_s32(*v94.i8, *&v25), *&v14, *v55.i8), 0xEuLL);
    v108 = vrshrn_high_n_s64(vrshrn_n_s64(v86, 0xEuLL), vmlal_s32(vmull_s32(v89, *v72.i8), *v67.i8, *&v25), 0xEuLL);
    v109 = vrshrn_high_n_s64(vrshrn_n_s64(v99, 0xEuLL), vmlal_s32(vmull_s32(*v94.i8, *v72.i8), *&v14, *&v25), 0xEuLL);
    *v97.i8 = vsub_s32(v542, v555);
    *v96.i8 = vsub_s32(*v48.i8, v526);
    *v104.i8 = vsub_s32(v549, v561);
    *v105.i8 = vsub_s32(v521, v531);
    *v98.i8 = vdup_n_s32(0xFFFFE782);
    v746 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v729, *v55.i8), v734, *v98.i8), 0xEuLL), vmlal_s32(vmull_s32(*v104.i8, *v55.i8), *v97.i8, *v98.i8), 0xEuLL);
    v741 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v627, *v55.i8), v678, *v98.i8), 0xEuLL), vmlal_s32(vmull_s32(*v105.i8, *v55.i8), *v96.i8, *v98.i8), 0xEuLL);
    v110 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v729, *&v25), v734, *v55.i8), 0xEuLL), vmlal_s32(vmull_s32(*v104.i8, *&v25), *v97.i8, *v55.i8), 0xEuLL);
    v111 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v627, *&v25), v678, *v55.i8), 0xEuLL), vmlal_s32(vmull_s32(*v105.i8, *&v25), *v96.i8, *v55.i8), 0xEuLL);
    *v97.i8 = vsub_s32(vrshrn_n_s64(v703, 0xEuLL), vrshrn_n_s64(v713, 0xEuLL));
    *v96.i8 = vsub_s32(vrshrn_n_s64(v597, 0xEuLL), vrshrn_n_s64(v613, 0xEuLL));
    *v105.i8 = vsub_s32(vrshrn_n_s64(v567, 0xEuLL), vrshrn_n_s64(v574, 0xEuLL));
    *v60.i8 = vsub_s32(vrshrn_n_s64(v70, 0xEuLL), vrshrn_n_s64(v69, 0xEuLL));
    v735 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v607, *v55.i8), v718, *v98.i8), 0xEuLL), vmlal_s32(vmull_s32(*v105.i8, *v55.i8), *v97.i8, *v98.i8), 0xEuLL);
    v792 = *v98.i8;
    v730 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v580, *v55.i8), v683, *v98.i8), 0xEuLL), vmlal_s32(vmull_s32(*v60.i8, *v55.i8), *v96.i8, *v98.i8), 0xEuLL);
    v112 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v607, *&v25), v718, *v55.i8), 0xEuLL), vmlal_s32(vmull_s32(*v105.i8, *&v25), *v97.i8, *v55.i8), 0xEuLL);
    v113 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v580, *&v25), v683, *v55.i8), 0xEuLL), vmlal_s32(vmull_s32(*v60.i8, *&v25), *v96.i8, *v55.i8), 0xEuLL);
    v114 = vaddq_s32(v46, v622);
    v115 = vsubq_s32(v46, v622);
    v623 = vaddq_s32(v592, v46);
    v628 = v114;
    v575 = vsubq_s32(v46, v592);
    v581 = v115;
    v704 = vaddq_s32(v80, v46);
    v714 = vaddq_s32(v46, v617);
    v550 = vsubq_s32(v536, v76);
    v556 = vsubq_s32(v46, v617);
    v614 = vaddq_s32(v79, v536);
    v618 = vaddq_s32(v536, v76);
    v562 = vsubq_s32(v536, v79);
    v568 = vsubq_s32(v46, v80);
    v608 = vaddq_s32(v81, v536);
    v116 = vsubq_s32(v536, v81);
    v719 = vaddq_s32(v536, v602);
    v537 = vsubq_s32(v536, v602);
    v543 = v116;
    v117 = vdupq_n_s32(0x646u);
    *v50.i8 = vrshrn_n_s64(vmull_s32(*v709.i8, *v117.i8), 0xEuLL);
    v118 = vdupq_n_s32(0x3FB1u);
    *v97.i8 = vrshrn_n_s64(vmull_s32(*v709.i8, *v118.i8), 0xEuLL);
    v119 = vmlal_s32(vmull_s32(*v97.i8, *&v25), *v50.i8, *v55.i8);
    v120 = vmlal_s32(vmull_s32(*v97.i8, *v72.i8), *v50.i8, *&v25);
    *v93.i8 = vsub_s32(*v97.i8, *v50.i8);
    *v80.i8 = vadd_s32(*v97.i8, *v50.i8);
    v121 = vmull_high_s32(v709, v117);
    v122 = vrshrn_high_n_s64(*v50.i8, v121, 0xEuLL);
    *v70.i8 = vrshrn_n_s64(v121, 0xEuLL);
    v123 = vmull_high_s32(v709, v118);
    v710 = vrshrn_high_n_s64(*v97.i8, v123, 0xEuLL);
    *&v16 = vrshrn_n_s64(v123, 0xEuLL);
    v124 = vmlal_s32(vmull_s32(*&v16, *&v25), *v70.i8, *v55.i8);
    *v123.i8 = vrshrn_n_s64(v119, 0xEuLL);
    *v119.i8 = vrshrn_n_s64(v120, 0xEuLL);
    *v120.i8 = vsub_s32(*v119.i8, *v123.i8);
    *v81.i8 = vadd_s32(*v119.i8, *v123.i8);
    v125 = vrshrn_high_n_s64(*v123.i8, v124, 0xEuLL);
    *v69.i8 = vrshrn_n_s64(v124, 0xEuLL);
    v126 = vmlal_s32(vmull_s32(*&v16, *v72.i8), *v70.i8, *&v25);
    v603 = vrshrn_high_n_s64(*v119.i8, v126, 0xEuLL);
    *v48.i8 = vrshrn_n_s64(v126, 0xEuLL);
    v127 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v120.i8, *v45.i8), 0xEuLL), vmull_s32(vsub_s32(*v48.i8, *v69.i8), *v45.i8), 0xEuLL);
    *v85.i8 = vrshrn_n_s64(vmull_s32(*v693.i8, *v117.i8), 0xEuLL);
    *v76.i8 = vrshrn_n_s64(vmull_s32(*v693.i8, *v118.i8), 0xEuLL);
    v128 = vmlal_s32(vmull_s32(*v76.i8, *&v25), *v85.i8, *v55.i8);
    v129 = vmlal_s32(vmull_s32(*v76.i8, *v72.i8), *v85.i8, *&v25);
    *v79.i8 = vsub_s32(*v76.i8, *v85.i8);
    *v90.i8 = vadd_s32(*v76.i8, *v85.i8);
    v453 = v118;
    v454 = v117;
    v130 = vmull_high_s32(v693, v117);
    v131 = vrshrn_high_n_s64(*v85.i8, v130, 0xEuLL);
    *v116.i8 = vrshrn_n_s64(v130, 0xEuLL);
    v132 = vmull_high_s32(v693, v118);
    v694 = vrshrn_high_n_s64(*v76.i8, v132, 0xEuLL);
    *v117.i8 = vrshrn_n_s64(v132, 0xEuLL);
    v455 = v25;
    v456 = *v55.i8;
    v133 = vmlal_s32(vmull_s32(*v117.i8, *&v25), *v116.i8, *v55.i8);
    *v128.i8 = vrshrn_n_s64(v128, 0xEuLL);
    *v129.i8 = vrshrn_n_s64(v129, 0xEuLL);
    *v118.i8 = vsub_s32(*v129.i8, *v128.i8);
    *v55.i8 = vadd_s32(*v129.i8, *v128.i8);
    v134 = vrshrn_high_n_s64(*v128.i8, v133, 0xEuLL);
    *v133.i8 = vrshrn_n_s64(v133, 0xEuLL);
    v794 = *v72.i8;
    v135 = vmlal_s32(vmull_s32(*v117.i8, *v72.i8), *v116.i8, *&v25);
    v598 = vrshrn_high_n_s64(*v129.i8, v135, 0xEuLL);
    *v135.i8 = vrshrn_n_s64(v135, 0xEuLL);
    v136 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v118.i8, *v45.i8), 0xEuLL), vmull_s32(vsub_s32(*v135.i8, *v133.i8), *v45.i8), 0xEuLL);
    v137 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v81.i8, *v45.i8), 0xEuLL), vmull_s32(vadd_s32(*v48.i8, *v69.i8), *v45.i8), 0xEuLL);
    v593 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v55.i8, *v45.i8), 0xEuLL), vmull_s32(vadd_s32(*v135.i8, *v133.i8), *v45.i8), 0xEuLL);
    v138 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v93.i8, *v45.i8), 0xEuLL), vmull_s32(vsub_s32(*&v16, *v70.i8), *v45.i8), 0xEuLL);
    v139 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v79.i8, *v45.i8), 0xEuLL), vmull_s32(vsub_s32(*v117.i8, *v116.i8), *v45.i8), 0xEuLL);
    v140 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v80.i8, *v45.i8), 0xEuLL), vmull_s32(vadd_s32(*&v16, *v70.i8), *v45.i8), 0xEuLL);
    v141 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v90.i8, *v45.i8), 0xEuLL), vmull_s32(vadd_s32(*v117.i8, *v116.i8), *v45.i8), 0xEuLL);
    v142 = vsubq_s32(v108, v110);
    v684 = vaddq_s32(v110, v108);
    v143 = vsubq_s32(v109, v111);
    v679 = vaddq_s32(v111, v109);
    v144 = vsubq_s32(v586, v112);
    v145 = vsubq_s32(v632, v113);
    v495 = vaddq_s32(v113, v632);
    v497 = vaddq_s32(v112, v586);
    v146 = vsubq_s32(v650, v664);
    v147 = vsubq_s32(v638, v659);
    v508 = vaddq_s32(v659, v638);
    v511 = vaddq_s32(v664, v650);
    v148 = vsubq_s32(v654, v669);
    v149 = vsubq_s32(v644, v674);
    v527 = vaddq_s32(v674, v644);
    v532 = vaddq_s32(v669, v654);
    v639 = vsubq_s32(v556, v122);
    v645 = vaddq_s32(v556, v122);
    v150 = vaddq_s32(v537, v131);
    v633 = vsubq_s32(v537, v131);
    v151 = vaddq_s32(v568, v125);
    v152 = vsubq_s32(v568, v125);
    v655 = vaddq_s32(v543, v134);
    v569 = vsubq_s32(v543, v134);
    v665 = vaddq_s32(v127, v575);
    v670 = vaddq_s32(v136, v562);
    v557 = v152;
    v675 = vaddq_s32(v138, v581);
    v587 = vsubq_s32(v581, v138);
    v153 = vaddq_s32(v139, v550);
    v582 = vsubq_s32(v550, v139);
    v154 = vsubq_s32(v760, v746);
    v551 = vsubq_s32(v142, v154);
    v155 = vaddq_s32(v142, v154);
    v156 = vsubq_s32(v751, v741);
    v514 = vsubq_s32(v143, v156);
    v157 = vaddq_s32(v143, v156);
    v484 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v155.i8, *v45.i8), 0xEuLL), vmull_high_s32(v155, v45), 0xEuLL);
    v486 = v153;
    v488 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v157.i8, *v45.i8), 0xEuLL), vmull_high_s32(v157, v45), 0xEuLL);
    v158 = vsubq_s32(v699, v735);
    v159 = vsubq_s32(v144, v158);
    v522 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v159.i8, *v45.i8), 0xEuLL), vmull_high_s32(v159, v45), 0xEuLL);
    v160 = vsubq_s32(v689, v730);
    v161 = vsubq_s32(v145, v160);
    v517 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v161.i8, *v45.i8), 0xEuLL), vmull_high_s32(v161, v45), 0xEuLL);
    v162 = vaddq_s32(v144, v158);
    v163 = vaddq_s32(v145, v160);
    v480 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v163.i8, *v45.i8), 0xEuLL), vmull_high_s32(v163, v45), 0xEuLL);
    v482 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v162.i8, *v45.i8), 0xEuLL), vmull_high_s32(v162, v45), 0xEuLL);
    v164 = vsubq_s32(v782, v763);
    v165 = vsubq_s32(v146, v164);
    v166 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v165.i8, *v45.i8), 0xEuLL), vmull_high_s32(v165, v45), 0xEuLL);
    v167 = vsubq_s32(v768, v755);
    v168 = vsubq_s32(v147, v167);
    v469 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v168.i8, *v45.i8), 0xEuLL), vmull_high_s32(v168, v45), 0xEuLL);
    v169 = vaddq_s32(v146, v164);
    v170 = vaddq_s32(v147, v167);
    v476 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v170.i8, *v45.i8), 0xEuLL), vmull_high_s32(v170, v45), 0xEuLL);
    v478 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v169.i8, *v45.i8), 0xEuLL), vmull_high_s32(v169, v45), 0xEuLL);
    v171 = vsubq_s32(v787, v777);
    v172 = vsubq_s32(v148, v171);
    v173 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v172.i8, *v45.i8), 0xEuLL), vmull_high_s32(v172, v45), 0xEuLL);
    v174 = vaddq_s32(v148, v171);
    v175 = vsubq_s32(v724, v772);
    v176 = vsubq_s32(v149, v175);
    v177 = vaddq_s32(v149, v175);
    v472 = v173;
    v474 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v174.i8, *v45.i8), 0xEuLL), vmull_high_s32(v174, v45), 0xEuLL);
    v538 = vaddq_s32(v719, v694);
    v544 = vaddq_s32(v714, v710);
    v799[0] = vaddq_s32(v544, v532);
    v799[1] = vaddq_s32(v538, v527);
    v503 = vaddq_s32(v608, v598);
    v505 = vaddq_s32(v704, v603);
    v799[2] = vaddq_s32(v505, v511);
    v799[3] = vaddq_s32(v503, v508);
    v499 = vaddq_s32(v593, v614);
    v501 = vaddq_s32(v137, v623);
    v799[4] = vaddq_s32(v501, v497);
    v799[5] = vaddq_s32(v499, v495);
    v490 = vaddq_s32(v141, v618);
    v492 = vaddq_s32(v140, v628);
    v799[6] = vaddq_s32(v492, v684);
    v799[7] = vaddq_s32(v490, v679);
    v178 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v177.i8, *v45.i8), 0xEuLL), vmull_high_s32(v177, v45), 0xEuLL);
    v800[0] = vaddq_s32(v484, v675);
    v800[1] = vaddq_s32(v488, v153);
    v800[2] = vaddq_s32(v482, v665);
    v800[3] = vaddq_s32(v480, v670);
    v800[4] = vaddq_s32(v478, v151);
    v800[5] = vaddq_s32(v476, v655);
    v800[6] = vaddq_s32(v474, v645);
    v800[7] = vaddq_s32(v178, v150);
    v179 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v176.i8, *v45.i8), 0xEuLL), vmull_high_s32(v176, v45), 0xEuLL);
    v801[0] = vaddq_s32(v173, v639);
    v801[1] = vaddq_s32(v179, v633);
    v180 = vsubq_s32(v562, v136);
    v181 = vsubq_s32(v575, v127);
    v801[2] = vaddq_s32(v166, v557);
    v801[3] = vaddq_s32(v469, v569);
    v801[4] = vaddq_s32(v522, v181);
    v801[5] = vaddq_s32(v517, v180);
    v182 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v551.i8, *v45.i8), 0xEuLL), vmull_high_s32(v551, v45), 0xEuLL);
    v463 = v45;
    v183 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v514.i8, *v45.i8), 0xEuLL), vmull_high_s32(v514, v45), 0xEuLL);
    v801[6] = vaddq_s32(v182, v587);
    v801[7] = vaddq_s32(v183, v582);
    v184 = vaddq_s32(v746, v760);
    v185 = vaddq_s32(v741, v751);
    v186 = vsubq_s32(v628, v140);
    v187 = vsubq_s32(v618, v141);
    v802[0] = vaddq_s32(v186, v184);
    v802[1] = vaddq_s32(v187, v185);
    v188 = vaddq_s32(v735, v699);
    v189 = vaddq_s32(v730, v689);
    v190 = vsubq_s32(v623, v137);
    v191 = vsubq_s32(v614, v593);
    v802[2] = vaddq_s32(v190, v188);
    v802[3] = vaddq_s32(v191, v189);
    v192 = vaddq_s32(v763, v782);
    v193 = vaddq_s32(v755, v768);
    v194 = vsubq_s32(v704, v603);
    v195 = vsubq_s32(v608, v598);
    v802[4] = vaddq_s32(v194, v192);
    v802[5] = vaddq_s32(v195, v193);
    v196 = vaddq_s32(v777, v787);
    v197 = vaddq_s32(v772, v724);
    v198 = vsubq_s32(v714, v710);
    v199 = vsubq_s32(v719, v694);
    v802[6] = vaddq_s32(v198, v196);
    v802[7] = vaddq_s32(v199, v197);
    v802[8] = vsubq_s32(v198, v196);
    v802[9] = vsubq_s32(v199, v197);
    v802[10] = vsubq_s32(v194, v192);
    v802[11] = vsubq_s32(v195, v193);
    v802[12] = vsubq_s32(v190, v188);
    v802[13] = vsubq_s32(v191, v189);
    v802[14] = vsubq_s32(v186, v184);
    v802[15] = vsubq_s32(v187, v185);
    v802[16] = vsubq_s32(v587, v182);
    v802[17] = vsubq_s32(v582, v183);
    v802[18] = vsubq_s32(v181, v522);
    v802[19] = vsubq_s32(v180, v517);
    v802[20] = vsubq_s32(v557, v166);
    v802[21] = vsubq_s32(v569, v469);
    v802[22] = vsubq_s32(v639, v472);
    v802[23] = vsubq_s32(v633, v179);
    v802[24] = vsubq_s32(v645, v474);
    v802[25] = vsubq_s32(v150, v178);
    v802[26] = vsubq_s32(v151, v478);
    v802[27] = vsubq_s32(v655, v476);
    v802[28] = vsubq_s32(v665, v482);
    v802[29] = vsubq_s32(v670, v480);
    v802[30] = vsubq_s32(v675, v484);
    v802[31] = vsubq_s32(v486, v488);
    v802[32] = vsubq_s32(v492, v684);
    v802[33] = vsubq_s32(v490, v679);
    v802[34] = vsubq_s32(v501, v497);
    v802[35] = vsubq_s32(v499, v495);
    v802[36] = vsubq_s32(v505, v511);
    v802[37] = vsubq_s32(v503, v508);
    v802[38] = vsubq_s32(v544, v532);
    v802[39] = vsubq_s32(v538, v527);
    v200 = 2 * a3;
    v201 = -8;
    v452 = vdupq_n_s16(~(-1 << a4));
    do
    {
      v202 = &v799[v7 * 16];
      v203 = v202[1];
      v204 = v202[2];
      v205 = v202[3];
      v206 = v202[4];
      v207 = v202[5];
      v208 = v202[6];
      v209 = v202[7];
      v210 = v202[8];
      v211 = v202[9];
      v212 = v202[10];
      v213 = v202[11];
      v214 = v202[12];
      v215 = v202[13];
      v216 = v202[14];
      v217 = v202[15];
      v218 = vtrn1q_s32(*v202, v204);
      v219 = vtrn2q_s32(*v202, v204);
      v220 = vtrn1q_s32(v206, v208);
      v671 = vtrn2q_s32(v206, v208);
      v221 = vtrn1q_s32(v210, v212);
      v742 = vtrn2q_s32(v210, v212);
      v222 = vtrn1q_s32(v214, v216);
      v747 = vtrn2q_s32(v214, v216);
      v223 = vtrn1q_s32(v203, v205);
      v773 = *v223.i8;
      v224 = vtrn2q_s32(v203, v205);
      v736 = *v224.i8;
      v225 = vtrn1q_s32(v207, v209);
      v778 = *v225.i8;
      v226 = vtrn2q_s32(v207, v209);
      v752 = *v226.i8;
      v227 = vtrn1q_s32(v211, v213);
      v783 = *v227.i8;
      v228 = vtrn2q_s32(v211, v213);
      v764 = *v228.i8;
      v229 = vtrn1q_s32(v215, v217);
      v788 = *v229.i8;
      v230 = vtrn2q_s32(v215, v217);
      v769 = *v230.i8;
      v231 = vzip2q_s64(v223, v225);
      v232 = vzip2q_s64(v224, v226);
      v233 = vzip2q_s64(v227, v229);
      v234 = vzip2q_s64(v228, v230);
      v235 = vdupq_n_s32(0xFFFFEA70);
      v236 = vmull_s32(*v232.i8, *v235.i8);
      v237 = vmull_high_s32(v232, v235);
      v238 = vmull_s32(*v234.i8, *v235.i8);
      v239 = vmull_high_s32(v234, v235);
      v240 = vdupq_n_s32(0x3C42u);
      v241 = vmull_s32(*v232.i8, *v240.i8);
      v242 = vmull_high_s32(v232, v240);
      v243 = vmull_s32(*v234.i8, *v240.i8);
      v244 = vmull_high_s32(v234, v240);
      v245 = vdupq_n_s32(0xFFFFED6C);
      v246 = vmull_s32(*v231.i8, *v245.i8);
      v247 = vmull_high_s32(v231, v245);
      v248 = vmull_s32(*v233.i8, *v245.i8);
      v249 = vmull_high_s32(v233, v245);
      v250 = vdupq_n_s32(0x3D3Fu);
      v251 = vmull_s32(*v231.i8, *v250.i8);
      v252 = vmull_high_s32(v231, v250);
      v253 = vmull_s32(*v233.i8, *v250.i8);
      v254 = vmull_high_s32(v233, v250);
      *v250.i8 = vrshrn_n_s64(v236, 0xEuLL);
      *v217.i8 = vrshrn_n_s64(v238, 0xEuLL);
      *v236.i8 = vrshrn_n_s64(v241, 0xEuLL);
      v705 = vrshrn_high_n_s64(*v236.i8, v242, 0xEuLL);
      v720 = vrshrn_n_s64(v242, 0xEuLL);
      v255 = vrshrn_n_s64(v243, 0xEuLL);
      v731 = vrshrn_n_s64(v244, 0xEuLL);
      *v231.i8 = vdup_n_s32(0xFFFFF384);
      v242.i64[0] = v236.i64[0];
      v583 = *v236.i8;
      v256 = vmlal_s32(vmull_s32(*v236.i8, v457), *v250.i8, *v231.i8);
      v715 = vrshrn_n_s64(v237, 0xEuLL);
      v685 = vmlal_s32(vmull_s32(v720, v457), v715, *v231.i8);
      v695 = vmlal_s32(vmull_s32(v255, v457), *v217.i8, *v231.i8);
      v725 = vrshrn_n_s64(v239, 0xEuLL);
      v700 = vmlal_s32(vmull_s32(v731, v457), v725, *v231.i8);
      v257 = vrshrn_high_n_s64(*v250.i8, v237, 0xEuLL);
      v258 = vzip2q_s64(v218, v220);
      v756 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v218.i8, *v463.i8), 0xEuLL), vmull_s32(*v220.i8, *v463.i8), 0xEuLL);
      v259 = vzip2q_s64(v221, v222);
      v761 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v221.i8, *v463.i8), 0xEuLL), vmull_s32(*v222.i8, *v463.i8), 0xEuLL);
      v260 = vrshrn_high_n_s64(*v217.i8, v239, 0xEuLL);
      *v239.i8 = vrshrn_n_s64(vmull_s32(*v258.i8, *v454.i8), 0xEuLL);
      *v233.i8 = vrshrn_n_s64(vmull_s32(*v258.i8, *v453.i8), 0xEuLL);
      v261 = vmlal_s32(vmull_s32(*v233.i8, v455), *v239.i8, v456);
      v262 = vmlal_s32(vmull_s32(*v233.i8, v794), *v239.i8, v455);
      v236.i64[0] = v239.i64[0];
      v263 = vmull_high_s32(v258, v454);
      v264 = vrshrn_high_n_s64(*v236.i8, v263, 0xEuLL);
      *v263.i8 = vrshrn_n_s64(v263, 0xEuLL);
      v265 = vmull_high_s32(v258, v453);
      v656 = v264;
      v660 = vrshrn_high_n_s64(*v233.i8, v265, 0xEuLL);
      *v233.i8 = vrshrn_n_s64(v265, 0xEuLL);
      v651 = vrshrn_high_n_s64(vrshrn_n_s64(v261, 0xEuLL), vmlal_s32(vmull_s32(*v233.i8, v455), *v263.i8, v456), 0xEuLL);
      v266 = vmlal_s32(vmull_s32(*v233.i8, v794), *v263.i8, v455);
      *v263.i8 = vrshrn_n_s64(vmull_s32(*v259.i8, *v454.i8), 0xEuLL);
      *v261.i8 = vrshrn_n_s64(vmull_s32(*v259.i8, *v453.i8), 0xEuLL);
      v267 = vmlal_s32(vmull_s32(*v261.i8, v455), *v263.i8, v456);
      v268 = vmlal_s32(vmull_s32(*v261.i8, v794), *v263.i8, v455);
      v269 = *v263.i8;
      v270 = vmull_high_s32(v259, v454);
      v271 = vrshrn_high_n_s64(v269, v270, 0xEuLL);
      *v270.i8 = vrshrn_n_s64(v270, 0xEuLL);
      v272 = vmull_high_s32(v259, v453);
      v634 = v271;
      v640 = vrshrn_high_n_s64(*v261.i8, v272, 0xEuLL);
      *v272.i8 = vrshrn_n_s64(v272, 0xEuLL);
      v629 = vrshrn_high_n_s64(vrshrn_n_s64(v267, 0xEuLL), vmlal_s32(vmull_s32(*v272.i8, v455), *v270.i8, v456), 0xEuLL);
      v646 = vrshrn_high_n_s64(vrshrn_n_s64(v262, 0xEuLL), v266, 0xEuLL);
      v666 = vrshrn_high_n_s64(vrshrn_n_s64(v268, 0xEuLL), vmlal_s32(vmull_s32(*v272.i8, v794), *v270.i8, v455), 0xEuLL);
      *v267.i8 = vrshrn_n_s64(v246, 0xEuLL);
      *v266.i8 = vrshrn_n_s64(v251, 0xEuLL);
      v588 = vmlal_s32(vmull_s32(*v266.i8, v456), *v267.i8, v792);
      v273 = vmlal_s32(vmull_s32(*v266.i8, v455), *v267.i8, v456);
      v274 = vrshrn_high_n_s64(*v267.i8, v247, 0xEuLL);
      v609 = v273;
      *v273.i8 = vrshrn_n_s64(v247, 0xEuLL);
      v619 = vrshrn_high_n_s64(*v266.i8, v252, 0xEuLL);
      *v246.i8 = vrshrn_n_s64(v252, 0xEuLL);
      v275 = vmlal_s32(vmull_s32(*v246.i8, v456), *v273.i8, v792);
      v276 = vmlal_s32(vmull_s32(*v246.i8, v455), *v273.i8, v456);
      *v247.i8 = vrshrn_n_s64(v248, 0xEuLL);
      *v246.i8 = vrshrn_n_s64(v253, 0xEuLL);
      v570 = vmlal_s32(vmull_s32(*v246.i8, v456), *v247.i8, v792);
      v563 = vmlal_s32(vmull_s32(*v246.i8, v455), *v247.i8, v456);
      v599 = vrshrn_high_n_s64(*v247.i8, v249, 0xEuLL);
      *v261.i8 = vrshrn_n_s64(v249, 0xEuLL);
      v604 = vrshrn_high_n_s64(*v246.i8, v254, 0xEuLL);
      *v246.i8 = vrshrn_n_s64(v254, 0xEuLL);
      v277 = vmlal_s32(vmull_s32(*v246.i8, v456), *v261.i8, v792);
      v545 = vmlal_s32(vmull_s32(*v246.i8, v455), *v261.i8, v456);
      *v246.i8 = vrshrn_n_s64(vmull_s32(*v219.i8, v459), 0xEuLL);
      v624 = vmlal_s32(vmull_s32(*v242.i8, v461), *v250.i8, v457);
      v523 = vsub_s32(*v246.i8, *v250.i8);
      *v243.i8 = vrshrn_n_s64(vmull_s32(*v219.i8, v458), 0xEuLL);
      v278 = vmlal_s32(vmull_s32(*v243.i8, v461), *v246.i8, v457);
      v496 = vmlal_s32(vmull_s32(*v243.i8, v460), *v246.i8, v461);
      v477 = vmull_s32(*v671.i8, v459);
      v680 = vaddq_s32(v257, vrshrn_high_n_s64(*v246.i8, v477, 0xEuLL));
      v500 = vmlal_s32(vmull_s32(v255, v461), *v217.i8, v457);
      *v257.i8 = vrshrn_n_s64(vmull_s32(*v742.i8, v459), 0xEuLL);
      *v251.i8 = vrshrn_n_s64(vmull_s32(*v742.i8, v458), 0xEuLL);
      v506 = vsub_s32(*v257.i8, *v217.i8);
      v485 = vmlal_s32(vmull_s32(*v251.i8, v461), *v257.i8, v457);
      v493 = vmlal_s32(vmull_s32(*v251.i8, v460), *v257.i8, v461);
      v473 = vmull_s32(*v747.i8, v459);
      v676 = vaddq_s32(v260, vrshrn_high_n_s64(*v257.i8, v473, 0xEuLL));
      v279 = vzip2q_s64(v219, v671);
      *v262.i8 = vrshrn_n_s64(vmull_s32(v736, v466), 0xEuLL);
      *v217.i8 = vrshrn_n_s64(vmull_s32(v736, v465), 0xEuLL);
      *v260.i8 = vrshrn_n_s64(vmull_s32(*v279.i8, *v464.i8), 0xEuLL);
      v509 = vmlal_s32(vmull_s32(*v217.i8, v797), *v262.i8, v798);
      v576 = vmlal_s32(vmull_s32(*v217.i8, v796), *v262.i8, v797);
      v558 = vsub_s32(*v260.i8, *v262.i8);
      v483 = vmull_s32(v752, v466);
      *v268.i8 = vrshrn_n_s64(vmull_s32(*v279.i8, *v462.i8), 0xEuLL);
      v487 = vmlal_s32(vmull_s32(*v268.i8, v798), *v260.i8, v795);
      v518 = vmlal_s32(vmull_s32(*v268.i8, v797), *v260.i8, v798);
      v475 = vmull_high_s32(v279, v464);
      v737 = vaddq_s32(vrshrn_high_n_s64(*v260.i8, v475, 0xEuLL), vrshrn_high_n_s64(*v262.i8, v483, 0xEuLL));
      v280 = vzip2q_s64(v742, v747);
      *v260.i8 = vrshrn_n_s64(vmull_s32(v764, v466), 0xEuLL);
      *v273.i8 = vrshrn_n_s64(vmull_s32(v764, v465), 0xEuLL);
      *v272.i8 = vrshrn_n_s64(vmull_s32(*v280.i8, *v464.i8), 0xEuLL);
      v502 = vmlal_s32(vmull_s32(*v273.i8, v797), *v260.i8, v798);
      v515 = vmlal_s32(vmull_s32(*v273.i8, v796), *v260.i8, v797);
      v533 = vsub_s32(*v272.i8, *v260.i8);
      v491 = vmull_s32(v769, v466);
      *v249.i8 = vrshrn_n_s64(vmull_s32(*v280.i8, *v462.i8), 0xEuLL);
      v498 = vmlal_s32(vmull_s32(*v249.i8, v798), *v272.i8, v795);
      v504 = vmlal_s32(vmull_s32(*v249.i8, v797), *v272.i8, v798);
      v489 = vmull_high_s32(v280, v464);
      v743 = vaddq_s32(vrshrn_high_n_s64(*v272.i8, v489, 0xEuLL), vrshrn_high_n_s64(*v260.i8, v491, 0xEuLL));
      v281 = vrshrn_high_n_s64(vrshrn_n_s64(v588, 0xEuLL), v275, 0xEuLL);
      v528 = vsub_s32(*v268.i8, *v217.i8);
      v468 = vmull_high_s32(v279, v462);
      v470 = vmull_s32(v752, v465);
      v765 = vaddq_s32(vrshrn_high_n_s64(*v268.i8, v468, 0xEuLL), vrshrn_high_n_s64(*v217.i8, v470, 0xEuLL));
      v512 = vsub_s32(*v249.i8, *v273.i8);
      v479 = vmull_high_s32(v280, v462);
      v481 = vmull_s32(v769, v465);
      v770 = vaddq_s32(vrshrn_high_n_s64(*v249.i8, v479, 0xEuLL), vrshrn_high_n_s64(*v273.i8, v481, 0xEuLL));
      v282 = vrshrn_high_n_s64(vrshrn_n_s64(v570, 0xEuLL), v277, 0xEuLL);
      *v257.i8 = vsub_s32(*v243.i8, v583);
      v283 = vmull_s32(*v671.i8, v458);
      v753 = vaddq_s32(v705, vrshrn_high_n_s64(*v243.i8, v283, 0xEuLL));
      *v273.i8 = vsub_s32(*v251.i8, v255);
      v284 = vmull_s32(*v747.i8, v458);
      v748 = vaddq_s32(vrshrn_high_n_s64(v255, v244, 0xEuLL), vrshrn_high_n_s64(*v251.i8, v284, 0xEuLL));
      v285 = vrshrn_high_n_s64(vrshrn_n_s64(v609, 0xEuLL), v276, 0xEuLL);
      *v244.i8 = vrshrn_n_s64(vmull_s32(v773, v461), 0xEuLL);
      *v251.i8 = vrshrn_n_s64(vmull_s32(v773, v460), 0xEuLL);
      *v242.i8 = vsub_s32(*v251.i8, *v244.i8);
      *v243.i8 = vadd_s32(*v251.i8, *v244.i8);
      v246.i64[0] = v244.i64[0];
      v286 = vmull_s32(v778, v461);
      v594 = vrshrn_high_n_s64(*v246.i8, v286, 0xEuLL);
      *v286.i8 = vrshrn_n_s64(v286, 0xEuLL);
      v287 = vmull_s32(v778, v460);
      v610 = vrshrn_high_n_s64(*v251.i8, v287, 0xEuLL);
      *v251.i8 = vrshrn_n_s64(v287, 0xEuLL);
      v571 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v242.i8, *v463.i8), 0xEuLL), vmull_s32(vsub_s32(*v251.i8, *v286.i8), *v463.i8), 0xEuLL);
      *v242.i8 = vrshrn_n_s64(vmull_s32(v783, v461), 0xEuLL);
      v288 = vmull_s32(v788, v461);
      v289 = vrshrn_high_n_s64(*v242.i8, v288, 0xEuLL);
      *v288.i8 = vrshrn_n_s64(v288, 0xEuLL);
      v290 = vmull_s32(v788, v460);
      *v275.i8 = vrshrn_n_s64(vmull_s32(v783, v460), 0xEuLL);
      *v277.i8 = vsub_s32(*v275.i8, *v242.i8);
      *v242.i8 = vadd_s32(*v275.i8, *v242.i8);
      v584 = vrshrn_high_n_s64(*v275.i8, v290, 0xEuLL);
      v589 = v289;
      *v275.i8 = vrshrn_n_s64(v290, 0xEuLL);
      v552 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v277.i8, *v463.i8), 0xEuLL), vmull_s32(vsub_s32(*v275.i8, *v288.i8), *v463.i8), 0xEuLL);
      *v251.i8 = vadd_s32(*v251.i8, *v286.i8);
      v291 = vrshrn_high_n_s64(vrshrn_n_s64(v563, 0xEuLL), v545, 0xEuLL);
      v539 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v243.i8, *v463.i8), 0xEuLL), vmull_s32(*v251.i8, *v463.i8), 0xEuLL);
      v546 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v242.i8, *v463.i8), 0xEuLL), vmull_s32(vadd_s32(*v275.i8, *v288.i8), *v463.i8), 0xEuLL);
      v672 = vaddq_s32(v274, v656);
      v615 = vsubq_s32(v656, v274);
      v779 = vaddq_s32(v281, v651);
      v564 = vsubq_s32(v651, v281);
      v706 = vaddq_s32(v599, v634);
      v652 = vsubq_s32(v660, v619);
      v657 = vsubq_s32(v634, v599);
      v789 = vaddq_s32(v619, v660);
      v774 = vaddq_s32(v282, v629);
      v635 = vsubq_s32(v629, v282);
      v661 = vsubq_s32(v640, v604);
      v784 = vaddq_s32(v604, v640);
      *v281.i8 = vrshrn_n_s64(v477, 0xEuLL);
      *v276.i8 = vrshrn_n_s64(v283, 0xEuLL);
      v292 = vmlal_s32(vmull_s32(*v276.i8, v461), *v281.i8, v457);
      *v252.i8 = vrshrn_n_s64(v278, 0xEuLL);
      v293 = vrshrn_high_n_s64(*v252.i8, v292, 0xEuLL);
      *v282.i8 = vrshrn_n_s64(v256, 0xEuLL);
      v471 = vsub_s32(*v252.i8, *v282.i8);
      *v477.i8 = vsub_s32(vrshrn_n_s64(v292, 0xEuLL), vrshrn_n_s64(v685, 0xEuLL));
      v641 = vsubq_s32(v646, v285);
      v711 = vaddq_s32(v285, v646);
      *v288.i8 = vrshrn_n_s64(v473, 0xEuLL);
      *v284.i8 = vrshrn_n_s64(v284, 0xEuLL);
      v294 = vmlal_s32(vmull_s32(*v284.i8, v461), *v288.i8, v457);
      *v289.i8 = vrshrn_n_s64(v485, 0xEuLL);
      *v285.i8 = vrshrn_n_s64(v695, 0xEuLL);
      *v485.i8 = vsub_s32(*v289.i8, *v285.i8);
      v295 = vrshrn_high_n_s64(*v289.i8, v294, 0xEuLL);
      v296 = vrshrn_high_n_s64(*v285.i8, v700, 0xEuLL);
      *v473.i8 = vsub_s32(vrshrn_n_s64(v294, 0xEuLL), vrshrn_n_s64(v700, 0xEuLL));
      v297 = vmlal_s32(vmull_s32(*v276.i8, v460), *v281.i8, v461);
      *v285.i8 = vrshrn_n_s64(v496, 0xEuLL);
      v298 = vrshrn_high_n_s64(*v285.i8, v297, 0xEuLL);
      v299 = vmlal_s32(vmull_s32(v720, v461), v715, v457);
      *v251.i8 = vrshrn_n_s64(v624, 0xEuLL);
      v467 = vsub_s32(*v285.i8, *v251.i8);
      v300 = vrshrn_high_n_s64(*v251.i8, v299, 0xEuLL);
      *v496.i8 = vsub_s32(vrshrn_n_s64(v297, 0xEuLL), vrshrn_n_s64(v299, 0xEuLL));
      v647 = vsubq_s32(v666, v291);
      v701 = vaddq_s32(v291, v666);
      v301 = vmlal_s32(vmull_s32(*v284.i8, v460), *v288.i8, v461);
      *v243.i8 = vrshrn_n_s64(v493, 0xEuLL);
      *v278.i8 = vrshrn_n_s64(v500, 0xEuLL);
      *v666.i8 = vsub_s32(*v243.i8, *v278.i8);
      v494 = vrshrn_high_n_s64(*v243.i8, v301, 0xEuLL);
      *v246.i8 = vrshrn_n_s64(v301, 0xEuLL);
      v302 = vmlal_s32(vmull_s32(v731, v461), v725, v457);
      v303 = vrshrn_high_n_s64(*v278.i8, v302, 0xEuLL);
      *v500.i8 = vsub_s32(*v246.i8, vrshrn_n_s64(v302, 0xEuLL));
      *v302.i8 = vsub_s32(*v281.i8, v715);
      *v281.i8 = vsub_s32(*v288.i8, v725);
      *v276.i8 = vsub_s32(*v276.i8, v720);
      v726 = vaddq_s32(vrshrn_high_n_s64(*v282.i8, v685, 0xEuLL), v293);
      *v252.i8 = vsub_s32(*v284.i8, v731);
      v690 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(*v257.i8, v455), v523, v456), 0xEuLL), vmlal_s32(vmull_s32(*v276.i8, v455), *v302.i8, v456), 0xEuLL);
      v686 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(*v273.i8, v455), v506, v456), 0xEuLL), vmlal_s32(vmull_s32(*v252.i8, v455), *v281.i8, v456), 0xEuLL);
      v304 = vmlal_s32(vmull_s32(*v257.i8, v794), v523, v455);
      v305 = vmlal_s32(vmull_s32(*v276.i8, v794), *v302.i8, v455);
      v696 = vaddq_s32(v296, v295);
      v306 = vmlal_s32(vmull_s32(*v273.i8, v794), v506, v455);
      *v290.i8 = vrshrn_n_s64(v483, 0xEuLL);
      *v249.i8 = vrshrn_n_s64(v470, 0xEuLL);
      *v282.i8 = vrshrn_n_s64(v468, 0xEuLL);
      *v285.i8 = vrshrn_n_s64(v475, 0xEuLL);
      v307 = vmlal_s32(vmull_s32(*v249.i8, v797), *v290.i8, v798);
      *v257.i8 = vrshrn_n_s64(v509, 0xEuLL);
      *v273.i8 = vrshrn_n_s64(v487, 0xEuLL);
      v721 = vsub_s32(*v273.i8, *v257.i8);
      v308 = vmlal_s32(vmull_s32(*v282.i8, v798), *v285.i8, v795);
      v630 = vaddq_s32(vrshrn_high_n_s64(*v273.i8, v308, 0xEuLL), vrshrn_high_n_s64(*v257.i8, v307, 0xEuLL));
      v309 = vmlal_s32(vmull_s32(*v252.i8, v794), *v281.i8, v455);
      *v284.i8 = vrshrn_n_s64(v491, 0xEuLL);
      *v288.i8 = vrshrn_n_s64(v481, 0xEuLL);
      *v295.i8 = vrshrn_n_s64(v489, 0xEuLL);
      *v297.i8 = vrshrn_n_s64(v479, 0xEuLL);
      v310 = vmlal_s32(vmull_s32(*v288.i8, v797), *v284.i8, v798);
      *v281.i8 = vrshrn_n_s64(v502, 0xEuLL);
      *v246.i8 = vrshrn_n_s64(v498, 0xEuLL);
      v732 = vsub_s32(*v246.i8, *v281.i8);
      v311 = vmlal_s32(vmull_s32(*v297.i8, v798), *v295.i8, v795);
      v620 = vaddq_s32(vrshrn_high_n_s64(*v246.i8, v311, 0xEuLL), vrshrn_high_n_s64(*v281.i8, v310, 0xEuLL));
      v312 = vmlal_s32(vmull_s32(*v249.i8, v796), *v290.i8, v797);
      *v281.i8 = vrshrn_n_s64(v576, 0xEuLL);
      *v252.i8 = vrshrn_n_s64(v518, 0xEuLL);
      *v243.i8 = vsub_s32(*v252.i8, *v281.i8);
      v313 = vmlal_s32(vmull_s32(*v282.i8, v797), *v285.i8, v798);
      v625 = vaddq_s32(vrshrn_high_n_s64(*v252.i8, v313, 0xEuLL), vrshrn_high_n_s64(*v281.i8, v312, 0xEuLL));
      v314 = vmlal_s32(vmull_s32(*v288.i8, v796), *v284.i8, v797);
      *v281.i8 = vrshrn_n_s64(v515, 0xEuLL);
      *v257.i8 = vrshrn_n_s64(v504, 0xEuLL);
      *v252.i8 = vsub_s32(*v257.i8, *v281.i8);
      v315 = vmlal_s32(vmull_s32(*v297.i8, v797), *v295.i8, v798);
      v316 = vaddq_s32(vrshrn_high_n_s64(*v257.i8, v315, 0xEuLL), vrshrn_high_n_s64(*v281.i8, v314, 0xEuLL));
      v524 = vrshrn_high_n_s64(vrshrn_n_s64(v304, 0xEuLL), v305, 0xEuLL);
      v577 = vrshrn_high_n_s64(vrshrn_n_s64(v306, 0xEuLL), v309, 0xEuLL);
      *v281.i8 = vsub_s32(*v285.i8, *v290.i8);
      v600 = vaddq_s32(v300, v298);
      v605 = v316;
      *v284.i8 = vsub_s32(*v295.i8, *v284.i8);
      *v257.i8 = vsub_s32(*v282.i8, *v249.i8);
      *v295.i8 = vsub_s32(*v297.i8, *v288.i8);
      v317 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v528, v456), v558, v792), 0xEuLL), vmlal_s32(vmull_s32(*v257.i8, v456), *v281.i8, v792), 0xEuLL);
      v318 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v512, v456), v533, v792), 0xEuLL), vmlal_s32(vmull_s32(*v295.i8, v456), *v284.i8, v792), 0xEuLL);
      v319 = vmlal_s32(vmull_s32(v528, v455), v558, v456);
      v559 = vaddq_s32(v303, v494);
      v320 = vmlal_s32(vmull_s32(*v666.i8, v794), *v485.i8, v455);
      v321 = vrshrn_high_n_s64(vrshrn_n_s64(v319, 0xEuLL), vmlal_s32(vmull_s32(*v257.i8, v455), *v281.i8, v456), 0xEuLL);
      v322 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v512, v455), v533, v456), 0xEuLL), vmlal_s32(vmull_s32(*v295.i8, v455), *v284.i8, v456), 0xEuLL);
      *v284.i8 = vsub_s32(vrshrn_n_s64(v308, 0xEuLL), vrshrn_n_s64(v307, 0xEuLL));
      *v297.i8 = vsub_s32(vrshrn_n_s64(v311, 0xEuLL), vrshrn_n_s64(v310, 0xEuLL));
      *v313.i8 = vsub_s32(vrshrn_n_s64(v313, 0xEuLL), vrshrn_n_s64(v312, 0xEuLL));
      *v315.i8 = vsub_s32(vrshrn_n_s64(v315, 0xEuLL), vrshrn_n_s64(v314, 0xEuLL));
      v323 = vmlal_s32(vmull_s32(*v243.i8, v456), v721, v792);
      v324 = vmlal_s32(vmull_s32(*v243.i8, v455), v721, v456);
      v325 = vmlal_s32(vmull_s32(*v313.i8, v456), *v284.i8, v792);
      v326 = vmlal_s32(vmull_s32(*v313.i8, v455), *v284.i8, v456);
      v327 = vmlal_s32(vmull_s32(*v252.i8, v456), v732, v792);
      v328 = vmlal_s32(vmull_s32(*v252.i8, v455), v732, v456);
      v329 = vmlal_s32(vmull_s32(*v315.i8, v456), *v297.i8, v792);
      v330 = vmlal_s32(vmull_s32(*v315.i8, v455), *v297.i8, v456);
      v331 = vaddq_s32(v756, v594);
      v595 = vsubq_s32(v756, v594);
      v733 = vaddq_s32(v571, v756);
      v716 = vsubq_s32(v756, v571);
      v529 = v331;
      v534 = vaddq_s32(v539, v756);
      v572 = vaddq_s32(v756, v610);
      v507 = vsubq_s32(v756, v610);
      v510 = vsubq_s32(v756, v539);
      v519 = vaddq_s32(v761, v589);
      v513 = vsubq_s32(v761, v589);
      v757 = vaddq_s32(v552, v761);
      v722 = vsubq_s32(v761, v552);
      v540 = vaddq_s32(v546, v761);
      v611 = vsubq_s32(v761, v546);
      v553 = vaddq_s32(v761, v584);
      v590 = vsubq_s32(v761, v584);
      v332 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v467, v455), v471, v456), 0xEuLL), vmlal_s32(vmull_s32(*v496.i8, v455), *v477.i8, v456), 0xEuLL);
      v333 = vsubq_s32(v641, v564);
      v334 = vsubq_s32(v647, v635);
      v642 = vaddq_s32(v641, v564);
      v648 = vaddq_s32(v647, v635);
      v335 = vsubq_s32(v652, v615);
      v336 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v335.i8, *v463.i8), 0xEuLL), vmull_high_s32(v335, v463), 0xEuLL);
      v337 = vsubq_s32(v661, v657);
      v338 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v337.i8, *v463.i8), 0xEuLL), vmull_high_s32(v337, v463), 0xEuLL);
      v339 = vaddq_s32(v652, v615);
      v340 = vaddq_s32(v661, v657);
      v341 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(*v666.i8, v455), *v485.i8, v456), 0xEuLL), vmlal_s32(vmull_s32(*v500.i8, v455), *v473.i8, v456), 0xEuLL);
      v342 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v339.i8, *v463.i8), 0xEuLL), vmull_high_s32(v339, v463), 0xEuLL);
      v343 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v340.i8, *v463.i8), 0xEuLL), vmull_high_s32(v340, v463), 0xEuLL);
      v667 = vaddq_s32(v317, v690);
      v344 = vsubq_s32(v690, v317);
      v345 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v467, v794), v471, v455), 0xEuLL), vmlal_s32(vmull_s32(*v496.i8, v794), *v477.i8, v455), 0xEuLL);
      v691 = vaddq_s32(v318, v686);
      v346 = vsubq_s32(v686, v318);
      v687 = vaddq_s32(v630, v726);
      v516 = vsubq_s32(v726, v630);
      v347 = vrshrn_high_n_s64(vrshrn_n_s64(v320, 0xEuLL), vmlal_s32(vmull_s32(*v500.i8, v794), *v473.i8, v455), 0xEuLL);
      v662 = vaddq_s32(v620, v696);
      v348 = vsubq_s32(v696, v620);
      v697 = vaddq_s32(v737, v680);
      v547 = vsubq_s32(v680, v737);
      v349 = vrshrn_high_n_s64(vrshrn_n_s64(v323, 0xEuLL), v325, 0xEuLL);
      v681 = vaddq_s32(v743, v676);
      v350 = vsubq_s32(v676, v743);
      v351 = vsubq_s32(v524, v321);
      v738 = vaddq_s32(v321, v524);
      v352 = vrshrn_high_n_s64(vrshrn_n_s64(v327, 0xEuLL), v329, 0xEuLL);
      v353 = vsubq_s32(v577, v322);
      v727 = vaddq_s32(v322, v577);
      v354 = vsubq_s32(v600, v625);
      v762 = vaddq_s32(v625, v600);
      v355 = vrshrn_high_n_s64(vrshrn_n_s64(v324, 0xEuLL), v326, 0xEuLL);
      v356 = vsubq_s32(v559, v605);
      v744 = vaddq_s32(v605, v559);
      v357 = vsubq_s32(v753, v765);
      v766 = vaddq_s32(v765, v753);
      v358 = vrshrn_high_n_s64(vrshrn_n_s64(v328, 0xEuLL), v330, 0xEuLL);
      v359 = vsubq_s32(v748, v770);
      v771 = vaddq_s32(v770, v748);
      v360 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v333.i8, *v463.i8), 0xEuLL), vmull_high_s32(v333, v463), 0xEuLL);
      v749 = vaddq_s32(v507, v672);
      v626 = vsubq_s32(v507, v672);
      v361 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v334.i8, *v463.i8), 0xEuLL), vmull_high_s32(v334, v463), 0xEuLL);
      v677 = vaddq_s32(v590, v706);
      v636 = vsubq_s32(v590, v706);
      v362 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v642.i8, *v463.i8), 0xEuLL), vmull_high_s32(v642, v463), 0xEuLL);
      v754 = vaddq_s32(v510, v779);
      v707 = vsubq_s32(v510, v779);
      v363 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v648.i8, *v463.i8), 0xEuLL), vmull_high_s32(v648, v463), 0xEuLL);
      v780 = vaddq_s32(v611, v774);
      v775 = vsubq_s32(v611, v774);
      v612 = vaddq_s32(v336, v595);
      v591 = vsubq_s32(v595, v336);
      v578 = vaddq_s32(v349, v332);
      v364 = vsubq_s32(v332, v349);
      v616 = vaddq_s32(v338, v513);
      v596 = vsubq_s32(v513, v338);
      v631 = vaddq_s32(v342, v529);
      v565 = vsubq_s32(v529, v342);
      v585 = vaddq_s32(v352, v341);
      v365 = vsubq_s32(v341, v352);
      v621 = vaddq_s32(v343, v519);
      v560 = vsubq_s32(v519, v343);
      v653 = vaddq_s32(v534, v711);
      v366 = vsubq_s32(v534, v711);
      v367 = vsubq_s32(v345, v355);
      v606 = vaddq_s32(v355, v345);
      v643 = vaddq_s32(v540, v701);
      v368 = vsubq_s32(v540, v701);
      v649 = vaddq_s32(v572, v789);
      v369 = vsubq_s32(v572, v789);
      v370 = vsubq_s32(v347, v358);
      v601 = vaddq_s32(v358, v347);
      v658 = vaddq_s32(v553, v784);
      v371 = vsubq_s32(v553, v784);
      v372 = vsubq_s32(v351, v344);
      v373 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v372.i8, *v463.i8), 0xEuLL), vmull_high_s32(v372, v463), 0xEuLL);
      v374 = vsubq_s32(v353, v346);
      v375 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v374.i8, *v463.i8), 0xEuLL), vmull_high_s32(v374, v463), 0xEuLL);
      v376 = vaddq_s32(v351, v344);
      v377 = vaddq_s32(v353, v346);
      v378 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v376.i8, *v463.i8), 0xEuLL), vmull_high_s32(v376, v463), 0xEuLL);
      v379 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v377.i8, *v463.i8), 0xEuLL), vmull_high_s32(v377, v463), 0xEuLL);
      v380 = vsubq_s32(v367, v364);
      v554 = vaddq_s32(v367, v364);
      v381 = vsubq_s32(v370, v365);
      v573 = vaddq_s32(v370, v365);
      v382 = vsubq_s32(v354, v516);
      v383 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v382.i8, *v463.i8), 0xEuLL), vmull_high_s32(v382, v463), 0xEuLL);
      v384 = vsubq_s32(v356, v348);
      v385 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v384.i8, *v463.i8), 0xEuLL), vmull_high_s32(v384, v463), 0xEuLL);
      v386 = vaddq_s32(v354, v516);
      v387 = vaddq_s32(v356, v348);
      v388 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v386.i8, *v463.i8), 0xEuLL), vmull_high_s32(v386, v463), 0xEuLL);
      v389 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v387.i8, *v463.i8), 0xEuLL), vmull_high_s32(v387, v463), 0xEuLL);
      v390 = vsubq_s32(v357, v547);
      v391 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v390.i8, *v463.i8), 0xEuLL), vmull_high_s32(v390, v463), 0xEuLL);
      v392 = vsubq_s32(v359, v350);
      v393 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v392.i8, *v463.i8), 0xEuLL), vmull_high_s32(v392, v463), 0xEuLL);
      v394 = vaddq_s32(v357, v547);
      v395 = vaddq_s32(v359, v350);
      v396 = vaddq_s32(v360, v716);
      v397 = vsubq_s32(v716, v360);
      v398 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v394.i8, *v463.i8), 0xEuLL), vmull_high_s32(v394, v463), 0xEuLL);
      v548 = vaddq_s32(v361, v722);
      v399 = vsubq_s32(v722, v361);
      v400 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v395.i8, *v463.i8), 0xEuLL), vmull_high_s32(v395, v463), 0xEuLL);
      v401 = vaddq_s32(v362, v733);
      v402 = vsubq_s32(v733, v362);
      v702 = vsubq_s32(v369, v697);
      v535 = vaddq_s32(v371, v681);
      v541 = vaddq_s32(v369, v697);
      v723 = vsubq_s32(v371, v681);
      v403 = vaddq_s32(v363, v757);
      v404 = vsubq_s32(v757, v363);
      v673 = vsubq_s32(v366, v687);
      v525 = vaddq_s32(v366, v687);
      v530 = vaddq_s32(v368, v662);
      v688 = vsubq_s32(v368, v662);
      v520 = vaddq_s32(v402, v578);
      v663 = vsubq_s32(v402, v578);
      v405 = vaddq_s32(v565, v667);
      v682 = vsubq_s32(v565, v667);
      v566 = vaddq_s32(v560, v691);
      v698 = vsubq_s32(v560, v691);
      v579 = vaddq_s32(v404, v585);
      v692 = vsubq_s32(v404, v585);
      v406 = vaddq_s32(v391, v626);
      v758 = vsubq_s32(v626, v391);
      v407 = vaddq_s32(v393, v636);
      v790 = vsubq_s32(v636, v393);
      v408 = vaddq_s32(v373, v591);
      v668 = vsubq_s32(v591, v373);
      v409 = vaddq_s32(v398, v749);
      v717 = vsubq_s32(v749, v398);
      v410 = vaddq_s32(v379, v616);
      v785 = vsubq_s32(v616, v379);
      v411 = vaddq_s32(v383, v707);
      v708 = vsubq_s32(v707, v383);
      v412 = vaddq_s32(v631, v738);
      v712 = vsubq_s32(v631, v738);
      v413 = vaddq_s32(v621, v727);
      v739 = vsubq_s32(v621, v727);
      v414 = vaddq_s32(v385, v775);
      v415 = vsubq_s32(v775, v385);
      v416 = vaddq_s32(v401, v606);
      v728 = vsubq_s32(v401, v606);
      v417 = vaddq_s32(v403, v601);
      v776 = vsubq_s32(v403, v601);
      v418 = vaddq_s32(v388, v754);
      v637 = vsubq_s32(v754, v388);
      v419 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v380.i8, *v463.i8), 0xEuLL), vmull_high_s32(v380, v463), 0xEuLL);
      v420 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v381.i8, *v463.i8), 0xEuLL), vmull_high_s32(v381, v463), 0xEuLL);
      v421 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v554.i8, *v463.i8), 0xEuLL), vmull_high_s32(v554, v463), 0xEuLL);
      v422 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v573.i8, *v463.i8), 0xEuLL), vmull_high_s32(v573, v463), 0xEuLL);
      a2[v7] = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(vaddq_s32(v649, v766), 6uLL), vaddq_s32(v658, v771), 6uLL), a2[v7]), v452), 0);
      v423 = (&a2[v7] + v200);
      *v423 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(vaddq_s32(v653, v762), 6uLL), vaddq_s32(v643, v744), 6uLL), *v423), v452), 0);
      v424 = (v423 + v200);
      *v424 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v416, 6uLL), v417, 6uLL), *v424), v452), 0);
      v425 = (v424 + v200);
      *v425 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v412, 6uLL), v413, 6uLL), *v425), v452), 0);
      v426 = (v425 + v200);
      *v426 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(vaddq_s32(v378, v612), 6uLL), v410, 6uLL), *v426), v452), 0);
      v427 = (v426 + v200);
      *v427 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(vaddq_s32(v421, v396), 6uLL), vaddq_s32(v422, v548), 6uLL), *v427), v452), 0);
      v428 = (v427 + v200);
      *v428 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v418, 6uLL), vaddq_s32(v389, v780), 6uLL), *v428), v452), 0);
      v429 = (v428 + v200);
      *v429 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v409, 6uLL), vaddq_s32(v400, v677), 6uLL), *v429), v452), 0);
      v430 = (v429 + v200);
      *v430 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v406, 6uLL), v407, 6uLL), *v430), v452), 0);
      v431 = (v430 + v200);
      *v431 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v411, 6uLL), v414, 6uLL), *v431), v452), 0);
      v432 = (v431 + v200);
      *v432 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(vaddq_s32(v419, v397), 6uLL), vaddq_s32(v420, v399), 6uLL), *v432), v452), 0);
      v433 = (v432 + v200);
      *v433 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v408, 6uLL), vaddq_s32(v375, v596), 6uLL), *v433), v452), 0);
      v434 = (v433 + v200);
      *v434 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v405, 6uLL), v566, 6uLL), *v434), v452), 0);
      v435 = (v434 + v200);
      *v435 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v520, 6uLL), v579, 6uLL), *v435), v452), 0);
      v436 = (v435 + v200);
      *v436 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v525, 6uLL), v530, 6uLL), *v436), v452), 0);
      v437 = (v436 + v200);
      *v437 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v541, 6uLL), v535, 6uLL), *v437), v452), 0);
      v438 = (v437 + 32 * a3 - 30 * a3);
      *v438 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v702, 6uLL), v723, 6uLL), *v438), v452), 0);
      v439 = (v438 + v200);
      *v439 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v673, 6uLL), v688, 6uLL), *v439), v452), 0);
      v440 = (v439 + v200);
      *v440 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v663, 6uLL), v692, 6uLL), *v440), v452), 0);
      v441 = (v440 + v200);
      *v441 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v682, 6uLL), v698, 6uLL), *v441), v452), 0);
      v442 = (v441 + v200);
      *v442 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v668, 6uLL), vsubq_s32(v596, v375), 6uLL), *v442), v452), 0);
      v443 = (v442 + v200);
      *v443 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(vsubq_s32(v397, v419), 6uLL), vsubq_s32(v399, v420), 6uLL), *v443), v452), 0);
      v444 = (v443 + v200);
      *v444 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v708, 6uLL), v415, 6uLL), *v444), v452), 0);
      v445 = (v444 + v200);
      *v445 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v758, 6uLL), v790, 6uLL), *v445), v452), 0);
      v446 = (v445 + v200);
      *v446 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v717, 6uLL), vsubq_s32(v677, v400), 6uLL), *v446), v452), 0);
      v447 = (v446 + v200);
      *v447 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v637, 6uLL), vsubq_s32(v780, v389), 6uLL), *v447), v452), 0);
      v448 = (v447 + v200);
      *v448 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(vsubq_s32(v396, v421), 6uLL), vsubq_s32(v548, v422), 6uLL), *v448), v452), 0);
      v449 = (v448 + v200);
      *v449 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(vsubq_s32(v612, v378), 6uLL), v785, 6uLL), *v449), v452), 0);
      v450 = (v449 + v200);
      *v450 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v712, 6uLL), v739, 6uLL), *v450), v452), 0);
      v451 = (v450 + v200);
      *v451 = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v728, 6uLL), v776, 6uLL), *v451), v452), 0);
      result = (v451 + v200);
      *result = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(vsubq_s32(v653, v762), 6uLL), vsubq_s32(v643, v744), 6uLL), *result), v452), 0);
      *(result + 2 * a3) = vqshluq_n_s16(vminq_s16(vqaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(vsubq_s32(v649, v766), 6uLL), vsubq_s32(v658, v771), 6uLL), *(result + 2 * a3)), v452), 0);
      ++v7;
      v201 += 8;
    }

    while (v201 < 0x18);
  }

  return result;
}

int *vpx_highbd_idct32x32_1_add_neon(int *result, int16x8_t *a2, int a3, char a4)
{
  v4 = ((11585 * ((3036938240 * *result + 0x80000000) >> 32) + 0x2000) >> 14) + 32;
  v5 = vdupq_n_s16(v4 >> 6);
  if ((v4 & 0x200000) != 0)
  {
    v24 = 2 * a3;
    v25 = 8;
    do
    {
      v26 = vqshluq_n_s16(vaddq_s16(a2[1], v5), 0);
      v27 = vqshluq_n_s16(vaddq_s16(a2[2], v5), 0);
      v28 = vqshluq_n_s16(vaddq_s16(a2[3], v5), 0);
      *a2 = vqshluq_n_s16(vaddq_s16(*a2, v5), 0);
      a2[1] = v26;
      a2[2] = v27;
      a2[3] = v28;
      v29 = (a2 + v24);
      v30 = vqshluq_n_s16(vaddq_s16(*(&a2[1] + v24), v5), 0);
      v31 = vqshluq_n_s16(vaddq_s16(*(&a2[2] + v24), v5), 0);
      v32 = vqshluq_n_s16(vaddq_s16(*(&a2[3] + v24), v5), 0);
      *v29 = vqshluq_n_s16(vaddq_s16(*(a2 + 2 * a3), v5), 0);
      v29[1] = v30;
      v29[2] = v31;
      v29[3] = v32;
      v33 = (a2 + v24 + v24);
      v34 = vqshluq_n_s16(vaddq_s16(v33[1], v5), 0);
      v35 = vqshluq_n_s16(vaddq_s16(v33[2], v5), 0);
      v36 = vqshluq_n_s16(vaddq_s16(v33[3], v5), 0);
      *v33 = vqshluq_n_s16(vaddq_s16(*v33, v5), 0);
      v33[1] = v34;
      v37 = (v33 + v24);
      v33[2] = v35;
      v33[3] = v36;
      v38 = vqshluq_n_s16(vaddq_s16(*(&v33[1] + v24), v5), 0);
      v39 = vqshluq_n_s16(vaddq_s16(*(&v33[2] + v24), v5), 0);
      v40 = vqshluq_n_s16(vaddq_s16(*(&v33[3] + v24), v5), 0);
      *v37 = vqshluq_n_s16(vaddq_s16(*(v33 + 2 * a3), v5), 0);
      v37[1] = v38;
      v37[2] = v39;
      v37[3] = v40;
      a2 = (v33 + v24 + v24);
      --v25;
    }

    while (v25);
  }

  else
  {
    v6 = vdupq_n_s16(~(-1 << a4));
    v7 = 2 * a3;
    v8 = 8;
    do
    {
      v9 = vminq_s16(vaddq_s16(a2[1], v5), v6);
      v10 = vminq_s16(vaddq_s16(a2[2], v5), v6);
      v11 = vminq_s16(vaddq_s16(a2[3], v5), v6);
      *a2 = vminq_s16(vaddq_s16(*a2, v5), v6);
      a2[1] = v9;
      a2[2] = v10;
      a2[3] = v11;
      v12 = (a2 + v7);
      v13 = vminq_s16(vaddq_s16(*(&a2[1] + v7), v5), v6);
      v14 = vminq_s16(vaddq_s16(*(&a2[2] + v7), v5), v6);
      v15 = vminq_s16(vaddq_s16(*(&a2[3] + v7), v5), v6);
      *v12 = vminq_s16(vaddq_s16(*(a2 + 2 * a3), v5), v6);
      v12[1] = v13;
      v12[2] = v14;
      v12[3] = v15;
      v16 = (a2 + v7 + v7);
      v17 = vminq_s16(vaddq_s16(v16[1], v5), v6);
      v18 = vminq_s16(vaddq_s16(v16[2], v5), v6);
      v19 = vminq_s16(vaddq_s16(v16[3], v5), v6);
      *v16 = vminq_s16(vaddq_s16(*v16, v5), v6);
      v16[1] = v17;
      v20 = (v16 + v7);
      v16[2] = v18;
      v16[3] = v19;
      v21 = vminq_s16(vaddq_s16(*(&v16[1] + v7), v5), v6);
      v22 = vminq_s16(vaddq_s16(*(&v16[2] + v7), v5), v6);
      v23 = vminq_s16(vaddq_s16(*(&v16[3] + v7), v5), v6);
      *v20 = vminq_s16(vaddq_s16(*(v16 + 2 * a3), v5), v6);
      v20[1] = v21;
      v20[2] = v22;
      v20[3] = v23;
      a2 = (v16 + v7 + v7);
      --v8;
    }

    while (v8);
  }

  return result;
}

int8x16_t vpx_highbd_idct4x4_16_add_neon(int16x8_t *a1, uint64_t *a2, int a3, int a4)
{
  v4 = *a1;
  v5 = a1[1];
  v6 = a1[2];
  v7 = a1[3];
  if (a4 == 10)
  {
    v41 = vtrn1q_s32(v4, v5);
    v42 = vtrn2q_s32(v4, v5);
    v43 = vtrn1q_s32(v6, v7);
    v44 = vtrn2q_s32(v6, v7);
    v45 = vzip2q_s64(v41, v43);
    v41.i64[1] = v43.i64[0];
    v46 = vzip2q_s64(v42, v44);
    v42.i64[1] = v44.i64[0];
    v47 = vaddq_s32(v41, v45);
    v48 = vsubq_s32(v41, v45);
    v49 = vdupq_n_s32(0x2D41u);
    v50 = vmulq_s32(v47, v49);
    v51 = vmulq_s32(v48, v49);
    v52 = vdupq_n_s32(0x187Eu);
    v53 = vdupq_n_s32(0x3B21u);
    v54 = vdupq_n_s32(0xFFFFC4DF);
    v55 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v42, v52), v46, v54), 0xEuLL);
    v56 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v42, v53), v46, v52), 0xEuLL);
    v57 = vsubq_s32(vrshrq_n_s32(v51, 0xEuLL), v55);
    v58 = vrsraq_n_s32(v55, v51, 0xEuLL);
    v59 = vsubq_s32(vrshrq_n_s32(v50, 0xEuLL), v56);
    v60 = vrsraq_n_s32(v56, v50, 0xEuLL);
    v61 = vtrn1q_s32(v60, v58);
    v62 = vtrn2q_s32(v60, v58);
    v63 = vtrn1q_s32(v57, v59);
    v64 = vtrn2q_s32(v57, v59);
    v65 = vzip2q_s64(v61, v63);
    v61.i64[1] = v63.i64[0];
    v66 = vzip2q_s64(v62, v64);
    v62.i64[1] = v64.i64[0];
    v67 = vaddq_s32(v61, v65);
    v68 = vsubq_s32(v61, v65);
    v69 = vmulq_s32(v67, v49);
    v70 = vmulq_s32(v68, v49);
    v71 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v62, v52), v66, v54), 0xEuLL);
    v72 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v62, v53), v66, v52), 0xEuLL);
    v73 = vsubq_s32(vrshrq_n_s32(v70, 0xEuLL), v71);
    v74 = vrsraq_n_s32(v71, v70, 0xEuLL);
    v75 = vsubq_s32(vrshrq_n_s32(v69, 0xEuLL), v72);
    v76 = vrsraq_n_s32(v72, v69, 0xEuLL);
LABEL_6:
    v39 = vqrshrn_high_n_s32(vqrshrn_n_s32(v76, 4uLL), v74, 4uLL);
    v40 = vqrshrn_high_n_s32(vqrshrn_n_s32(v75, 4uLL), v73, 4uLL);
    goto LABEL_7;
  }

  if (a4 != 8)
  {
    v77 = vtrn1q_s32(v4, v5);
    v78 = vtrn2q_s32(v4, v5);
    v79 = vtrn1q_s32(v6, v7);
    v80 = vzip2q_s64(v77, v79);
    v77.i64[1] = v79.i64[0];
    v81 = vtrn2q_s32(v6, v7);
    v82 = vzip2q_s64(v78, v81);
    v83 = vaddq_s32(v77, v80);
    v84 = vdupq_n_s32(0x2D41u);
    v85 = vsubq_s32(v77, v80);
    v86 = vmull_s32(*v83.i8, *v84.i8);
    v87 = vmull_high_s32(v83, v84);
    v88 = vmull_s32(*v85.i8, *v84.i8);
    v89 = vmull_high_s32(v85, v84);
    v90 = vdupq_n_s32(0x187Eu);
    v91 = vdupq_n_s32(0x3B21u);
    v92 = vmlsl_s32(vmull_s32(*v78.i8, *v90.i8), *v82.i8, *v91.i8);
    v93 = vmlal_s32(vmull_s32(*v82.i8, *v90.i8), *v78.i8, *v91.i8);
    v94 = vrshrn_high_n_s64(vrshrn_n_s64(v86, 0xEuLL), v87, 0xEuLL);
    v95 = vrshrn_high_n_s64(vrshrn_n_s64(v88, 0xEuLL), v89, 0xEuLL);
    v96 = vrshrn_high_n_s64(vrshrn_n_s64(v92, 0xEuLL), vmlsl_high_s32(vmull_s32(*v81.i8, *v90.i8), v82, v91), 0xEuLL);
    v97 = vrshrn_high_n_s64(vrshrn_n_s64(v93, 0xEuLL), vmlal_high_s32(vmull_s32(*v81.i8, *v91.i8), v82, v90), 0xEuLL);
    v98 = vaddq_s32(v97, v94);
    v99 = vaddq_s32(v96, v95);
    v100 = vsubq_s32(v95, v96);
    v101 = vsubq_s32(v94, v97);
    v102 = vtrn1q_s32(v98, v99);
    v103 = vtrn2q_s32(v98, v99);
    v104 = vtrn1q_s32(v100, v101);
    v105 = vtrn2q_s32(v100, v101);
    v106 = vzip2q_s64(v102, v104);
    v102.i64[1] = v104.i64[0];
    v107 = vzip2q_s64(v103, v105);
    v108 = vaddq_s32(v102, v106);
    v109 = vsubq_s32(v102, v106);
    v110 = vmull_s32(*v108.i8, *v84.i8);
    v111 = vmull_high_s32(v108, v84);
    v112 = vmull_s32(*v109.i8, *v84.i8);
    v113 = vmull_high_s32(v109, v84);
    v114 = vmlsl_s32(vmull_s32(*v103.i8, *v90.i8), *v107.i8, *v91.i8);
    v115 = vmlsl_high_s32(vmull_s32(*v105.i8, *v90.i8), v107, v91);
    v116 = vmlal_s32(vmull_s32(*v107.i8, *v90.i8), *v103.i8, *v91.i8);
    v117 = vmlal_high_s32(vmull_s32(*v105.i8, *v91.i8), v107, v90);
    v118 = vrshrn_high_n_s64(vrshrn_n_s64(v110, 0xEuLL), v111, 0xEuLL);
    v119 = vrshrn_high_n_s64(vrshrn_n_s64(v112, 0xEuLL), v113, 0xEuLL);
    v120 = vrshrn_high_n_s64(vrshrn_n_s64(v114, 0xEuLL), v115, 0xEuLL);
    v121 = vrshrn_high_n_s64(vrshrn_n_s64(v116, 0xEuLL), v117, 0xEuLL);
    v76 = vaddq_s32(v121, v118);
    v74 = vaddq_s32(v120, v119);
    v73 = vsubq_s32(v119, v120);
    v75 = vsubq_s32(v118, v121);
    goto LABEL_6;
  }

  v8 = vuzp1q_s16(v4, v5);
  v9 = vuzp1q_s16(v6, v7);
  v10 = vtrn1q_s32(v8, v9);
  v11 = vtrn2q_s32(v8, v9);
  v8.i64[0] = v10.i64[0];
  v8.i64[1] = v11.i64[0];
  v12 = vzip2q_s64(v10, v11);
  v13 = vtrn1q_s16(v8, v12);
  v14 = vtrn2q_s16(v8, v12);
  v15 = vdupq_n_s16(0x2D41u);
  v16 = vmull_s16(*v13.i8, *v15.i8);
  v17 = vmlsl_high_s16(v16, v13, v15);
  v18 = vdupq_n_s16(0x187Eu);
  v19 = vmlal_high_s16(v16, v13, v15);
  v20 = vdupq_n_s16(0x3B21u);
  v21 = vmlsl_high_s16(vmull_s16(*v14.i8, *v18.i8), v14, v20);
  v22 = vmlal_high_s16(vmull_s16(*v14.i8, *v20.i8), v14, v18);
  v23 = vaddq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v22, 0xEuLL), v21, 0xEuLL), vrshrn_high_n_s32(vrshrn_n_s32(v19, 0xEuLL), v17, 0xEuLL));
  v24 = vsubq_s16(vrshrn_high_n_s32(vrshrn_n_s32(v17, 0xEuLL), v19, 0xEuLL), vrshrn_high_n_s32(vrshrn_n_s32(v21, 0xEuLL), v22, 0xEuLL));
  v25 = vtrn1q_s32(v23, v24);
  v26 = vtrn2q_s32(v23, v24);
  v27 = vzip2q_s64(v25, v26);
  v25.i64[1] = v26.i64[0];
  v28 = vtrn1q_s16(v25, v27);
  v29 = vtrn2q_s16(v25, v27);
  v30 = vmull_s16(*v28.i8, *v15.i8);
  v31 = vmlsl_high_s16(v30, v28, v15);
  v32 = vmlal_high_s16(v30, v28, v15);
  v33 = vmlsl_high_s16(vmull_s16(*v29.i8, *v18.i8), v29, v20);
  v34 = vmlal_high_s16(vmull_s16(*v29.i8, *v20.i8), v29, v18);
  v35 = vrshrn_high_n_s32(vrshrn_n_s32(v32, 0xEuLL), v31, 0xEuLL);
  v36 = vrshrn_high_n_s32(vrshrn_n_s32(v34, 0xEuLL), v33, 0xEuLL);
  v37 = vaddq_s16(v36, v35);
  v38 = vsubq_s16(v35, v36);
  v39 = vrshrq_n_s16(v37, 4uLL);
  v40 = vrshrq_n_s16(v38, 4uLL);
LABEL_7:
  v122 = vdupq_n_s16(~(-1 << a4));
  v123.i64[0] = *a2;
  v124 = 2 * a3;
  v123.i64[1] = *(a2 + v124);
  v125 = vqshluq_n_s16(vminq_s16(vqaddq_s16(v39, v123), v122), 0);
  *a2 = v125.i64[0];
  *(a2 + v124) = vextq_s8(v125, v125, 8uLL).u64[0];
  v126 = (a2 + v124 + v124);
  v123.i64[0] = *(v126 + v124);
  v123.i64[1] = *v126;
  result = vqshluq_n_s16(vminq_s16(vqaddq_s16(v40, v123), v122), 0);
  *v126 = vextq_s8(result, result, 8uLL).u64[0];
  *(v126 + v124) = result.i64[0];
  return result;
}

int *vpx_highbd_idct8x8_1_add_neon(int *result, int16x8_t *a2, int a3, char a4)
{
  v4 = ((11585 * ((3036938240 * *result + 0x80000000) >> 32) + 0x2000) >> 14) + 16;
  v5 = vdupq_n_s16(v4 >> 5);
  if ((v4 & 0x100000) != 0)
  {
    *a2 = vqshluq_n_s16(vaddq_s16(*a2, v5), 0);
    v7 = 2 * a3;
    *(a2 + v7) = vqshluq_n_s16(vaddq_s16(*(a2 + v7), v5), 0);
    v14 = (a2 + v7 + v7);
    *v14 = vqshluq_n_s16(vaddq_s16(*v14, v5), 0);
    v15 = (v14 + v7);
    *v15 = vqshluq_n_s16(vaddq_s16(*v15, v5), 0);
    v16 = (v15 + v7);
    *v16 = vqshluq_n_s16(vaddq_s16(*v16, v5), 0);
    v17 = (v16 + v7);
    *v17 = vqshluq_n_s16(vaddq_s16(*v17, v5), 0);
    v12 = (v17 + v7);
    *v12 = vqshluq_n_s16(vaddq_s16(*v12, v5), 0);
    v13 = vqshluq_n_s16(vaddq_s16(*(v12 + v7), v5), 0);
  }

  else
  {
    v6 = vdupq_n_s16(~(-1 << a4));
    *a2 = vminq_s16(vaddq_s16(*a2, v5), v6);
    v7 = 2 * a3;
    *(a2 + v7) = vminq_s16(vaddq_s16(*(a2 + v7), v5), v6);
    v8 = (a2 + v7 + v7);
    *v8 = vminq_s16(vaddq_s16(*v8, v5), v6);
    v9 = (v8 + v7);
    *v9 = vminq_s16(vaddq_s16(*v9, v5), v6);
    v10 = (v9 + v7);
    *v10 = vminq_s16(vaddq_s16(*v10, v5), v6);
    v11 = (v10 + v7);
    *v11 = vminq_s16(vaddq_s16(*v11, v5), v6);
    v12 = (v11 + v7);
    *v12 = vminq_s16(vaddq_s16(*v12, v5), v6);
    v13 = vminq_s16(vaddq_s16(*(v12 + v7), v5), v6);
  }

  *(v12 + v7) = v13;
  return result;
}

int16x8_t vpx_highbd_idct8x8_12_add_neon(int32x4_t *a1, int16x8_t *a2, int a3, int a4)
{
  v4 = *a1;
  v5 = a1[2];
  v6 = a1[4];
  v7 = a1[6];
  if (a4 == 10)
  {
    v86 = vtrn1q_s32(v4, v5);
    v87 = vtrn2q_s32(v4, v5);
    v88 = vtrn1q_s32(v6, v7);
    v89 = vtrn2q_s32(v6, v7);
    v90 = vzip2q_s64(v86, v88);
    v86.i64[1] = v88.i64[0];
    v91 = vzip2q_s64(v87, v89);
    v87.i64[1] = v89.i64[0];
    v92 = vdupq_n_s32(0xC7Cu);
    v93 = vdupq_n_s32(0xFFFFDC72);
    v94 = vmulq_s32(v87, v92);
    v95 = vdupq_n_s32(0x3537u);
    v96 = vmulq_s32(v91, v95);
    v97 = vdupq_n_s32(0x3EC5u);
    v98 = vrshrq_n_s32(vmulq_s32(v91, v93), 0xEuLL);
    v99 = vrshrq_n_s32(vmulq_s32(v87, v97), 0xEuLL);
    v100 = vdupq_n_s32(0x2D41u);
    v101 = vmulq_s32(v86, v100);
    v102 = vdupq_n_s32(0x187Eu);
    v103 = vdupq_n_s32(0x3B21u);
    v104 = vrshrq_n_s32(v101, 0xEuLL);
    v105 = vrshrq_n_s32(vmulq_s32(v90, v102), 0xEuLL);
    v106 = vrshrq_n_s32(vmulq_s32(v90, v103), 0xEuLL);
    v107 = vsubq_s32(vrshrq_n_s32(v94, 0xEuLL), v98);
    v108 = vrsraq_n_s32(v98, v94, 0xEuLL);
    v109 = vsubq_s32(v99, vrshrq_n_s32(v96, 0xEuLL));
    v110 = vrsraq_n_s32(v99, v96, 0xEuLL);
    v111 = vsubq_s32(v104, v106);
    v112 = vrsraq_n_s32(v106, v101, 0xEuLL);
    v113 = vsubq_s32(v104, v105);
    v114 = vrsraq_n_s32(v105, v101, 0xEuLL);
    v115 = vmulq_s32(v109, v100);
    v116 = vmulq_s32(v107, v100);
    v117 = vsubq_s32(v115, v116);
    v118 = vaddq_s32(v115, v116);
    v119 = vaddq_s32(v112, v110);
    v120 = vaddq_s32(v111, v108);
    v121 = vsubq_s32(v111, v108);
    v122 = vsubq_s32(v113, vrshrq_n_s32(v117, 0xEuLL));
    v123 = vrsraq_n_s32(v113, v117, 0xEuLL);
    v124 = vsubq_s32(v114, vrshrq_n_s32(v118, 0xEuLL));
    v125 = vrsraq_n_s32(v114, v118, 0xEuLL);
    v126 = vsubq_s32(v112, v110);
    v127 = vtrn1q_s32(v119, v125);
    v128 = vtrn2q_s32(v119, v125);
    v129 = vtrn1q_s32(v123, v120);
    v130 = vtrn2q_s32(v123, v120);
    v131 = vzip2q_s64(v127, v129);
    v127.i64[1] = v129.i64[0];
    v132 = vzip2q_s64(v128, v130);
    v128.i64[1] = v130.i64[0];
    v133 = vmulq_s32(v128, v92);
    v134 = vmulq_s32(v132, v93);
    v135 = vmulq_s32(v132, v95);
    v136 = vrshrq_n_s32(v134, 0xEuLL);
    v137 = vrshrq_n_s32(vmulq_s32(v128, v97), 0xEuLL);
    v138 = vmulq_s32(v127, v100);
    v139 = vmulq_s32(v131, v102);
    v140 = vmulq_s32(v131, v103);
    v141 = vrshrq_n_s32(v138, 0xEuLL);
    v142 = vrshrq_n_s32(v139, 0xEuLL);
    v143 = vrshrq_n_s32(v140, 0xEuLL);
    v144 = vsubq_s32(vrshrq_n_s32(v133, 0xEuLL), v136);
    v145 = vrsraq_n_s32(v136, v133, 0xEuLL);
    v146 = vsubq_s32(v137, vrshrq_n_s32(v135, 0xEuLL));
    v147 = vrsraq_n_s32(v137, v135, 0xEuLL);
    v148 = vsubq_s32(v141, v143);
    v149 = vrsraq_n_s32(v143, v138, 0xEuLL);
    v150 = vsubq_s32(v141, v142);
    v151 = vrsraq_n_s32(v142, v138, 0xEuLL);
    v152 = vmulq_s32(v146, v100);
    v153 = vmulq_s32(v144, v100);
    v154 = vsubq_s32(v152, v153);
    v155 = vaddq_s32(v152, v153);
    v156 = vaddq_s32(v149, v147);
    v157 = vaddq_s32(v148, v145);
    v158 = vsubq_s32(v148, v145);
    v159 = vsubq_s32(v150, vrshrq_n_s32(v154, 0xEuLL));
    v160 = vrsraq_n_s32(v150, v154, 0xEuLL);
    v161 = vsubq_s32(v151, vrshrq_n_s32(v155, 0xEuLL));
    v162 = vrsraq_n_s32(v151, v155, 0xEuLL);
    v163 = vsubq_s32(v149, v147);
    v164 = vtrn1q_s32(v121, v122);
    v165 = vtrn2q_s32(v121, v122);
    v166 = vtrn1q_s32(v124, v126);
    v167 = vzip2q_s64(v164, v166);
    v164.i64[1] = v166.i64[0];
    v168 = vtrn2q_s32(v124, v126);
    v169 = vzip2q_s64(v165, v168);
    v165.i64[1] = v168.i64[0];
    v170 = vmulq_s32(v165, v92);
    v171 = vmulq_s32(v169, v93);
    v172 = vmulq_s32(v169, v95);
    v173 = vmulq_s32(v165, v97);
    v174 = vrshrq_n_s32(v171, 0xEuLL);
    v175 = vrshrq_n_s32(v173, 0xEuLL);
    v176 = vmulq_s32(v164, v100);
    v177 = vmulq_s32(v167, v102);
    v178 = vrshrq_n_s32(v176, 0xEuLL);
    v179 = vrshrq_n_s32(v177, 0xEuLL);
    v180 = vrshrq_n_s32(vmulq_s32(v167, v103), 0xEuLL);
    v181 = vsubq_s32(vrshrq_n_s32(v170, 0xEuLL), v174);
    v182 = vrsraq_n_s32(v174, v170, 0xEuLL);
    v183 = vsubq_s32(v175, vrshrq_n_s32(v172, 0xEuLL));
    v184 = vrsraq_n_s32(v175, v172, 0xEuLL);
    v185 = vsubq_s32(v178, v180);
    v186 = vrsraq_n_s32(v180, v176, 0xEuLL);
    v187 = vsubq_s32(v178, v179);
    v188 = vrsraq_n_s32(v179, v176, 0xEuLL);
    v189 = vmulq_s32(v183, v100);
    v190 = vmulq_s32(v181, v100);
    v191 = vsubq_s32(v189, v190);
    v192 = vaddq_s32(v189, v190);
    v193 = vaddq_s32(v186, v184);
    v194 = vaddq_s32(v185, v182);
    v195 = vsubq_s32(v185, v182);
    v196 = vsubq_s32(v187, vrshrq_n_s32(v191, 0xEuLL));
    v197 = vrsraq_n_s32(v187, v191, 0xEuLL);
    v198 = vsubq_s32(v188, vrshrq_n_s32(v192, 0xEuLL));
    v199 = vrsraq_n_s32(v188, v192, 0xEuLL);
    v200 = vsubq_s32(v186, v184);
LABEL_6:
    v77 = vrshrn_high_n_s32(vrshrn_n_s32(v156, 5uLL), v193, 5uLL);
    v78 = vrshrn_high_n_s32(vrshrn_n_s32(v162, 5uLL), v199, 5uLL);
    v79 = vrshrn_high_n_s32(vrshrn_n_s32(v160, 5uLL), v197, 5uLL);
    v80 = vrshrn_high_n_s32(vrshrn_n_s32(v157, 5uLL), v194, 5uLL);
    v81 = vrshrn_high_n_s32(vrshrn_n_s32(v158, 5uLL), v195, 5uLL);
    v82 = vrshrn_high_n_s32(vrshrn_n_s32(v159, 5uLL), v196, 5uLL);
    v83 = vrshrn_high_n_s32(vrshrn_n_s32(v161, 5uLL), v198, 5uLL);
    v84 = vrshrn_high_n_s32(vrshrn_n_s32(v163, 5uLL), v200, 5uLL);
    v85 = vdupq_n_s16(~(-1 << a4));
    goto LABEL_7;
  }

  if (a4 != 8)
  {
    v201 = vtrn1q_s32(v4, v5);
    v202 = vtrn2q_s32(v4, v5);
    v203 = vtrn1q_s32(v6, v7);
    v204 = vtrn2q_s32(v6, v7);
    v205 = vzip2q_s64(v201, v203);
    v206 = vzip2q_s64(v202, v204);
    v207 = vdup_n_s32(0xC7Cu);
    v208 = vmull_s32(*v204.i8, v207);
    v209 = vdupq_n_s32(0xFFFFDC72);
    v210 = vmull_high_s32(v206, v209);
    v211 = vdupq_n_s32(0x3537u);
    v212 = vmull_high_s32(v206, v211);
    v213 = vdup_n_s32(0x3EC5u);
    v214 = vmull_s32(*v202.i8, v213);
    v215 = vmull_s32(*v204.i8, v213);
    v216 = vrshrn_n_s64(vmull_s32(*v202.i8, v207), 0xEuLL);
    *v202.i8 = vrshrn_n_s64(v208, 0xEuLL);
    v217 = vrshrn_n_s64(vmull_s32(*v206.i8, *v209.i8), 0xEuLL);
    *v204.i8 = vrshrn_n_s64(v210, 0xEuLL);
    v218 = vrshrn_n_s64(vmull_s32(*v206.i8, *v211.i8), 0xEuLL);
    *v206.i8 = vrshrn_n_s64(v212, 0xEuLL);
    *v214.i8 = vrshrn_n_s64(v214, 0xEuLL);
    v219 = vrshrn_n_s64(v215, 0xEuLL);
    v220 = vsub_s32(v216, v217);
    v221 = vrshrn_high_n_s64(v216, v208, 0xEuLL);
    v222 = vrshrn_high_n_s64(v217, v210, 0xEuLL);
    *v210.i8 = vsub_s32(*v214.i8, v218);
    v223 = vrshrn_high_n_s64(v218, v212, 0xEuLL);
    v224 = vrshrn_high_n_s64(*v214.i8, v215, 0xEuLL);
    *v208.i8 = vdup_n_s32(0x2D41u);
    v225 = vdupq_n_s32(0x187Eu);
    v226 = vdupq_n_s32(0x3B21u);
    v227 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v201.i8, *v208.i8), 0xEuLL), vmull_s32(*v203.i8, *v208.i8), 0xEuLL);
    v228 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v205.i8, *v225.i8), 0xEuLL), vmull_high_s32(v205, v225), 0xEuLL);
    v229 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v205.i8, *v226.i8), 0xEuLL), vmull_high_s32(v205, v226), 0xEuLL);
    v230 = vaddq_s32(v222, v221);
    v231 = vaddq_s32(v224, v223);
    v232 = vaddq_s32(v229, v227);
    v233 = vaddq_s32(v228, v227);
    v234 = vsubq_s32(v227, v228);
    v235 = vsubq_s32(v227, v229);
    *v206.i8 = vsub_s32(v219, *v206.i8);
    *v202.i8 = vsub_s32(*v202.i8, *v204.i8);
    v236 = vmlal_s32(vmull_s32(*v202.i8, *v208.i8), *v206.i8, *v208.i8);
    v237 = vrshrn_high_n_s64(vrshrn_n_s64(vmlsl_s32(vmull_s32(*v210.i8, *v208.i8), v220, *v208.i8), 0xEuLL), vmlsl_s32(vmull_s32(*v206.i8, *v208.i8), *v202.i8, *v208.i8), 0xEuLL);
    v238 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v220, *v208.i8), *v210.i8, *v208.i8), 0xEuLL), v236, 0xEuLL);
    v239 = vaddq_s32(v232, v231);
    v240 = vaddq_s32(v238, v233);
    v241 = vaddq_s32(v237, v234);
    v242 = vaddq_s32(v235, v230);
    v243 = vsubq_s32(v235, v230);
    v244 = vsubq_s32(v234, v237);
    v245 = vsubq_s32(v233, v238);
    v246 = vsubq_s32(v232, v231);
    v247 = vtrn1q_s32(v239, v240);
    v248 = vtrn2q_s32(v239, v240);
    v249 = vtrn1q_s32(v241, v242);
    v250 = vtrn2q_s32(v241, v242);
    v251 = vzip2q_s64(v247, v249);
    v252 = vzip2q_s64(v248, v250);
    v253 = vmull_s32(*v250.i8, v207);
    v254 = vmull_high_s32(v252, v209);
    v255 = vmull_high_s32(v252, v211);
    v256 = vmull_s32(*v248.i8, v213);
    v257 = vmull_s32(*v250.i8, v213);
    v258 = vrshrn_n_s64(vmull_s32(*v248.i8, v207), 0xEuLL);
    *v248.i8 = vrshrn_n_s64(v253, 0xEuLL);
    *v242.i8 = vrshrn_n_s64(vmull_s32(*v252.i8, *v209.i8), 0xEuLL);
    *v250.i8 = vrshrn_n_s64(v254, 0xEuLL);
    v259 = vrshrn_n_s64(vmull_s32(*v252.i8, *v211.i8), 0xEuLL);
    *v252.i8 = vrshrn_n_s64(v255, 0xEuLL);
    *v256.i8 = vrshrn_n_s64(v256, 0xEuLL);
    *v240.i8 = vrshrn_n_s64(v257, 0xEuLL);
    v260 = vsub_s32(v258, *v242.i8);
    v261 = vrshrn_high_n_s64(v258, v253, 0xEuLL);
    v262 = vrshrn_high_n_s64(*v242.i8, v254, 0xEuLL);
    *v253.i8 = vsub_s32(*v256.i8, v259);
    v263 = vrshrn_high_n_s64(*v256.i8, v257, 0xEuLL);
    v264 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v247.i8, *v208.i8), 0xEuLL), vmull_s32(*v249.i8, *v208.i8), 0xEuLL);
    v265 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v251.i8, *v225.i8), 0xEuLL), vmull_high_s32(v251, v225), 0xEuLL);
    v266 = vrshrn_high_n_s64(vrshrn_n_s64(vmull_s32(*v251.i8, *v226.i8), 0xEuLL), vmull_high_s32(v251, v226), 0xEuLL);
    v267 = vaddq_s32(v262, v261);
    v268 = vaddq_s32(v263, vrshrn_high_n_s64(v259, v255, 0xEuLL));
    v269 = vaddq_s32(v266, v264);
    v270 = vaddq_s32(v265, v264);
    v271 = vsubq_s32(v264, v265);
    v272 = vsubq_s32(v264, v266);
    *v252.i8 = vsub_s32(*v240.i8, *v252.i8);
    *v248.i8 = vsub_s32(*v248.i8, *v250.i8);
    v273 = vrshrn_high_n_s64(vrshrn_n_s64(vmlsl_s32(vmull_s32(*v253.i8, *v208.i8), v260, *v208.i8), 0xEuLL), vmlsl_s32(vmull_s32(*v252.i8, *v208.i8), *v248.i8, *v208.i8), 0xEuLL);
    v274 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(v260, *v208.i8), *v253.i8, *v208.i8), 0xEuLL), vmlal_s32(vmull_s32(*v248.i8, *v208.i8), *v252.i8, *v208.i8), 0xEuLL);
    v156 = vaddq_s32(v269, v268);
    v162 = vaddq_s32(v274, v270);
    v160 = vaddq_s32(v273, v271);
    v157 = vaddq_s32(v272, v267);
    v158 = vsubq_s32(v272, v267);
    v159 = vsubq_s32(v271, v273);
    v161 = vsubq_s32(v270, v274);
    v163 = vsubq_s32(v269, v268);
    v275 = vtrn1q_s32(v243, v244);
    v276 = vtrn2q_s32(v243, v244);
    v277 = vtrn1q_s32(v245, v246);
    v278 = vtrn2q_s32(v245, v246);
    v279 = vzip2q_s64(v275, v277);
    v280 = vzip2q_s64(v276, v278);
    v281 = vmull_s32(*v278.i8, v207);
    v282 = vmull_s32(*v280.i8, *v209.i8);
    v283 = vmull_high_s32(v280, v209);
    v284 = vmull_s32(*v280.i8, *v211.i8);
    v285 = vmull_high_s32(v280, v211);
    v286 = vmull_s32(*v278.i8, v213);
    *v269.i8 = vrshrn_n_s64(vmull_s32(*v276.i8, v207), 0xEuLL);
    v287 = vrshrn_n_s64(v281, 0xEuLL);
    *v282.i8 = vrshrn_n_s64(v282, 0xEuLL);
    *v284.i8 = vrshrn_n_s64(v284, 0xEuLL);
    *v211.i8 = vrshrn_n_s64(v285, 0xEuLL);
    *v276.i8 = vrshrn_n_s64(vmull_s32(*v276.i8, v213), 0xEuLL);
    v288 = vrshrn_n_s64(v286, 0xEuLL);
    v289 = vsub_s32(*v269.i8, *v282.i8);
    v290 = vrshrn_high_n_s64(*v269.i8, v281, 0xEuLL);
    *v281.i8 = vsub_s32(*v276.i8, *v284.i8);
    v291 = vrshrn_high_n_s64(*v284.i8, v285, 0xEuLL);
    v292 = vrshrn_high_n_s64(*v276.i8, v286, 0xEuLL);
    v293 = vmull_s32(*v275.i8, *v208.i8);
    v294 = vmull_s32(*v279.i8, *v225.i8);
    v295 = vmull_high_s32(v279, v225);
    v296 = vmull_s32(*v279.i8, *v226.i8);
    v297 = vmull_high_s32(v279, v226);
    v298 = vrshrn_high_n_s64(vrshrn_n_s64(v293, 0xEuLL), vmull_s32(*v277.i8, *v208.i8), 0xEuLL);
    v299 = vrshrn_high_n_s64(vrshrn_n_s64(v294, 0xEuLL), v295, 0xEuLL);
    v300 = vrshrn_high_n_s64(vrshrn_n_s64(v296, 0xEuLL), v297, 0xEuLL);
    v301 = vaddq_s32(vrshrn_high_n_s64(*v282.i8, v283, 0xEuLL), v290);
    v302 = vaddq_s32(v292, v291);
    v303 = vaddq_s32(v300, v298);
    v304 = vaddq_s32(v299, v298);
    v305 = vsubq_s32(v298, v299);
    v306 = vsubq_s32(v298, v300);
    *v211.i8 = vsub_s32(v288, *v211.i8);
    v307 = vsub_s32(v287, vrshrn_n_s64(v283, 0xEuLL));
    v308 = vmlal_s32(vmull_s32(v289, *v208.i8), *v281.i8, *v208.i8);
    v309 = vmlal_s32(vmull_s32(v307, *v208.i8), *v211.i8, *v208.i8);
    v310 = vrshrn_high_n_s64(vrshrn_n_s64(vmlsl_s32(vmull_s32(*v281.i8, *v208.i8), v289, *v208.i8), 0xEuLL), vmlsl_s32(vmull_s32(*v211.i8, *v208.i8), v307, *v208.i8), 0xEuLL);
    v311 = vrshrn_high_n_s64(vrshrn_n_s64(v308, 0xEuLL), v309, 0xEuLL);
    v193 = vaddq_s32(v303, v302);
    v199 = vaddq_s32(v311, v304);
    v197 = vaddq_s32(v310, v305);
    v194 = vaddq_s32(v306, v301);
    v195 = vsubq_s32(v306, v301);
    v196 = vsubq_s32(v305, v310);
    v198 = vsubq_s32(v304, v311);
    v200 = vsubq_s32(v303, v302);
    goto LABEL_6;
  }

  v8 = vmovn_s32(v4);
  v9 = vmovn_s32(v5);
  v10 = vmovn_s32(v6);
  v11 = vmovn_s32(v7);
  v12 = vtrn1_s16(v8, v9);
  v13 = vtrn2_s16(v8, v9);
  v14 = vtrn1_s16(v10, v11);
  v15 = vtrn2_s16(v10, v11);
  v16 = vzip1_s32(v12, v14);
  v17 = vzip2_s32(v12, v14);
  v18 = vzip1_s32(v13, v15);
  v19 = vzip2_s32(v13, v15);
  v20 = vqrdmulh_lane_s16(v18, 0x18F8B8E46A6E7D8ALL, 3);
  v21 = vqrdmulh_lane_s16(v19, 0x18F8B8E46A6E7D8ALL, 2);
  v22 = vqrdmulh_lane_s16(v19, 0x18F8B8E46A6E7D8ALL, 1);
  v23 = vqrdmulh_lane_s16(v18, 0x18F8B8E46A6E7D8ALL, 0);
  v24 = vqrdmulh_lane_s16(v16, 0x30FC5A8276428000, 2);
  v25 = vqrdmulh_lane_s16(v17, 0x30FC5A8276428000, 3);
  v26 = vqrdmulh_lane_s16(v17, 0x30FC5A8276428000, 1);
  v27 = vadd_s16(v21, v20);
  v28 = vsub_s16(v20, v21);
  v29 = vsub_s16(v23, v22);
  v30 = vadd_s16(v23, v22);
  v31 = vadd_s16(v26, v24);
  v32 = vadd_s16(v25, v24);
  v33 = vsub_s16(v24, v25);
  v34 = vsub_s16(v24, v26);
  v35 = vdupq_n_s16(0x2D41u);
  v36 = vrshrn_n_s32(vmlsl_s16(vmull_s16(v29, *v35.i8), v28, *v35.i8), 0xEuLL);
  v37 = vrshrn_n_s32(vmlal_s16(vmull_s16(v28, *v35.i8), v29, *v35.i8), 0xEuLL);
  *v38.i8 = vadd_s16(v31, v30);
  *v39.i8 = vadd_s16(v37, v32);
  *v40.i8 = vadd_s16(v36, v33);
  *v41.i8 = vadd_s16(v34, v27);
  v38.u64[1] = vsub_s16(v34, v27);
  v39.u64[1] = vsub_s16(v33, v36);
  v40.u64[1] = vsub_s16(v32, v37);
  v41.u64[1] = vsub_s16(v31, v30);
  v42 = vtrn1q_s16(v38, v39);
  v43 = vtrn2q_s16(v38, v39);
  v44 = vtrn1q_s16(v40, v41);
  v45 = vtrn2q_s16(v40, v41);
  v46 = vtrn1q_s32(v42, v44);
  v47 = vtrn2q_s32(v42, v44);
  v48 = vtrn1q_s32(v43, v45);
  v49 = vtrn2q_s32(v43, v45);
  v50 = vqrdmulhq_lane_s16(v48, 0x18F8B8E46A6E7D8ALL, 3);
  v51 = vqrdmulhq_lane_s16(v49, 0x18F8B8E46A6E7D8ALL, 2);
  v52 = vqrdmulhq_lane_s16(v49, 0x18F8B8E46A6E7D8ALL, 1);
  v53 = vqrdmulhq_lane_s16(v48, 0x18F8B8E46A6E7D8ALL, 0);
  v54 = vqrdmulhq_lane_s16(v46, 0x30FC5A8276428000, 2);
  v55 = vqrdmulhq_lane_s16(v47, 0x30FC5A8276428000, 3);
  v56 = vqrdmulhq_lane_s16(v47, 0x30FC5A8276428000, 1);
  v57 = vaddq_s16(v51, v50);
  v58 = vsubq_s16(v50, v51);
  v59 = vsubq_s16(v53, v52);
  v60 = vaddq_s16(v53, v52);
  v61 = vaddq_s16(v56, v54);
  v62 = vaddq_s16(v55, v54);
  v63 = vsubq_s16(v54, v55);
  v64 = vsubq_s16(v54, v56);
  v65 = vmlal_s16(vmull_s16(*v58.i8, *v35.i8), *v59.i8, *v35.i8);
  v66 = vmlal_high_s16(vmull_high_s16(v58, v35), v59, v35);
  v67 = vrshrn_high_n_s32(vrshrn_n_s32(vmlsl_s16(vmull_s16(*v59.i8, *v35.i8), *v58.i8, *v35.i8), 0xEuLL), vmlsl_high_s16(vmull_high_s16(v59, v35), v58, v35), 0xEuLL);
  v68 = vrshrn_high_n_s32(vrshrn_n_s32(v65, 0xEuLL), v66, 0xEuLL);
  v69 = vaddq_s16(v61, v60);
  v70 = vaddq_s16(v68, v62);
  v71 = vaddq_s16(v67, v63);
  v72 = vaddq_s16(v64, v57);
  v73 = vsubq_s16(v64, v57);
  v74 = vsubq_s16(v63, v67);
  v75 = vsubq_s16(v62, v68);
  v76 = vsubq_s16(v61, v60);
  v77 = vrshrq_n_s16(v69, 5uLL);
  v78 = vrshrq_n_s16(v70, 5uLL);
  v79 = vrshrq_n_s16(v71, 5uLL);
  v80 = vrshrq_n_s16(v72, 5uLL);
  v81 = vrshrq_n_s16(v73, 5uLL);
  v82 = vrshrq_n_s16(v74, 5uLL);
  v83 = vrshrq_n_s16(v75, 5uLL);
  v84 = vrshrq_n_s16(v76, 5uLL);
  v85.i64[0] = 0xFF00FF00FF00FFLL;
  v85.i64[1] = 0xFF00FF00FF00FFLL;
LABEL_7:
  v312 = 2 * a3;
  v313 = (a2 + v312 + v312);
  v314 = (v313 + v312 + v312);
  v315 = (v314 + v312 + v312);
  v316 = vqaddq_s16(v81, *v314);
  v317 = vqaddq_s16(v84, *(v315 + v312));
  v318 = vminq_s16(vqaddq_s16(v79, *v313), v85);
  v319 = vminq_s16(vqaddq_s16(v80, *(v313 + v312)), v85);
  v320 = vminq_s16(vqaddq_s16(v82, *(v314 + v312)), v85);
  v321 = vminq_s16(vqaddq_s16(v83, *v315), v85);
  v322 = vqshluq_n_s16(vminq_s16(vqaddq_s16(v78, *(a2 + v312)), v85), 0);
  *a2 = vqshluq_n_s16(vminq_s16(vqaddq_s16(v77, *a2), v85), 0);
  *(a2 + v312) = v322;
  *v313 = vqshluq_n_s16(v318, 0);
  *(v313 + v312) = vqshluq_n_s16(v319, 0);
  *v314 = vqshluq_n_s16(vminq_s16(v316, v85), 0);
  *(v314 + v312) = vqshluq_n_s16(v320, 0);
  result = vqshluq_n_s16(vminq_s16(v317, v85), 0);
  *v315 = vqshluq_n_s16(v321, 0);
  *(v315 + v312) = result;
  return result;
}

int16x8_t vpx_highbd_idct8x8_64_add_neon(int16x8_t *a1, int16x8_t *a2, int a3, int a4)
{
  v5 = *a1;
  v4 = a1[1];
  v7 = a1[2];
  v6 = a1[3];
  v9 = a1[4];
  v8 = a1[5];
  v11 = a1[6];
  v10 = a1[7];
  v13 = a1[8];
  v12 = a1[9];
  v15 = a1[10];
  v14 = a1[11];
  v17 = a1[12];
  v16 = a1[13];
  v19 = a1[14];
  v18 = a1[15];
  if (a4 == 10)
  {
    v174 = vtrn1q_s32(v5, v7);
    v175 = vtrn2q_s32(v5, v7);
    v176 = vtrn1q_s32(v4, v6);
    v177 = vtrn2q_s32(v4, v6);
    v178 = vtrn1q_s32(v9, v11);
    v179 = vtrn2q_s32(v9, v11);
    v180 = vtrn1q_s32(v8, v10);
    v181 = vtrn2q_s32(v8, v10);
    v182 = vzip2q_s64(v174, v178);
    v183.i64[0] = v174.i64[0];
    v183.i64[1] = v178.i64[0];
    v184 = vzip2q_s64(v175, v179);
    v175.i64[1] = v179.i64[0];
    v185 = vzip2q_s64(v176, v180);
    v186.i64[0] = v176.i64[0];
    v186.i64[1] = v180.i64[0];
    v187 = vzip2q_s64(v177, v181);
    v177.i64[1] = v181.i64[0];
    v188 = vdupq_n_s32(0xC7Cu);
    v189 = vdupq_n_s32(0xFFFFDC72);
    v190 = vdupq_n_s32(0x3537u);
    v191 = vmulq_s32(v184, v189);
    v192 = vmulq_s32(v184, v190);
    v193 = vdupq_n_s32(0x3EC5u);
    v194 = vmulq_s32(v175, v193);
    v195 = vdupq_n_s32(0xFFFFC13B);
    v196 = vmlaq_s32(vmulq_s32(v175, v188), v187, v195);
    v197 = vdupq_n_s32(0x238Eu);
    v198 = vmlaq_s32(v192, v177, v197);
    v199 = vrshrq_n_s32(vmlaq_s32(v191, v177, v190), 0xEuLL);
    v200 = vrshrq_n_s32(vmlaq_s32(v194, v187, v188), 0xEuLL);
    v201 = vdupq_n_s32(0x2D41u);
    v202 = vmulq_s32(v183, v201);
    v203 = vdupq_n_s32(0x187Eu);
    v204 = vdupq_n_s32(0x3B21u);
    v205 = vmulq_s32(v182, v203);
    v206 = vmulq_s32(v182, v204);
    v207 = vmulq_s32(v186, v201);
    v208 = vaddq_s32(v207, v202);
    v209 = vsubq_s32(v202, v207);
    v210 = vdupq_n_s32(0xFFFFC4DF);
    v211 = vrshrq_n_s32(vmlaq_s32(v205, v185, v210), 0xEuLL);
    v212 = vrshrq_n_s32(vmlaq_s32(v206, v185, v203), 0xEuLL);
    v213 = vsubq_s32(vrshrq_n_s32(v196, 0xEuLL), v199);
    v214 = vrsraq_n_s32(v199, v196, 0xEuLL);
    v215 = vsubq_s32(v200, vrshrq_n_s32(v198, 0xEuLL));
    v216 = vrsraq_n_s32(v200, v198, 0xEuLL);
    v217 = vsubq_s32(vrshrq_n_s32(v209, 0xEuLL), v211);
    v218 = vrsraq_n_s32(v211, v209, 0xEuLL);
    v219 = vsubq_s32(vrshrq_n_s32(v208, 0xEuLL), v212);
    v220 = vrsraq_n_s32(v212, v208, 0xEuLL);
    v221 = vmulq_s32(v215, v201);
    v222 = vmulq_s32(v213, v201);
    v223 = vsubq_s32(v221, v222);
    v224 = vaddq_s32(v221, v222);
    v225 = vaddq_s32(v220, v216);
    v226 = vaddq_s32(v219, v214);
    v570 = vsubq_s32(v219, v214);
    v227 = vsubq_s32(v217, vrshrq_n_s32(v223, 0xEuLL));
    v228 = vrsraq_n_s32(v217, v223, 0xEuLL);
    v229 = vsubq_s32(v218, vrshrq_n_s32(v224, 0xEuLL));
    v230 = vrsraq_n_s32(v218, v224, 0xEuLL);
    v231 = vsubq_s32(v220, v216);
    v232 = vtrn1q_s32(v13, v15);
    v233 = vtrn2q_s32(v13, v15);
    v234 = vtrn1q_s32(v12, v14);
    v235 = vtrn2q_s32(v12, v14);
    v236 = vtrn1q_s32(v17, v19);
    v237 = vtrn2q_s32(v17, v19);
    v238 = vtrn1q_s32(v16, v18);
    v239 = vtrn2q_s32(v16, v18);
    v240 = vzip2q_s64(v232, v236);
    v232.i64[1] = v236.i64[0];
    v241 = vzip2q_s64(v233, v237);
    v233.i64[1] = v237.i64[0];
    v242 = vzip2q_s64(v234, v238);
    v234.i64[1] = v238.i64[0];
    v243 = vzip2q_s64(v235, v239);
    v235.i64[1] = v239.i64[0];
    v244 = vmulq_s32(v241, v189);
    v245 = vmlaq_s32(vmulq_s32(v233, v188), v243, v195);
    v246 = vmlaq_s32(vmulq_s32(v241, v190), v235, v197);
    v247 = vrshrq_n_s32(vmlaq_s32(v244, v235, v190), 0xEuLL);
    v248 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v233, v193), v243, v188), 0xEuLL);
    v249 = vmulq_s32(v232, v201);
    v250 = vmulq_s32(v234, v201);
    v251 = vaddq_s32(v250, v249);
    v252 = vsubq_s32(v249, v250);
    v253 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v240, v203), v242, v210), 0xEuLL);
    v254 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v240, v204), v242, v203), 0xEuLL);
    v255 = vsubq_s32(vrshrq_n_s32(v245, 0xEuLL), v247);
    v256 = vrsraq_n_s32(v247, v245, 0xEuLL);
    v257 = vsubq_s32(v248, vrshrq_n_s32(v246, 0xEuLL));
    v258 = vrsraq_n_s32(v248, v246, 0xEuLL);
    v259 = vsubq_s32(vrshrq_n_s32(v252, 0xEuLL), v253);
    v260 = vrsraq_n_s32(v253, v252, 0xEuLL);
    v261 = vsubq_s32(vrshrq_n_s32(v251, 0xEuLL), v254);
    v262 = vrsraq_n_s32(v254, v251, 0xEuLL);
    v263 = vmulq_s32(v257, v201);
    v264 = vmulq_s32(v255, v201);
    v265 = vsubq_s32(v263, v264);
    v266 = vaddq_s32(v263, v264);
    v267 = vaddq_s32(v262, v258);
    v268 = vaddq_s32(v261, v256);
    v269 = vsubq_s32(v261, v256);
    v270 = vsubq_s32(v259, vrshrq_n_s32(v265, 0xEuLL));
    v271 = vrsraq_n_s32(v259, v265, 0xEuLL);
    v272 = vsubq_s32(v260, vrshrq_n_s32(v266, 0xEuLL));
    v273 = vrsraq_n_s32(v260, v266, 0xEuLL);
    v274 = vsubq_s32(v262, v258);
    v275 = vtrn1q_s32(v225, v230);
    v276 = vtrn2q_s32(v225, v230);
    v277 = vtrn1q_s32(v267, v273);
    v278 = vtrn2q_s32(v267, v273);
    v279 = vtrn1q_s32(v228, v226);
    v280 = vtrn2q_s32(v228, v226);
    v281 = vtrn1q_s32(v271, v268);
    v282 = vtrn2q_s32(v271, v268);
    v283 = vzip2q_s64(v275, v279);
    v275.i64[1] = v279.i64[0];
    v284 = vzip2q_s64(v276, v280);
    v276.i64[1] = v280.i64[0];
    v285 = vzip2q_s64(v277, v281);
    v277.i64[1] = v281.i64[0];
    v286 = vzip2q_s64(v278, v282);
    v278.i64[1] = v282.i64[0];
    v287 = vmulq_s32(v284, v189);
    v288 = vmlaq_s32(vmulq_s32(v276, v188), v286, v195);
    v289 = vmlaq_s32(vmulq_s32(v284, v190), v278, v197);
    v290 = vrshrq_n_s32(vmlaq_s32(v287, v278, v190), 0xEuLL);
    v291 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v276, v193), v286, v188), 0xEuLL);
    v292 = vmulq_s32(v275, v201);
    v293 = vmulq_s32(v277, v201);
    v294 = vaddq_s32(v293, v292);
    v295 = vsubq_s32(v292, v293);
    v296 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v283, v203), v285, v210), 0xEuLL);
    v297 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v283, v204), v285, v203), 0xEuLL);
    v298 = vsubq_s32(vrshrq_n_s32(v288, 0xEuLL), v290);
    v299 = vrsraq_n_s32(v290, v288, 0xEuLL);
    v300 = vsubq_s32(v291, vrshrq_n_s32(v289, 0xEuLL));
    v301 = vrsraq_n_s32(v291, v289, 0xEuLL);
    v302 = vsubq_s32(vrshrq_n_s32(v295, 0xEuLL), v296);
    v303 = vrsraq_n_s32(v296, v295, 0xEuLL);
    v304 = vsubq_s32(vrshrq_n_s32(v294, 0xEuLL), v297);
    v305 = vrsraq_n_s32(v297, v294, 0xEuLL);
    v306 = vmulq_s32(v300, v201);
    v307 = vmulq_s32(v298, v201);
    v308 = vsubq_s32(v306, v307);
    v309 = vaddq_s32(v306, v307);
    v310 = vaddq_s32(v305, v301);
    v311 = vaddq_s32(v304, v299);
    v312 = vsubq_s32(v304, v299);
    v313 = vsubq_s32(v302, vrshrq_n_s32(v308, 0xEuLL));
    v314 = vrsraq_n_s32(v302, v308, 0xEuLL);
    v315 = vsubq_s32(v303, vrshrq_n_s32(v309, 0xEuLL));
    v316 = vrsraq_n_s32(v303, v309, 0xEuLL);
    v317 = vsubq_s32(v305, v301);
    v318 = vtrn1q_s32(v570, v227);
    v319 = vtrn2q_s32(v570, v227);
    v320 = vtrn1q_s32(v269, v270);
    v321 = vtrn2q_s32(v269, v270);
    v322 = vtrn1q_s32(v229, v231);
    v323 = vtrn2q_s32(v229, v231);
    v324 = vtrn1q_s32(v272, v274);
    v325 = vtrn2q_s32(v272, v274);
    v326 = vzip2q_s64(v318, v322);
    v318.i64[1] = v322.i64[0];
    v327 = vzip2q_s64(v319, v323);
    v319.i64[1] = v323.i64[0];
    v328 = vzip2q_s64(v320, v324);
    v320.i64[1] = v324.i64[0];
    v329 = vzip2q_s64(v321, v325);
    v321.i64[1] = v325.i64[0];
    v330 = vmlaq_s32(vmulq_s32(v319, v188), v329, v195);
    v331 = vmlaq_s32(vmulq_s32(v327, v189), v321, v190);
    v332 = vmlaq_s32(vmulq_s32(v327, v190), v321, v197);
    v333 = vrshrq_n_s32(v331, 0xEuLL);
    v334 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v319, v193), v329, v188), 0xEuLL);
    v335 = vmulq_s32(v318, v201);
    v336 = vmulq_s32(v326, v204);
    v337 = vmulq_s32(v320, v201);
    v338 = vaddq_s32(v337, v335);
    v339 = vsubq_s32(v335, v337);
    v340 = vrshrq_n_s32(vmlaq_s32(vmulq_s32(v326, v203), v328, v210), 0xEuLL);
    v341 = vrshrq_n_s32(vmlaq_s32(v336, v328, v203), 0xEuLL);
    v342 = vsubq_s32(vrshrq_n_s32(v330, 0xEuLL), v333);
    v343 = vrsraq_n_s32(v333, v330, 0xEuLL);
    v344 = vsubq_s32(v334, vrshrq_n_s32(v332, 0xEuLL));
    v345 = vrsraq_n_s32(v334, v332, 0xEuLL);
    v346 = vsubq_s32(vrshrq_n_s32(v339, 0xEuLL), v340);
    v347 = vrsraq_n_s32(v340, v339, 0xEuLL);
    v348 = vsubq_s32(vrshrq_n_s32(v338, 0xEuLL), v341);
    v349 = vrsraq_n_s32(v341, v338, 0xEuLL);
    v350 = vmulq_s32(v344, v201);
    v351 = vmulq_s32(v342, v201);
    v352 = vsubq_s32(v350, v351);
    v353 = vaddq_s32(v350, v351);
    v354 = vaddq_s32(v349, v345);
    v355 = vaddq_s32(v348, v343);
    v356 = vsubq_s32(v348, v343);
    v357 = vsubq_s32(v346, vrshrq_n_s32(v352, 0xEuLL));
    v358 = vrsraq_n_s32(v346, v352, 0xEuLL);
    v359 = vsubq_s32(v347, vrshrq_n_s32(v353, 0xEuLL));
    v360 = vrsraq_n_s32(v347, v353, 0xEuLL);
    v361 = vsubq_s32(v349, v345);
LABEL_6:
    v165 = vrshrn_high_n_s32(vrshrn_n_s32(v310, 5uLL), v354, 5uLL);
    v166 = vrshrn_high_n_s32(vrshrn_n_s32(v316, 5uLL), v360, 5uLL);
    v167 = vrshrn_high_n_s32(vrshrn_n_s32(v314, 5uLL), v358, 5uLL);
    v168 = vrshrn_high_n_s32(vrshrn_n_s32(v311, 5uLL), v355, 5uLL);
    v169 = vrshrn_high_n_s32(vrshrn_n_s32(v312, 5uLL), v356, 5uLL);
    v170 = vrshrn_high_n_s32(vrshrn_n_s32(v313, 5uLL), v357, 5uLL);
    v171 = vrshrn_high_n_s32(vrshrn_n_s32(v315, 5uLL), v359, 5uLL);
    v172 = vrshrn_high_n_s32(vrshrn_n_s32(v317, 5uLL), v361, 5uLL);
    v173 = vdupq_n_s16(~(-1 << a4));
    goto LABEL_7;
  }

  if (a4 != 8)
  {
    v362 = vtrn1q_s32(v5, v7);
    v363 = vtrn2q_s32(v5, v7);
    v364 = vtrn1q_s32(v4, v6);
    v365 = vtrn2q_s32(v4, v6);
    v366 = vtrn1q_s32(v9, v11);
    v367 = vtrn2q_s32(v9, v11);
    v368 = vtrn1q_s32(v8, v10);
    v369 = vtrn2q_s32(v8, v10);
    v370 = vzip2q_s64(v362, v366);
    v371 = vzip2q_s64(v363, v367);
    v372 = vzip2q_s64(v364, v368);
    v373 = vzip2q_s64(v365, v369);
    v374 = vdupq_n_s32(0xC7Cu);
    v375 = vdupq_n_s32(0xFFFFDC72);
    v376 = vdupq_n_s32(0x3537u);
    v377 = vdupq_n_s32(0x3EC5u);
    v378 = vmlsl_high_s32(vmull_s32(*v367.i8, *v374.i8), v373, v377);
    v379 = vmlal_s32(vmull_s32(*v365.i8, *v376.i8), *v371.i8, *v375.i8);
    v568 = a1[11];
    v571 = a1[8];
    v380 = vmlal_high_s32(vmull_s32(*v369.i8, *v376.i8), v371, v375);
    v381 = vmlsl_s32(vmull_s32(*v371.i8, *v376.i8), *v365.i8, *v375.i8);
    v382 = vmlsl_s32(vmull_high_s32(v371, v376), *v369.i8, *v375.i8);
    v383 = vmlal_s32(vmull_s32(*v373.i8, *v374.i8), *v363.i8, *v377.i8);
    v384 = vmlal_high_s32(vmull_s32(*v367.i8, *v377.i8), v373, v374);
    *v363.i8 = vrshrn_n_s64(vmlsl_s32(vmull_s32(*v363.i8, *v374.i8), *v373.i8, *v377.i8), 0xEuLL);
    *v371.i8 = vrshrn_n_s64(v379, 0xEuLL);
    v385 = vdup_n_s32(0x2D41u);
    v386 = vmull_s32(*v362.i8, v385);
    v387 = vdupq_n_s32(0x187Eu);
    v388 = a1[15];
    v389 = a1[13];
    v390 = vmlal_s32(vmull_s32(*v364.i8, v385), *v362.i8, v385);
    v391 = vmlal_s32(vmull_s32(*v368.i8, v385), *v366.i8, v385);
    v392 = vdupq_n_s32(0x3B21u);
    v393 = vmlsl_s32(v386, *v364.i8, v385);
    v394 = vmlsl_s32(vmull_s32(*v366.i8, v385), *v368.i8, v385);
    v395 = vmlsl_s32(vmull_s32(*v370.i8, *v387.i8), *v372.i8, *v392.i8);
    v396 = vmlsl_high_s32(vmull_high_s32(v370, v387), v372, v392);
    v397 = vmlal_s32(vmull_s32(*v372.i8, *v387.i8), *v370.i8, *v392.i8);
    v398 = vmlal_high_s32(vmull_high_s32(v372, v387), v370, v392);
    *v370.i8 = vsub_s32(*v363.i8, *v371.i8);
    *v381.i8 = vrshrn_n_s64(v381, 0xEuLL);
    *v366.i8 = vrshrn_n_s64(v383, 0xEuLL);
    v399 = vrshrn_high_n_s64(vrshrn_n_s64(v390, 0xEuLL), v391, 0xEuLL);
    *v391.i8 = vsub_s32(*v366.i8, *v381.i8);
    v400 = vrshrn_high_n_s64(vrshrn_n_s64(v393, 0xEuLL), v394, 0xEuLL);
    v401 = vrshrn_high_n_s64(vrshrn_n_s64(v395, 0xEuLL), v396, 0xEuLL);
    v402 = vrshrn_high_n_s64(vrshrn_n_s64(v397, 0xEuLL), v398, 0xEuLL);
    v403 = vaddq_s32(vrshrn_high_n_s64(*v371.i8, v380, 0xEuLL), vrshrn_high_n_s64(*v363.i8, v378, 0xEuLL));
    v404 = vaddq_s32(vrshrn_high_n_s64(*v366.i8, v384, 0xEuLL), vrshrn_high_n_s64(*v381.i8, v382, 0xEuLL));
    v405 = vaddq_s32(v402, v399);
    v406 = vaddq_s32(v401, v400);
    v407 = vsubq_s32(v400, v401);
    v408 = vsubq_s32(v399, v402);
    *v382.i8 = vsub_s32(vrshrn_n_s64(v384, 0xEuLL), vrshrn_n_s64(v382, 0xEuLL));
    *v378.i8 = vsub_s32(vrshrn_n_s64(v378, 0xEuLL), vrshrn_n_s64(v380, 0xEuLL));
    v409 = vmlal_s32(vmull_s32(*v370.i8, v385), *v391.i8, v385);
    v410 = vrshrn_high_n_s64(vrshrn_n_s64(vmlsl_s32(vmull_s32(*v391.i8, v385), *v370.i8, v385), 0xEuLL), vmlsl_s32(vmull_s32(*v382.i8, v385), *v378.i8, v385), 0xEuLL);
    v411 = vrshrn_high_n_s64(vrshrn_n_s64(v409, 0xEuLL), vmlal_s32(vmull_s32(*v378.i8, v385), *v382.i8, v385), 0xEuLL);
    v412 = vaddq_s32(v405, v404);
    v555 = vaddq_s32(v411, v406);
    v550 = vaddq_s32(v410, v407);
    v413 = vaddq_s32(v408, v403);
    v562 = vsubq_s32(v408, v403);
    v560 = vsubq_s32(v407, v410);
    v564 = vsubq_s32(v405, v404);
    v566 = vsubq_s32(v406, v411);
    v414 = vtrn1q_s32(v571, v15);
    v415 = vtrn2q_s32(v571, v15);
    v416 = vtrn1q_s32(v12, v568);
    v417 = vtrn2q_s32(v12, v568);
    v418 = vtrn1q_s32(v17, v19);
    v419 = vtrn2q_s32(v17, v19);
    v420 = vtrn1q_s32(v389, v388);
    v421 = vtrn2q_s32(v389, v388);
    v422 = vzip2q_s64(v414, v418);
    v423 = vzip2q_s64(v415, v419);
    v424 = vzip2q_s64(v416, v420);
    v425 = vzip2q_s64(v417, v421);
    v426 = vmlal_s32(vmull_s32(*v417.i8, *v376.i8), *v423.i8, *v375.i8);
    v427 = vmlal_high_s32(vmull_s32(*v421.i8, *v376.i8), v423, v375);
    v428 = vmlsl_high_s32(vmull_s32(*v419.i8, *v374.i8), v425, v377);
    v429 = vmlsl_s32(vmull_s32(*v423.i8, *v376.i8), *v417.i8, *v375.i8);
    v430 = vmlsl_s32(vmull_high_s32(v423, v376), *v421.i8, *v375.i8);
    v431 = vmlal_s32(vmull_s32(*v425.i8, *v374.i8), *v415.i8, *v377.i8);
    v432 = vmlal_high_s32(vmull_s32(*v419.i8, *v377.i8), v425, v374);
    *v417.i8 = vrshrn_n_s64(vmlsl_s32(vmull_s32(*v415.i8, *v374.i8), *v425.i8, *v377.i8), 0xEuLL);
    *v425.i8 = vrshrn_n_s64(v426, 0xEuLL);
    v433 = vmull_s32(*v414.i8, v385);
    v434 = vmlal_s32(vmull_s32(*v416.i8, v385), *v414.i8, v385);
    v435 = vmlal_s32(vmull_s32(*v420.i8, v385), *v418.i8, v385);
    v436 = vmlsl_s32(v433, *v416.i8, v385);
    v437 = vmlsl_s32(vmull_s32(*v418.i8, v385), *v420.i8, v385);
    v438 = vmlsl_s32(vmull_s32(*v422.i8, *v387.i8), *v424.i8, *v392.i8);
    v439 = vmlsl_high_s32(vmull_high_s32(v422, v387), v424, v392);
    v440 = vmlal_s32(vmull_s32(*v424.i8, *v387.i8), *v422.i8, *v392.i8);
    v441 = vmlal_high_s32(vmull_high_s32(v424, v387), v422, v392);
    *v422.i8 = vsub_s32(*v417.i8, *v425.i8);
    *v429.i8 = vrshrn_n_s64(v429, 0xEuLL);
    *v431.i8 = vrshrn_n_s64(v431, 0xEuLL);
    v442 = vrshrn_high_n_s64(vrshrn_n_s64(v434, 0xEuLL), v435, 0xEuLL);
    *v435.i8 = vsub_s32(*v431.i8, *v429.i8);
    v443 = vrshrn_high_n_s64(vrshrn_n_s64(v436, 0xEuLL), v437, 0xEuLL);
    v444 = vrshrn_high_n_s64(vrshrn_n_s64(v438, 0xEuLL), v439, 0xEuLL);
    v445 = vrshrn_high_n_s64(vrshrn_n_s64(v440, 0xEuLL), v441, 0xEuLL);
    v446 = vaddq_s32(vrshrn_high_n_s64(*v425.i8, v427, 0xEuLL), vrshrn_high_n_s64(*v417.i8, v428, 0xEuLL));
    v447 = vaddq_s32(vrshrn_high_n_s64(*v431.i8, v432, 0xEuLL), vrshrn_high_n_s64(*v429.i8, v430, 0xEuLL));
    v448 = vaddq_s32(v445, v442);
    v449 = vaddq_s32(v444, v443);
    v450 = vsubq_s32(v443, v444);
    v451 = vsubq_s32(v442, v445);
    *v445.i8 = vsub_s32(vrshrn_n_s64(v432, 0xEuLL), vrshrn_n_s64(v430, 0xEuLL));
    *v428.i8 = vsub_s32(vrshrn_n_s64(v428, 0xEuLL), vrshrn_n_s64(v427, 0xEuLL));
    v452 = vmlal_s32(vmull_s32(*v428.i8, v385), *v445.i8, v385);
    v453 = vrshrn_high_n_s64(vrshrn_n_s64(vmlsl_s32(vmull_s32(*v435.i8, v385), *v422.i8, v385), 0xEuLL), vmlsl_s32(vmull_s32(*v445.i8, v385), *v428.i8, v385), 0xEuLL);
    v454 = vrshrn_high_n_s64(vrshrn_n_s64(vmlal_s32(vmull_s32(*v422.i8, v385), *v435.i8, v385), 0xEuLL), v452, 0xEuLL);
    v455 = vaddq_s32(v448, v447);
    v456 = vaddq_s32(v454, v449);
    v457 = vaddq_s32(v453, v450);
    v458 = vaddq_s32(v451, v446);
    v459 = vsubq_s32(v451, v446);
    v460 = vsubq_s32(v450, v453);
    v461 = vsubq_s32(v449, v454);
    v462 = vsubq_s32(v448, v447);
    v463 = vtrn2q_s32(v412, v555);
    v464 = vtrn2q_s32(v455, v456);
    v465 = vtrn2q_s32(v550, v413);
    v466 = vtrn2q_s32(v457, v458);
    v467 = vzip2q_s64(v463, v465);
    v468 = vzip2q_s64(v464, v466);
    v469 = vmlal_s32(vmull_s32(*v464.i8, *v376.i8), *v467.i8, *v375.i8);
    v470 = vmlsl_s32(vmull_s32(*v467.i8, *v376.i8), *v464.i8, *v375.i8);
    v559 = vmlal_high_s32(vmull_s32(*v466.i8, *v376.i8), v467, v375);
    v471 = vmlsl_s32(vmull_high_s32(v467, v376), *v466.i8, *v375.i8);
    v472 = vmlal_s32(vmull_s32(*v468.i8, *v374.i8), *v463.i8, *v377.i8);
    v473 = vmlsl_s32(vmull_s32(*v463.i8, *v374.i8), *v468.i8, *v377.i8);
    v557 = vmlal_high_s32(vmull_s32(*v465.i8, *v377.i8), v468, v374);
    v558 = vmlsl_high_s32(vmull_s32(*v465.i8, *v374.i8), v468, v377);
    v474 = vtrn2q_s32(v562, v560);
    v475 = vtrn2q_s32(v459, v460);
    v476 = vtrn2q_s32(v566, v564);
    v477 = vzip2q_s64(v474, v476);
    v552 = v471;
    v553 = vmlal_s32(vmull_s32(*v475.i8, *v376.i8), *v477.i8, *v375.i8);
    v478 = vtrn2q_s32(v461, v462);
    v547 = vmlal_high_s32(vmull_s32(*v478.i8, *v376.i8), v477, v375);
    v548 = vmlsl_s32(vmull_s32(*v477.i8, *v376.i8), *v475.i8, *v375.i8);
    v569 = vmlsl_s32(vmull_high_s32(v477, v376), *v478.i8, *v375.i8);
    v479 = vzip2q_s64(v475, v478);
    v480 = vmlal_s32(vmull_s32(*v479.i8, *v374.i8), *v474.i8, *v377.i8);
    v481 = vmlsl_s32(vmull_s32(*v474.i8, *v374.i8), *v479.i8, *v377.i8);
    v546 = vmlsl_high_s32(vmull_s32(*v476.i8, *v374.i8), v479, v377);
    v572 = vmlal_high_s32(vmull_s32(*v476.i8, *v377.i8), v479, v374);
    v482 = vtrn1q_s32(v412, v555);
    v483 = vtrn1q_s32(v455, v456);
    v484 = vtrn1q_s32(v550, v413);
    v485 = vtrn1q_s32(v457, v458);
    v486 = vzip2q_s64(v482, v484);
    v487 = vzip2q_s64(v483, v485);
    v556 = vmlsl_s32(vmull_s32(*v486.i8, *v387.i8), *v487.i8, *v392.i8);
    v554 = vmlsl_high_s32(vmull_high_s32(v486, v387), v487, v392);
    v551 = vmlal_s32(vmull_s32(*v487.i8, *v387.i8), *v486.i8, *v392.i8);
    v549 = vmlal_high_s32(vmull_high_s32(v487, v387), v486, v392);
    v488 = vtrn1q_s32(v562, v560);
    v489 = vtrn1q_s32(v459, v460);
    v490 = vtrn1q_s32(v566, v564);
    v491 = vtrn1q_s32(v461, v462);
    v492 = vzip2q_s64(v488, v490);
    v493 = vzip2q_s64(v489, v491);
    v567 = vmlsl_s32(vmull_s32(*v492.i8, *v387.i8), *v493.i8, *v392.i8);
    v565 = vmlsl_high_s32(vmull_high_s32(v492, v387), v493, v392);
    v561 = vmlal_high_s32(vmull_high_s32(v493, v387), v492, v392);
    v563 = vmlal_s32(vmull_s32(*v493.i8, *v387.i8), *v492.i8, *v392.i8);
    v494 = vmlal_s32(vmull_s32(*v483.i8, v385), *v482.i8, v385);
    v495 = vmlsl_s32(vmull_s32(*v482.i8, v385), *v483.i8, v385);
    v496 = vmlal_s32(vmull_s32(*v485.i8, v385), *v484.i8, v385);
    v497 = vmlsl_s32(vmull_s32(*v484.i8, v385), *v485.i8, v385);
    *v485.i8 = vsub_s32(vrshrn_n_s64(v557, 0xEuLL), vrshrn_n_s64(v552, 0xEuLL));
    *v484.i8 = vsub_s32(vrshrn_n_s64(v558, 0xEuLL), vrshrn_n_s64(v559, 0xEuLL));
    *v492.i8 = vrshrn_n_s64(v473, 0xEuLL);
    *v493.i8 = vrshrn_n_s64(v469, 0xEuLL);
    *v468.i8 = vrshrn_n_s64(v470, 0xEuLL);
    *v452.i8 = vrshrn_n_s64(v472, 0xEuLL);
    *v473.i8 = vsub_s32(*v452.i8, *v468.i8);
    *v470.i8 = vsub_s32(*v492.i8, *v493.i8);
    v498 = vmlsl_s32(vmull_s32(*v473.i8, v385), *v470.i8, v385);
    v499 = vmlal_s32(vmull_s32(*v470.i8, v385), *v473.i8, v385);
    v500 = vmlsl_s32(vmull_s32(*v485.i8, v385), *v484.i8, v385);
    v501 = vmlal_s32(vmull_s32(*v484.i8, v385), *v485.i8, v385);
    v502 = vmlal_s32(vmull_s32(*v489.i8, v385), *v488.i8, v385);
    v503 = vmlsl_s32(vmull_s32(*v488.i8, v385), *v489.i8, v385);
    v504 = vmlal_s32(vmull_s32(*v491.i8, v385), *v490.i8, v385);
    v505 = vmlsl_s32(vmull_s32(*v490.i8, v385), *v491.i8, v385);
    *v488.i8 = vsub_s32(vrshrn_n_s64(v572, 0xEuLL), vrshrn_n_s64(v569, 0xEuLL));
    *v490.i8 = vsub_s32(vrshrn_n_s64(v546, 0xEuLL), vrshrn_n_s64(v547, 0xEuLL));
    *v481.i8 = vrshrn_n_s64(v481, 0xEuLL);
    *v484.i8 = vrshrn_n_s64(v553, 0xEuLL);
    *v485.i8 = vrshrn_n_s64(v548, 0xEuLL);
    *v461.i8 = vrshrn_n_s64(v480, 0xEuLL);
    *v491.i8 = vsub_s32(*v461.i8, *v485.i8);
    *v480.i8 = vsub_s32(*v481.i8, *v484.i8);
    v506 = vmlsl_s32(vmull_s32(*v491.i8, v385), *v480.i8, v385);
    v507 = vmlal_s32(vmull_s32(*v480.i8, v385), *v491.i8, v385);
    v508 = vmlsl_s32(vmull_s32(*v488.i8, v385), *v490.i8, v385);
    v509 = vmlal_s32(vmull_s32(*v490.i8, v385), *v488.i8, v385);
    v510 = vrshrn_high_n_s64(vrshrn_n_s64(v494, 0xEuLL), v496, 0xEuLL);
    v511 = vrshrn_high_n_s64(vrshrn_n_s64(v495, 0xEuLL), v497, 0xEuLL);
    v512 = vrshrn_high_n_s64(vrshrn_n_s64(v556, 0xEuLL), v554, 0xEuLL);
    v513 = vrshrn_high_n_s64(vrshrn_n_s64(v551, 0xEuLL), v549, 0xEuLL);
    v514 = vaddq_s32(vrshrn_high_n_s64(*v493.i8, v559, 0xEuLL), vrshrn_high_n_s64(*v492.i8, v558, 0xEuLL));
    v515 = vaddq_s32(vrshrn_high_n_s64(*v452.i8, v557, 0xEuLL), vrshrn_high_n_s64(*v468.i8, v552, 0xEuLL));
    v516 = vaddq_s32(v512, v511);
    v517 = vsubq_s32(v511, v512);
    v518 = vaddq_s32(v513, v510);
    v519 = vsubq_s32(v510, v513);
    v520 = vrshrn_high_n_s64(vrshrn_n_s64(v498, 0xEuLL), v500, 0xEuLL);
    v521 = vrshrn_high_n_s64(vrshrn_n_s64(v499, 0xEuLL), v501, 0xEuLL);
    v311 = vaddq_s32(v519, v514);
    v314 = vaddq_s32(v520, v517);
    v313 = vsubq_s32(v517, v520);
    v316 = vaddq_s32(v521, v516);
    v315 = vsubq_s32(v516, v521);
    v312 = vsubq_s32(v519, v514);
    v310 = vaddq_s32(v518, v515);
    v317 = vsubq_s32(v518, v515);
    v522 = vrshrn_high_n_s64(vrshrn_n_s64(v502, 0xEuLL), v504, 0xEuLL);
    v523 = vrshrn_high_n_s64(vrshrn_n_s64(v503, 0xEuLL), v505, 0xEuLL);
    v524 = vrshrn_high_n_s64(vrshrn_n_s64(v567, 0xEuLL), v565, 0xEuLL);
    v525 = vrshrn_high_n_s64(vrshrn_n_s64(v563, 0xEuLL), v561, 0xEuLL);
    v526 = vaddq_s32(vrshrn_high_n_s64(*v484.i8, v547, 0xEuLL), vrshrn_high_n_s64(*v481.i8, v546, 0xEuLL));
    v527 = vaddq_s32(vrshrn_high_n_s64(*v461.i8, v572, 0xEuLL), vrshrn_high_n_s64(*v485.i8, v569, 0xEuLL));
    v528 = vaddq_s32(v524, v523);
    v529 = vsubq_s32(v523, v524);
    v530 = vaddq_s32(v525, v522);
    v531 = vsubq_s32(v522, v525);
    v532 = vrshrn_high_n_s64(vrshrn_n_s64(v506, 0xEuLL), v508, 0xEuLL);
    v533 = vrshrn_high_n_s64(vrshrn_n_s64(v507, 0xEuLL), v509, 0xEuLL);
    v355 = vaddq_s32(v531, v526);
    v356 = vsubq_s32(v531, v526);
    v358 = vaddq_s32(v532, v529);
    v357 = vsubq_s32(v529, v532);
    v360 = vaddq_s32(v533, v528);
    v359 = vsubq_s32(v528, v533);
    v354 = vaddq_s32(v530, v527);
    v361 = vsubq_s32(v530, v527);
    goto LABEL_6;
  }

  v20 = vuzp1q_s16(v5, v4);
  v21 = vuzp1q_s16(v7, v6);
  v22 = vuzp1q_s16(v9, v8);
  v23 = vuzp1q_s16(v11, v10);
  v24 = vuzp1q_s16(v13, v12);
  v25 = vuzp1q_s16(v15, v14);
  v26 = vuzp1q_s16(v17, v16);
  v27 = vuzp1q_s16(v19, v18);
  v28 = vtrn1q_s16(v20, v21);
  v29 = vtrn2q_s16(v20, v21);
  v30 = vtrn1q_s16(v22, v23);
  v31 = vtrn2q_s16(v22, v23);
  v32 = vtrn1q_s16(v24, v25);
  v33 = vtrn2q_s16(v24, v25);
  v34 = vtrn1q_s16(v26, v27);
  v35 = vtrn2q_s16(v26, v27);
  v36 = vtrn1q_s32(v28, v30);
  v37 = vtrn2q_s32(v28, v30);
  v38 = vtrn1q_s32(v29, v31);
  v39 = vtrn2q_s32(v29, v31);
  v40 = vtrn1q_s32(v32, v34);
  v41 = vtrn2q_s32(v32, v34);
  v42 = vtrn1q_s32(v33, v35);
  v43 = vtrn2q_s32(v33, v35);
  v44 = vzip2q_s64(v36, v40);
  v45 = vzip2q_s64(v38, v42);
  v46 = vzip2q_s64(v37, v41);
  v47 = vzip2q_s64(v39, v43);
  v48 = vdupq_n_s16(0xC7Cu);
  v33.i64[0] = vextq_s8(v48, v48, 8uLL).u64[0];
  v49 = vdupq_n_s16(0xDC72u);
  v50 = vextq_s8(v49, v49, 8uLL).u64[0];
  v51 = vdupq_n_s16(0x3537u);
  v52 = vmull_s16(*v43.i8, v50);
  v53 = vextq_s8(v51, v51, 8uLL).u64[0];
  v54 = vdupq_n_s16(0x3EC5u);
  v55 = vmull_s16(*v43.i8, v53);
  v43.i64[0] = vextq_s8(v54, v54, 8uLL).u64[0];
  v56 = vmlsl_s16(vmull_s16(*v38.i8, *v48.i8), *v47.i8, *v54.i8);
  v57 = vmlsl_high_s16(vmull_s16(*v42.i8, *v33.i8), v47, v54);
  v58 = vmlal_s16(vmull_s16(*v45.i8, *v51.i8), *v39.i8, *v49.i8);
  v59 = vmlal_high_s16(v52, v45, v51);
  v60 = vmlsl_s16(vmull_s16(*v39.i8, *v51.i8), *v45.i8, *v49.i8);
  v61 = vmlsl_high_s16(v55, v45, v49);
  v62 = vmlal_s16(vmull_s16(*v47.i8, *v48.i8), *v38.i8, *v54.i8);
  v63 = vmlal_high_s16(vmull_s16(*v42.i8, *v43.i8), v47, v48);
  v64 = vdupq_n_s16(0x2D41u);
  v65 = vmull_s16(*v36.i8, *v64.i8);
  v39.i64[0] = vextq_s8(v64, v64, 8uLL).u64[0];
  v66 = vmull_s16(*v40.i8, *v39.i8);
  v67 = vdupq_n_s16(0x187Eu);
  v68 = vmull_s16(*v37.i8, *v67.i8);
  v45.i64[0] = vextq_s8(v67, v67, 8uLL).u64[0];
  v69 = vdupq_n_s16(0x3B21u);
  v70 = vmlal_s16(vmull_s16(*v44.i8, *v64.i8), *v36.i8, *v64.i8);
  v71 = vmlal_high_s16(v66, v44, v64);
  v72 = vmlsl_s16(v65, *v44.i8, *v64.i8);
  v73 = vmlsl_high_s16(v66, v44, v64);
  v74 = vmlal_s16(vmull_s16(*v46.i8, *v67.i8), *v37.i8, *v69.i8);
  v37.i64[0] = vextq_s8(v69, v69, 8uLL).u64[0];
  v75 = vmlsl_s16(v68, *v46.i8, *v69.i8);
  v76 = vmlsl_high_s16(vmull_s16(*v41.i8, *v45.i8), v46, v69);
  v77 = vmlal_high_s16(vmull_s16(*v41.i8, *v37.i8), v46, v67);
  v78 = vrshrn_high_n_s32(vrshrn_n_s32(v70, 0xEuLL), v71, 0xEuLL);
  v79 = vrshrn_high_n_s32(vrshrn_n_s32(v72, 0xEuLL), v73, 0xEuLL);
  v80 = vrshrn_high_n_s32(vrshrn_n_s32(v75, 0xEuLL), v76, 0xEuLL);
  *v56.i8 = vrshrn_n_s32(v56, 0xEuLL);
  *v58.i8 = vrshrn_n_s32(v58, 0xEuLL);
  v81 = vrshrn_high_n_s32(vrshrn_n_s32(v74, 0xEuLL), v77, 0xEuLL);
  *v77.i8 = vsub_s16(*v56.i8, *v58.i8);
  *v60.i8 = vrshrn_n_s32(v60, 0xEuLL);
  *v62.i8 = vrshrn_n_s32(v62, 0xEuLL);
  v82 = vaddq_s16(vrshrn_high_n_s32(*v58.i8, v59, 0xEuLL), vrshrn_high_n_s32(*v56.i8, v57, 0xEuLL));
  *v58.i8 = vsub_s16(*v62.i8, *v60.i8);
  v83 = vaddq_s16(vrshrn_high_n_s32(*v62.i8, v63, 0xEuLL), vrshrn_high_n_s32(*v60.i8, v61, 0xEuLL));
  v84 = vaddq_s16(v80, v79);
  v85 = vsubq_s16(v79, v80);
  v86 = vaddq_s16(v81, v78);
  v87 = vsubq_s16(v78, v81);
  *v61.i8 = vsub_s16(vrshrn_n_s32(v63, 0xEuLL), vrshrn_n_s32(v61, 0xEuLL));
  *v57.i8 = vsub_s16(vrshrn_n_s32(v57, 0xEuLL), vrshrn_n_s32(v59, 0xEuLL));
  v88 = vrshrn_high_n_s32(vrshrn_n_s32(vmlsl_s16(vmull_s16(*v58.i8, *v64.i8), *v77.i8, *v64.i8), 0xEuLL), vmlsl_s16(vmull_s16(*v61.i8, *v64.i8), *v57.i8, *v64.i8), 0xEuLL);
  v89 = vrshrn_high_n_s32(vrshrn_n_s32(vmlal_s16(vmull_s16(*v77.i8, *v64.i8), *v58.i8, *v64.i8), 0xEuLL), vmlal_s16(vmull_s16(*v57.i8, *v64.i8), *v61.i8, *v64.i8), 0xEuLL);
  v90 = vaddq_s16(v86, v83);
  v91 = vaddq_s16(v87, v82);
  v92 = vsubq_s16(v87, v82);
  v93 = vaddq_s16(v88, v85);
  v94 = vsubq_s16(v85, v88);
  v95 = vaddq_s16(v89, v84);
  v96 = vsubq_s16(v84, v89);
  v97 = vsubq_s16(v86, v83);
  v98 = vtrn1q_s16(v90, v95);
  v99 = vtrn2q_s16(v90, v95);
  v100 = vtrn1q_s16(v93, v91);
  v101 = vtrn2q_s16(v93, v91);
  v102 = vtrn1q_s16(v92, v94);
  v103 = vtrn2q_s16(v92, v94);
  v104 = vtrn1q_s16(v96, v97);
  v105 = vtrn2q_s16(v96, v97);
  v106 = vtrn1q_s32(v98, v100);
  v107 = vtrn2q_s32(v98, v100);
  v108 = vtrn1q_s32(v99, v101);
  v109 = vtrn2q_s32(v99, v101);
  v110 = vtrn1q_s32(v102, v104);
  v111 = vtrn2q_s32(v102, v104);
  v112 = vtrn1q_s32(v103, v105);
  v113 = vtrn2q_s32(v103, v105);
  v114 = vzip2q_s64(v108, v112);
  v115 = vmlal_high_s16(vmull_s16(*v113.i8, v50), v114, v51);
  v116 = vmlsl_s16(vmull_s16(*v109.i8, *v51.i8), *v114.i8, *v49.i8);
  v117 = vmlsl_high_s16(vmull_s16(*v113.i8, v53), v114, v49);
  v118 = vmlal_s16(vmull_s16(*v114.i8, *v51.i8), *v109.i8, *v49.i8);
  v119 = vzip2q_s64(v109, v113);
  v120 = vzip2q_s64(v106, v110);
  v121 = vmull_s16(*v112.i8, *v33.i8);
  v122 = vmull_s16(*v112.i8, *v43.i8);
  v123 = vmlal_s16(vmull_s16(*v119.i8, *v48.i8), *v108.i8, *v54.i8);
  v124 = vmlsl_s16(vmull_s16(*v108.i8, *v48.i8), *v119.i8, *v54.i8);
  v125 = vmlsl_high_s16(v121, v119, v54);
  v126 = vzip2q_s64(v107, v111);
  v127 = vmlal_high_s16(v122, v119, v48);
  *v48.i8 = vrshrn_n_s32(v124, 0xEuLL);
  *v54.i8 = vrshrn_n_s32(v118, 0xEuLL);
  *v118.i8 = vrshrn_n_s32(v116, 0xEuLL);
  v128 = vmull_s16(*v111.i8, *v37.i8);
  v129 = vmlal_s16(vmull_s16(*v126.i8, *v67.i8), *v107.i8, *v69.i8);
  v130 = vmlsl_s16(vmull_s16(*v107.i8, *v67.i8), *v126.i8, *v69.i8);
  v131 = vmlsl_high_s16(vmull_s16(*v111.i8, *v45.i8), v126, v69);
  *v111.i8 = vrshrn_n_s32(v123, 0xEuLL);
  v132 = vmlal_high_s16(v128, v126, v67);
  v133 = vmull_s16(*v106.i8, *v64.i8);
  v134 = vmull_s16(*v110.i8, *v39.i8);
  v135 = vmlal_s16(vmull_s16(*v120.i8, *v64.i8), *v106.i8, *v64.i8);
  v136 = vmlal_high_s16(v134, v120, v64);
  v137 = vmlsl_s16(v133, *v120.i8, *v64.i8);
  v138 = vmlsl_high_s16(v134, v120, v64);
  *v120.i8 = vsub_s16(*v111.i8, *v118.i8);
  *v67.i8 = vsub_s16(vrshrn_n_s32(v127, 0xEuLL), vrshrn_n_s32(v117, 0xEuLL));
  *v119.i8 = vsub_s16(vrshrn_n_s32(v125, 0xEuLL), vrshrn_n_s32(v115, 0xEuLL));
  *v113.i8 = vsub_s16(*v48.i8, *v54.i8);
  v139 = vmlsl_s16(vmull_s16(*v120.i8, *v64.i8), *v113.i8, *v64.i8);
  v140 = vmlal_s16(vmull_s16(*v113.i8, *v64.i8), *v120.i8, *v64.i8);
  v141 = vmlsl_s16(vmull_s16(*v67.i8, *v64.i8), *v119.i8, *v64.i8);
  v142 = vmlal_s16(vmull_s16(*v119.i8, *v64.i8), *v67.i8, *v64.i8);
  v143 = vrshrn_high_n_s32(*v48.i8, v125, 0xEuLL);
  v144 = vrshrn_high_n_s32(*v111.i8, v127, 0xEuLL);
  v145 = vrshrn_high_n_s32(vrshrn_n_s32(v135, 0xEuLL), v136, 0xEuLL);
  v146 = vrshrn_high_n_s32(vrshrn_n_s32(v137, 0xEuLL), v138, 0xEuLL);
  v147 = vrshrn_high_n_s32(vrshrn_n_s32(v130, 0xEuLL), v131, 0xEuLL);
  v148 = vrshrn_high_n_s32(vrshrn_n_s32(v129, 0xEuLL), v132, 0xEuLL);
  v149 = vaddq_s16(vrshrn_high_n_s32(*v54.i8, v115, 0xEuLL), v143);
  v150 = vaddq_s16(v144, vrshrn_high_n_s32(*v118.i8, v117, 0xEuLL));
  v151 = vaddq_s16(v147, v146);
  v152 = vsubq_s16(v146, v147);
  v153 = vaddq_s16(v148, v145);
  v154 = vsubq_s16(v145, v148);
  v155 = vrshrn_high_n_s32(vrshrn_n_s32(v139, 0xEuLL), v141, 0xEuLL);
  v156 = vrshrn_high_n_s32(vrshrn_n_s32(v140, 0xEuLL), v142, 0xEuLL);
  v157 = vaddq_s16(v154, v149);
  v158 = vsubq_s16(v154, v149);
  v159 = vaddq_s16(v155, v152);
  v160 = vsubq_s16(v152, v155);
  v161 = vaddq_s16(v156, v151);
  v162 = vsubq_s16(v151, v156);
  v163 = vaddq_s16(v153, v150);
  v164 = vsubq_s16(v153, v150);
  v165 = vrshrq_n_s16(v163, 5uLL);
  v166 = vrshrq_n_s16(v161, 5uLL);
  v167 = vrshrq_n_s16(v159, 5uLL);
  v168 = vrshrq_n_s16(v157, 5uLL);
  v169 = vrshrq_n_s16(v158, 5uLL);
  v170 = vrshrq_n_s16(v160, 5uLL);
  v171 = vrshrq_n_s16(v162, 5uLL);
  v172 = vrshrq_n_s16(v164, 5uLL);
  v173.i64[0] = 0xFF00FF00FF00FFLL;
  v173.i64[1] = 0xFF00FF00FF00FFLL;
LABEL_7:
  v534 = 2 * a3;
  v535 = (a2 + v534 + v534);
  v536 = (v535 + v534 + v534);
  v537 = (v536 + v534 + v534);
  v538 = vqaddq_s16(v172, *(v537 + v534));
  v539 = vminq_s16(vqaddq_s16(v167, *v535), v173);
  v540 = vminq_s16(vqaddq_s16(v168, *(v535 + v534)), v173);
  v541 = vminq_s16(vqaddq_s16(v169, *v536), v173);
  v542 = vminq_s16(vqaddq_s16(v170, *(v536 + v534)), v173);
  v543 = vminq_s16(vqaddq_s16(v171, *v537), v173);
  v544 = vqshluq_n_s16(vminq_s16(vqaddq_s16(v166, *(a2 + v534)), v173), 0);
  *a2 = vqshluq_n_s16(vminq_s16(vqaddq_s16(v165, *a2), v173), 0);
  *(a2 + v534) = v544;
  *v535 = vqshluq_n_s16(v539, 0);
  *(v535 + v534) = vqshluq_n_s16(v540, 0);
  *v536 = vqshluq_n_s16(v541, 0);
  *(v536 + v534) = vqshluq_n_s16(v542, 0);
  result = vqshluq_n_s16(vminq_s16(v538, v173), 0);
  *v537 = vqshluq_n_s16(v543, 0);
  *(v537 + v534) = result;
  return result;
}

uint16x4_t vpx_highbd_dc_predictor_4x4_neon(uint16x4_t *a1, uint64_t a2, int16x4_t *a3, int16x4_t *a4)
{
  v4 = vadd_s16(*a4, *a3);
  v4.i16[0] = vaddv_s16(v4);
  result = vrshr_n_u16(vdup_lane_s16(v4, 0), 3uLL);
  *a1 = result;
  v6 = (a1 + 2 * a2);
  *v6 = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  *(v7 + 2 * a2) = result;
  return result;
}

uint16x4_t vpx_highbd_dc_left_predictor_4x4_neon(uint16x4_t *a1, uint64_t a2, uint64_t a3, int16x4_t *a4)
{
  v4 = *a4;
  v4.i16[0] = vaddv_s16(*a4);
  result = vrshr_n_u16(vdup_lane_s16(v4, 0), 2uLL);
  *a1 = result;
  v6 = (a1 + 2 * a2);
  *v6 = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  *(v7 + 2 * a2) = result;
  return result;
}

uint16x4_t vpx_highbd_dc_top_predictor_4x4_neon(uint16x4_t *a1, uint64_t a2, int16x4_t *a3)
{
  v3 = *a3;
  v3.i16[0] = vaddv_s16(*a3);
  result = vrshr_n_u16(vdup_lane_s16(v3, 0), 2uLL);
  *a1 = result;
  v5 = (a1 + 2 * a2);
  *v5 = result;
  v6 = (v5 + 2 * a2);
  *v6 = result;
  *(v6 + 2 * a2) = result;
  return result;
}

int16x4_t vpx_highbd_dc_128_predictor_4x4_neon(int16x4_t *a1, uint64_t a2, uint64_t a3, uint64_t a4, char a5)
{
  result = vdup_n_s16(1 << (a5 - 1));
  *a1 = result;
  v6 = (a1 + 2 * a2);
  *v6 = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  *(v7 + 2 * a2) = result;
  return result;
}

uint16x8_t vpx_highbd_dc_predictor_8x8_neon(uint16x8_t *a1, uint64_t a2, int16x8_t *a3, int16x8_t *a4)
{
  v4 = vaddq_s16(*a4, *a3);
  v4.i32[0] = vaddlvq_u16(v4);
  result = vrshrq_n_u16(vdupq_lane_s16(*v4.i8, 0), 4uLL);
  *a1 = result;
  v6 = (a1 + 2 * a2);
  *v6 = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  v8 = (v7 + 2 * a2);
  *v8 = result;
  v9 = (v8 + 2 * a2);
  *v9 = result;
  v10 = (v9 + 2 * a2);
  *v10 = result;
  v11 = (v10 + 2 * a2);
  *v11 = result;
  *(v11 + 2 * a2) = result;
  return result;
}

uint16x8_t vpx_highbd_dc_left_predictor_8x8_neon(uint16x8_t *a1, uint64_t a2, uint64_t a3, uint16x8_t *a4)
{
  v4 = *a4;
  v4.i32[0] = vaddlvq_u16(*a4);
  result = vrshrq_n_u16(vdupq_lane_s16(*v4.i8, 0), 3uLL);
  *a1 = result;
  v6 = (a1 + 2 * a2);
  *v6 = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  v8 = (v7 + 2 * a2);
  *v8 = result;
  v9 = (v8 + 2 * a2);
  *v9 = result;
  v10 = (v9 + 2 * a2);
  *v10 = result;
  v11 = (v10 + 2 * a2);
  *v11 = result;
  *(v11 + 2 * a2) = result;
  return result;
}

uint16x8_t vpx_highbd_dc_top_predictor_8x8_neon(uint16x8_t *a1, uint64_t a2, uint16x8_t *a3)
{
  v3 = *a3;
  v3.i32[0] = vaddlvq_u16(*a3);
  result = vrshrq_n_u16(vdupq_lane_s16(*v3.i8, 0), 3uLL);
  *a1 = result;
  v5 = (a1 + 2 * a2);
  *v5 = result;
  v6 = (v5 + 2 * a2);
  *v6 = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  v8 = (v7 + 2 * a2);
  *v8 = result;
  v9 = (v8 + 2 * a2);
  *v9 = result;
  v10 = (v9 + 2 * a2);
  *v10 = result;
  *(v10 + 2 * a2) = result;
  return result;
}

int16x8_t vpx_highbd_dc_128_predictor_8x8_neon(int16x8_t *a1, uint64_t a2, uint64_t a3, uint64_t a4, char a5)
{
  result = vdupq_n_s16(1 << (a5 - 1));
  *a1 = result;
  v6 = (a1 + 2 * a2);
  *v6 = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  v8 = (v7 + 2 * a2);
  *v8 = result;
  v9 = (v8 + 2 * a2);
  *v9 = result;
  v10 = (v9 + 2 * a2);
  *v10 = result;
  v11 = (v10 + 2 * a2);
  *v11 = result;
  *(v11 + 2 * a2) = result;
  return result;
}

int16x8_t vpx_highbd_dc_predictor_16x16_neon(int16x8_t *a1, uint64_t a2, int16x8_t *a3, int16x8_t *a4)
{
  v4 = vaddq_s16(vaddq_s16(a3[1], *a3), vaddq_s16(*a4, a4[1]));
  v4.i32[0] = vaddlvq_u16(v4);
  result = vdupq_lane_s16(vrshrn_n_s32(v4, 5uLL), 0);
  *a1 = result;
  a1[1] = result;
  v6 = (a1 + 2 * a2);
  *v6 = result;
  v6[1] = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  v7[1] = result;
  v8 = (v7 + 2 * a2);
  *v8 = result;
  v8[1] = result;
  v9 = (v8 + 2 * a2);
  *v9 = result;
  v9[1] = result;
  v10 = (v9 + 2 * a2);
  *v10 = result;
  v10[1] = result;
  v11 = (v10 + 2 * a2);
  *v11 = result;
  v11[1] = result;
  v12 = (v11 + 2 * a2);
  *v12 = result;
  v12[1] = result;
  v13 = (v12 + 2 * a2);
  *v13 = result;
  v13[1] = result;
  v14 = (v13 + 2 * a2);
  *v14 = result;
  v14[1] = result;
  v15 = (v14 + 2 * a2);
  *v15 = result;
  v15[1] = result;
  v16 = (v15 + 2 * a2);
  *v16 = result;
  v16[1] = result;
  v17 = (v16 + 2 * a2);
  *v17 = result;
  v17[1] = result;
  v18 = (v17 + 2 * a2);
  *v18 = result;
  v18[1] = result;
  v19 = (v18 + 2 * a2);
  *v19 = result;
  v19[1] = result;
  v20 = (v19 + 2 * a2);
  *v20 = result;
  v20[1] = result;
  return result;
}

uint16x8_t vpx_highbd_dc_left_predictor_16x16_neon(uint16x8_t *a1, uint64_t a2, uint64_t a3, int16x8_t *a4)
{
  v4 = vaddq_s16(a4[1], *a4);
  v4.i32[0] = vaddlvq_u16(v4);
  result = vrshrq_n_u16(vdupq_lane_s16(*v4.i8, 0), 4uLL);
  *a1 = result;
  a1[1] = result;
  v6 = (a1 + 2 * a2);
  *v6 = result;
  v6[1] = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  v7[1] = result;
  v8 = (v7 + 2 * a2);
  *v8 = result;
  v8[1] = result;
  v9 = (v8 + 2 * a2);
  *v9 = result;
  v9[1] = result;
  v10 = (v9 + 2 * a2);
  *v10 = result;
  v10[1] = result;
  v11 = (v10 + 2 * a2);
  *v11 = result;
  v11[1] = result;
  v12 = (v11 + 2 * a2);
  *v12 = result;
  v12[1] = result;
  v13 = (v12 + 2 * a2);
  *v13 = result;
  v13[1] = result;
  v14 = (v13 + 2 * a2);
  *v14 = result;
  v14[1] = result;
  v15 = (v14 + 2 * a2);
  *v15 = result;
  v15[1] = result;
  v16 = (v15 + 2 * a2);
  *v16 = result;
  v16[1] = result;
  v17 = (v16 + 2 * a2);
  *v17 = result;
  v17[1] = result;
  v18 = (v17 + 2 * a2);
  *v18 = result;
  v18[1] = result;
  v19 = (v18 + 2 * a2);
  *v19 = result;
  v19[1] = result;
  v20 = (v19 + 2 * a2);
  *v20 = result;
  v20[1] = result;
  return result;
}

uint16x8_t vpx_highbd_dc_top_predictor_16x16_neon(uint16x8_t *a1, uint64_t a2, int16x8_t *a3)
{
  v3 = vaddq_s16(a3[1], *a3);
  v3.i32[0] = vaddlvq_u16(v3);
  result = vrshrq_n_u16(vdupq_lane_s16(*v3.i8, 0), 4uLL);
  *a1 = result;
  a1[1] = result;
  v5 = (a1 + 2 * a2);
  *v5 = result;
  v5[1] = result;
  v6 = (v5 + 2 * a2);
  *v6 = result;
  v6[1] = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  v7[1] = result;
  v8 = (v7 + 2 * a2);
  *v8 = result;
  v8[1] = result;
  v9 = (v8 + 2 * a2);
  *v9 = result;
  v9[1] = result;
  v10 = (v9 + 2 * a2);
  *v10 = result;
  v10[1] = result;
  v11 = (v10 + 2 * a2);
  *v11 = result;
  v11[1] = result;
  v12 = (v11 + 2 * a2);
  *v12 = result;
  v12[1] = result;
  v13 = (v12 + 2 * a2);
  *v13 = result;
  v13[1] = result;
  v14 = (v13 + 2 * a2);
  *v14 = result;
  v14[1] = result;
  v15 = (v14 + 2 * a2);
  *v15 = result;
  v15[1] = result;
  v16 = (v15 + 2 * a2);
  *v16 = result;
  v16[1] = result;
  v17 = (v16 + 2 * a2);
  *v17 = result;
  v17[1] = result;
  v18 = (v17 + 2 * a2);
  *v18 = result;
  v18[1] = result;
  v19 = (v18 + 2 * a2);
  *v19 = result;
  v19[1] = result;
  return result;
}

int16x8_t vpx_highbd_dc_128_predictor_16x16_neon(int16x8_t *a1, uint64_t a2, uint64_t a3, uint64_t a4, char a5)
{
  result = vdupq_n_s16(1 << (a5 - 1));
  *a1 = result;
  a1[1] = result;
  v6 = (a1 + 2 * a2);
  *v6 = result;
  v6[1] = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  v7[1] = result;
  v8 = (v7 + 2 * a2);
  *v8 = result;
  v8[1] = result;
  v9 = (v8 + 2 * a2);
  *v9 = result;
  v9[1] = result;
  v10 = (v9 + 2 * a2);
  *v10 = result;
  v10[1] = result;
  v11 = (v10 + 2 * a2);
  *v11 = result;
  v11[1] = result;
  v12 = (v11 + 2 * a2);
  *v12 = result;
  v12[1] = result;
  v13 = (v12 + 2 * a2);
  *v13 = result;
  v13[1] = result;
  v14 = (v13 + 2 * a2);
  *v14 = result;
  v14[1] = result;
  v15 = (v14 + 2 * a2);
  *v15 = result;
  v15[1] = result;
  v16 = (v15 + 2 * a2);
  *v16 = result;
  v16[1] = result;
  v17 = (v16 + 2 * a2);
  *v17 = result;
  v17[1] = result;
  v18 = (v17 + 2 * a2);
  *v18 = result;
  v18[1] = result;
  v19 = (v18 + 2 * a2);
  *v19 = result;
  v19[1] = result;
  v20 = (v19 + 2 * a2);
  *v20 = result;
  v20[1] = result;
  return result;
}

__n128 *vpx_highbd_dc_predictor_32x32_neon(__n128 *a1, uint64_t a2, int16x8_t *a3, int16x8_t *a4)
{
  v4 = vaddq_s16(vaddq_s16(vaddq_s16(vaddq_s16(a3[1], *a3), vaddq_s16(a3[2], a3[3])), vaddq_s16(vaddq_s16(*a4, a4[1]), a4[2])), a4[3]);
  v4.i32[0] = vaddlvq_u16(v4);
  return dc_store_32x32(a1, a2, vdupq_lane_s16(vrshrn_n_s32(v4, 6uLL), 0));
}

__n128 *dc_store_32x32(__n128 *result, uint64_t a2, __n128 a3)
{
  *result = a3;
  result[1] = a3;
  result[2] = a3;
  result[3] = a3;
  v3 = (result + 2 * a2);
  *v3 = a3;
  v3[1] = a3;
  v3[2] = a3;
  v3[3] = a3;
  v4 = (v3 + 2 * a2);
  *v4 = a3;
  v4[1] = a3;
  v4[2] = a3;
  v4[3] = a3;
  v5 = (v4 + 2 * a2);
  *v5 = a3;
  v5[1] = a3;
  v5[2] = a3;
  v5[3] = a3;
  v6 = (v5 + 2 * a2);
  *v6 = a3;
  v6[1] = a3;
  v6[2] = a3;
  v6[3] = a3;
  v7 = (v6 + 2 * a2);
  *v7 = a3;
  v7[1] = a3;
  v7[2] = a3;
  v7[3] = a3;
  v8 = (v7 + 2 * a2);
  *v8 = a3;
  v8[1] = a3;
  v8[2] = a3;
  v8[3] = a3;
  v9 = (v8 + 2 * a2);
  *v9 = a3;
  v9[1] = a3;
  v9[2] = a3;
  v9[3] = a3;
  v10 = (v9 + 2 * a2);
  *v10 = a3;
  v10[1] = a3;
  v10[2] = a3;
  v10[3] = a3;
  v11 = (v10 + 2 * a2);
  *v11 = a3;
  v11[1] = a3;
  v11[2] = a3;
  v11[3] = a3;
  v12 = (v11 + 2 * a2);
  *v12 = a3;
  v12[1] = a3;
  v12[2] = a3;
  v12[3] = a3;
  v13 = (v12 + 2 * a2);
  *v13 = a3;
  v13[1] = a3;
  v13[2] = a3;
  v13[3] = a3;
  v14 = (v13 + 2 * a2);
  *v14 = a3;
  v14[1] = a3;
  v14[2] = a3;
  v14[3] = a3;
  v15 = (v14 + 2 * a2);
  *v15 = a3;
  v15[1] = a3;
  v15[2] = a3;
  v15[3] = a3;
  v16 = (v15 + 2 * a2);
  *v16 = a3;
  v16[1] = a3;
  v16[2] = a3;
  v16[3] = a3;
  v17 = (v16 + 2 * a2);
  *v17 = a3;
  v17[1] = a3;
  v17[2] = a3;
  v17[3] = a3;
  v18 = (v17 + 2 * a2);
  *v18 = a3;
  v18[1] = a3;
  v18[2] = a3;
  v18[3] = a3;
  v19 = (v18 + 2 * a2);
  *v19 = a3;
  v19[1] = a3;
  v19[2] = a3;
  v19[3] = a3;
  v20 = (v19 + 2 * a2);
  *v20 = a3;
  v20[1] = a3;
  v20[2] = a3;
  v20[3] = a3;
  v21 = (v20 + 2 * a2);
  *v21 = a3;
  v21[1] = a3;
  v21[2] = a3;
  v21[3] = a3;
  v22 = (v21 + 2 * a2);
  *v22 = a3;
  v22[1] = a3;
  v22[2] = a3;
  v22[3] = a3;
  v23 = (v22 + 2 * a2);
  *v23 = a3;
  v23[1] = a3;
  v23[2] = a3;
  v23[3] = a3;
  v24 = (v23 + 2 * a2);
  *v24 = a3;
  v24[1] = a3;
  v24[2] = a3;
  v24[3] = a3;
  v25 = (v24 + 2 * a2);
  *v25 = a3;
  v25[1] = a3;
  v25[2] = a3;
  v25[3] = a3;
  v26 = (v25 + 2 * a2);
  *v26 = a3;
  v26[1] = a3;
  v26[2] = a3;
  v26[3] = a3;
  v27 = (v26 + 2 * a2);
  *v27 = a3;
  v27[1] = a3;
  v27[2] = a3;
  v27[3] = a3;
  v28 = (v27 + 2 * a2);
  *v28 = a3;
  v28[1] = a3;
  v28[2] = a3;
  v28[3] = a3;
  v29 = (v28 + 2 * a2);
  *v29 = a3;
  v29[1] = a3;
  v29[2] = a3;
  v29[3] = a3;
  v30 = (v29 + 2 * a2);
  *v30 = a3;
  v30[1] = a3;
  v30[2] = a3;
  v30[3] = a3;
  v31 = (v30 + 2 * a2);
  *v31 = a3;
  v31[1] = a3;
  v31[2] = a3;
  v31[3] = a3;
  v32 = (v31 + 2 * a2);
  *v32 = a3;
  v32[1] = a3;
  v32[2] = a3;
  v32[3] = a3;
  v33 = (v32 + 2 * a2);
  *v33 = a3;
  v33[1] = a3;
  v33[2] = a3;
  v33[3] = a3;
  return result;
}

__n128 *vpx_highbd_dc_left_predictor_32x32_neon(__n128 *a1, uint64_t a2, uint64_t a3, int16x8_t *a4)
{
  v4 = vaddq_s16(vaddq_s16(a4[1], *a4), vaddq_s16(a4[2], a4[3]));
  v4.i32[0] = vaddlvq_u16(v4);
  return dc_store_32x32(a1, a2, vdupq_lane_s16(vrshrn_n_s32(v4, 5uLL), 0));
}

__n128 *vpx_highbd_dc_top_predictor_32x32_neon(__n128 *a1, uint64_t a2, int16x8_t *a3)
{
  v3 = vaddq_s16(vaddq_s16(a3[1], *a3), vaddq_s16(a3[2], a3[3]));
  v3.i32[0] = vaddlvq_u16(v3);
  return dc_store_32x32(a1, a2, vdupq_lane_s16(vrshrn_n_s32(v3, 5uLL), 0));
}

int8x16_t vpx_highbd_d45_predictor_4x4_neon(void *a1, uint64_t a2, uint16x8_t *a3)
{
  result = *a3;
  v4 = vrhaddq_u16(vhaddq_u16(*a3, vextq_s8(result, result, 4uLL)), vextq_s8(result, result, 2uLL));
  *a1 = v4.i64[0];
  *(a1 + 2 * a2) = vextq_s8(v4, v4, 2uLL).u64[0];
  *(a1 + 4 * a2) = vextq_s8(v4, v4, 4uLL).u64[0];
  v5 = a1 + 6 * a2;
  *v5 = vextq_s8(v4, v4, 6uLL).u64[0];
  *(v5 + 3) = result.i16[7];
  return result;
}

int8x16_t vpx_highbd_d45_predictor_8x8_neon(int8x16_t *a1, uint64_t a2, uint64_t a3)
{
  v3 = *(a3 + 2);
  v4 = vdupq_laneq_s16(v3, 6);
  v5 = vrhaddq_u16(vhaddq_u16(vextq_s8(*a3, *a3, 0xEuLL), v3), *a3);
  *a1 = vextq_s8(v5, v4, 2uLL);
  *(a1 + 2 * a2) = vextq_s8(v5, v4, 4uLL);
  *(a1 + 4 * a2) = vextq_s8(v5, v4, 6uLL);
  *(a1 + 6 * a2) = vextq_s8(v5, v4, 8uLL);
  *(a1 + 8 * a2) = vextq_s8(v5, v4, 0xAuLL);
  *(a1 + 10 * a2) = vextq_s8(v5, v4, 0xCuLL);
  result = vextq_s8(v5, v4, 0xEuLL);
  *(a1 + 12 * a2) = result;
  *(a1 + 14 * a2) = v4;
  return result;
}

int8x16_t vpx_highbd_d45_predictor_16x16_neon(int8x16_t *a1, uint64_t a2, uint64_t a3)
{
  v3 = *(a3 + 18);
  result = vdupq_laneq_s16(v3, 6);
  v5 = vrhaddq_u16(vhaddq_u16(vextq_s8(*a3, *a3, 0xEuLL), *(a3 + 2)), *a3);
  v6 = vrhaddq_u16(vhaddq_u16(*(a3 + 14), v3), *(a3 + 16));
  v7 = vextq_s8(v6, result, 2uLL);
  *a1 = vextq_s8(v5, v6, 2uLL);
  a1[1] = v7;
  v8 = (a1 + 2 * a2);
  v9 = vextq_s8(v6, result, 4uLL);
  *v8 = vextq_s8(v5, v6, 4uLL);
  v8[1] = v9;
  v10 = (a1 + 4 * a2);
  v11 = vextq_s8(v6, result, 6uLL);
  *v10 = vextq_s8(v5, v6, 6uLL);
  v10[1] = v11;
  v12 = (a1 + 6 * a2);
  v13 = vextq_s8(v6, result, 8uLL);
  *v12 = vextq_s8(v5, v6, 8uLL);
  v12[1] = v13;
  v14 = (a1 + 8 * a2);
  v15 = vextq_s8(v6, result, 0xAuLL);
  *v14 = vextq_s8(v5, v6, 0xAuLL);
  v14[1] = v15;
  v16 = (a1 + 10 * a2);
  v17 = vextq_s8(v6, result, 0xCuLL);
  *v16 = vextq_s8(v5, v6, 0xCuLL);
  v16[1] = v17;
  v18 = (a1 + 12 * a2);
  v19 = vextq_s8(v6, result, 0xEuLL);
  *v18 = vextq_s8(v5, v6, 0xEuLL);
  v18[1] = v19;
  v20 = (a1 + 14 * a2);
  *v20 = v6;
  v20[1] = result;
  v21 = &a1[a2];
  *v21 = v7;
  v21[1] = result;
  v22 = (a1 + 18 * a2);
  *v22 = v9;
  v22[1] = result;
  v23 = (a1 + 20 * a2);
  *v23 = v11;
  v23[1] = result;
  v24 = (a1 + 22 * a2);
  *v24 = v13;
  v24[1] = result;
  v25 = (a1 + 24 * a2);
  *v25 = v15;
  v25[1] = result;
  v26 = (a1 + 26 * a2);
  *v26 = v17;
  v26[1] = result;
  v27 = (a1 + 28 * a2);
  *v27 = v19;
  v27[1] = result;
  v28 = (a1 + 30 * a2);
  *v28 = result;
  v28[1] = result;
  return result;
}

int8x16_t vpx_highbd_d45_predictor_32x32_neon(int8x16_t *a1, uint64_t a2, uint64_t a3)
{
  v3 = *(a3 + 50);
  v4 = vdupq_laneq_s16(v3, 6);
  v5 = vrhaddq_u16(vhaddq_u16(*(a3 + 14), *(a3 + 18)), *(a3 + 16));
  v6 = vrhaddq_u16(vhaddq_u16(*(a3 + 30), *(a3 + 34)), *(a3 + 32));
  v7 = vrhaddq_u16(vhaddq_u16(*(a3 + 46), v3), *(a3 + 48));
  v8 = vextq_s8(vrhaddq_u16(vhaddq_u16(vextq_s8(*a3, *a3, 0xEuLL), *(a3 + 2)), *a3), v5, 2uLL);
  v9 = vextq_s8(v5, v6, 2uLL);
  v10 = vextq_s8(v6, v7, 2uLL);
  v11 = vextq_s8(v7, v4, 2uLL);
  *a1 = v8;
  a1[1] = v9;
  a1[2] = v10;
  a1[3] = v11;
  v12 = (a1 + 2 * a2);
  v13 = vextq_s8(v8, v9, 2uLL);
  v14 = vextq_s8(v9, v10, 2uLL);
  v15 = vextq_s8(v10, v11, 2uLL);
  v16 = vextq_s8(v11, v4, 2uLL);
  *v12 = v13;
  v12[1] = v14;
  v12[2] = v15;
  v12[3] = v16;
  v17 = (v12 + 2 * a2);
  v18 = vextq_s8(v13, v14, 2uLL);
  v19 = vextq_s8(v14, v15, 2uLL);
  v20 = vextq_s8(v15, v16, 2uLL);
  v21 = vextq_s8(v16, v4, 2uLL);
  *v17 = v18;
  v17[1] = v19;
  v17[2] = v20;
  v17[3] = v21;
  v22 = (v17 + 2 * a2);
  v23 = vextq_s8(v18, v19, 2uLL);
  v24 = vextq_s8(v19, v20, 2uLL);
  v25 = vextq_s8(v20, v21, 2uLL);
  v26 = vextq_s8(v21, v4, 2uLL);
  *v22 = v23;
  v22[1] = v24;
  v22[2] = v25;
  v22[3] = v26;
  v27 = (v22 + 2 * a2);
  v28 = vextq_s8(v23, v24, 2uLL);
  v29 = vextq_s8(v24, v25, 2uLL);
  v30 = vextq_s8(v25, v26, 2uLL);
  v31 = vextq_s8(v26, v4, 2uLL);
  *v27 = v28;
  v27[1] = v29;
  v27[2] = v30;
  v27[3] = v31;
  v32 = (v27 + 2 * a2);
  v33 = vextq_s8(v28, v29, 2uLL);
  v34 = vextq_s8(v29, v30, 2uLL);
  v35 = vextq_s8(v30, v31, 2uLL);
  v36 = vextq_s8(v31, v4, 2uLL);
  *v32 = v33;
  v32[1] = v34;
  v32[2] = v35;
  v32[3] = v36;
  v37 = (v32 + 2 * a2);
  v38 = vextq_s8(v33, v34, 2uLL);
  v39 = vextq_s8(v34, v35, 2uLL);
  v40 = vextq_s8(v35, v36, 2uLL);
  v41 = vextq_s8(v36, v4, 2uLL);
  *v37 = v38;
  v37[1] = v39;
  v37[2] = v40;
  v37[3] = v41;
  v42 = (v37 + 2 * a2);
  v43 = vextq_s8(v38, v39, 2uLL);
  v44 = vextq_s8(v39, v40, 2uLL);
  v45 = vextq_s8(v40, v41, 2uLL);
  v46 = vextq_s8(v41, v4, 2uLL);
  *v42 = v43;
  v42[1] = v44;
  v42[2] = v45;
  v42[3] = v46;
  v47 = (v42 + 2 * a2);
  v48 = vextq_s8(v43, v44, 2uLL);
  v49 = vextq_s8(v44, v45, 2uLL);
  v50 = vextq_s8(v45, v46, 2uLL);
  v51 = vextq_s8(v46, v4, 2uLL);
  *v47 = v48;
  v47[1] = v49;
  v47[2] = v50;
  v47[3] = v51;
  v52 = (v47 + 2 * a2);
  v53 = vextq_s8(v48, v49, 2uLL);
  v54 = vextq_s8(v49, v50, 2uLL);
  v55 = vextq_s8(v50, v51, 2uLL);
  v56 = vextq_s8(v51, v4, 2uLL);
  *v52 = v53;
  v52[1] = v54;
  v52[2] = v55;
  v52[3] = v56;
  v57 = (v52 + 2 * a2);
  v58 = vextq_s8(v53, v54, 2uLL);
  v59 = vextq_s8(v54, v55, 2uLL);
  v60 = vextq_s8(v55, v56, 2uLL);
  v61 = vextq_s8(v56, v4, 2uLL);
  *v57 = v58;
  v57[1] = v59;
  v57[2] = v60;
  v57[3] = v61;
  v62 = (v57 + 2 * a2);
  v63 = vextq_s8(v58, v59, 2uLL);
  v64 = vextq_s8(v59, v60, 2uLL);
  v65 = vextq_s8(v60, v61, 2uLL);
  v66 = vextq_s8(v61, v4, 2uLL);
  *v62 = v63;
  v62[1] = v64;
  v62[2] = v65;
  v62[3] = v66;
  v67 = (v62 + 2 * a2);
  v68 = vextq_s8(v63, v64, 2uLL);
  v69 = vextq_s8(v64, v65, 2uLL);
  v70 = vextq_s8(v65, v66, 2uLL);
  v71 = vextq_s8(v66, v4, 2uLL);
  *v67 = v68;
  v67[1] = v69;
  v67[2] = v70;
  v67[3] = v71;
  v72 = (v67 + 2 * a2);
  v73 = vextq_s8(v68, v69, 2uLL);
  v74 = vextq_s8(v69, v70, 2uLL);
  v75 = vextq_s8(v70, v71, 2uLL);
  v76 = vextq_s8(v71, v4, 2uLL);
  *v72 = v73;
  v72[1] = v74;
  v72[2] = v75;
  v72[3] = v76;
  v77 = (v72 + 2 * a2);
  v78 = vextq_s8(v73, v74, 2uLL);
  v79 = vextq_s8(v74, v75, 2uLL);
  v80 = vextq_s8(v75, v76, 2uLL);
  v81 = vextq_s8(v76, v4, 2uLL);
  *v77 = v78;
  v77[1] = v79;
  v77[2] = v80;
  v77[3] = v81;
  v82 = (v77 + 2 * a2);
  v83 = vextq_s8(v78, v79, 2uLL);
  v84 = vextq_s8(v79, v80, 2uLL);
  v85 = vextq_s8(v80, v81, 2uLL);
  v86 = vextq_s8(v81, v4, 2uLL);
  *v82 = v83;
  v82[1] = v84;
  v82[2] = v85;
  v82[3] = v86;
  v87 = (v82 + 2 * a2);
  v88 = vextq_s8(v83, v84, 2uLL);
  v89 = vextq_s8(v84, v85, 2uLL);
  v90 = vextq_s8(v85, v86, 2uLL);
  v91 = vextq_s8(v86, v4, 2uLL);
  *v87 = v88;
  v87[1] = v89;
  v87[2] = v90;
  v87[3] = v91;
  v92 = (v87 + 2 * a2);
  v93 = vextq_s8(v88, v89, 2uLL);
  v94 = vextq_s8(v89, v90, 2uLL);
  v95 = vextq_s8(v90, v91, 2uLL);
  v96 = vextq_s8(v91, v4, 2uLL);
  *v92 = v93;
  v92[1] = v94;
  v92[2] = v95;
  v92[3] = v96;
  v97 = (v92 + 2 * a2);
  v98 = vextq_s8(v93, v94, 2uLL);
  v99 = vextq_s8(v94, v95, 2uLL);
  v100 = vextq_s8(v95, v96, 2uLL);
  v101 = vextq_s8(v96, v4, 2uLL);
  *v97 = v98;
  v97[1] = v99;
  v97[2] = v100;
  v97[3] = v101;
  v102 = (v97 + 2 * a2);
  v103 = vextq_s8(v98, v99, 2uLL);
  v104 = vextq_s8(v99, v100, 2uLL);
  v105 = vextq_s8(v100, v101, 2uLL);
  v106 = vextq_s8(v101, v4, 2uLL);
  *v102 = v103;
  v102[1] = v104;
  v102[2] = v105;
  v102[3] = v106;
  v107 = (v102 + 2 * a2);
  v108 = vextq_s8(v103, v104, 2uLL);
  v109 = vextq_s8(v104, v105, 2uLL);
  v110 = vextq_s8(v105, v106, 2uLL);
  v111 = vextq_s8(v106, v4, 2uLL);
  *v107 = v108;
  v107[1] = v109;
  v107[2] = v110;
  v107[3] = v111;
  v112 = (v107 + 2 * a2);
  v113 = vextq_s8(v108, v109, 2uLL);
  v114 = vextq_s8(v109, v110, 2uLL);
  v115 = vextq_s8(v110, v111, 2uLL);
  v116 = vextq_s8(v111, v4, 2uLL);
  *v112 = v113;
  v112[1] = v114;
  v112[2] = v115;
  v112[3] = v116;
  v117 = (v112 + 2 * a2);
  v118 = vextq_s8(v113, v114, 2uLL);
  v119 = vextq_s8(v114, v115, 2uLL);
  v120 = vextq_s8(v115, v116, 2uLL);
  v121 = vextq_s8(v116, v4, 2uLL);
  *v117 = v118;
  v117[1] = v119;
  v117[2] = v120;
  v117[3] = v121;
  v122 = (v117 + 2 * a2);
  v123 = vextq_s8(v118, v119, 2uLL);
  v124 = vextq_s8(v119, v120, 2uLL);
  v125 = vextq_s8(v120, v121, 2uLL);
  v126 = vextq_s8(v121, v4, 2uLL);
  *v122 = v123;
  v122[1] = v124;
  v122[2] = v125;
  v122[3] = v126;
  v127 = (v122 + 2 * a2);
  v128 = vextq_s8(v123, v124, 2uLL);
  v129 = vextq_s8(v124, v125, 2uLL);
  v130 = vextq_s8(v125, v126, 2uLL);
  v131 = vextq_s8(v126, v4, 2uLL);
  *v127 = v128;
  v127[1] = v129;
  v127[2] = v130;
  v127[3] = v131;
  v132 = (v127 + 2 * a2);
  v133 = vextq_s8(v128, v129, 2uLL);
  v134 = vextq_s8(v129, v130, 2uLL);
  v135 = vextq_s8(v130, v131, 2uLL);
  v136 = vextq_s8(v131, v4, 2uLL);
  *v132 = v133;
  v132[1] = v134;
  v132[2] = v135;
  v132[3] = v136;
  v137 = (v132 + 2 * a2);
  v138 = vextq_s8(v133, v134, 2uLL);
  v139 = vextq_s8(v134, v135, 2uLL);
  v140 = vextq_s8(v135, v136, 2uLL);
  v141 = vextq_s8(v136, v4, 2uLL);
  *v137 = v138;
  v137[1] = v139;
  v137[2] = v140;
  v137[3] = v141;
  v142 = (v137 + 2 * a2);
  v143 = vextq_s8(v138, v139, 2uLL);
  v144 = vextq_s8(v139, v140, 2uLL);
  v145 = vextq_s8(v140, v141, 2uLL);
  v146 = vextq_s8(v141, v4, 2uLL);
  *v142 = v143;
  v142[1] = v144;
  v142[2] = v145;
  v142[3] = v146;
  v147 = (v142 + 2 * a2);
  v148 = vextq_s8(v143, v144, 2uLL);
  v149 = vextq_s8(v144, v145, 2uLL);
  v150 = vextq_s8(v145, v146, 2uLL);
  v151 = vextq_s8(v146, v4, 2uLL);
  *v147 = v148;
  v147[1] = v149;
  v147[2] = v150;
  v147[3] = v151;
  v152 = (v147 + 2 * a2);
  v153 = vextq_s8(v148, v149, 2uLL);
  v154 = vextq_s8(v149, v150, 2uLL);
  v155 = vextq_s8(v150, v151, 2uLL);
  v156 = vextq_s8(v151, v4, 2uLL);
  *v152 = v153;
  v152[1] = v154;
  v152[2] = v155;
  v152[3] = v156;
  v157 = (v152 + 2 * a2);
  v158 = vextq_s8(v153, v154, 2uLL);
  v159 = vextq_s8(v154, v155, 2uLL);
  v160 = vextq_s8(v155, v156, 2uLL);
  v161 = vextq_s8(v156, v4, 2uLL);
  *v157 = v158;
  v157[1] = v159;
  v157[2] = v160;
  v157[3] = v161;
  v162 = (v157 + 2 * a2);
  result = vextq_s8(v161, v4, 2uLL);
  *v162 = vextq_s8(v158, v159, 2uLL);
  v162[1] = vextq_s8(v159, v160, 2uLL);
  v162[2] = vextq_s8(v160, v161, 2uLL);
  v162[3] = result;
  return result;
}

uint16x4_t vpx_highbd_d63_predictor_4x4_neon(uint16x4_t *a1, uint64_t a2, uint64_t a3)
{
  v3 = *(a3 + 2);
  v4 = *(a3 + 4);
  v5 = vrhadd_u16(*a3, v3);
  result = vrhadd_u16(vhadd_u16(*a3, v4), v3);
  v7 = vrhadd_u16(v3, v4);
  v8 = vrhadd_u16(vhadd_u16(v3, *(a3 + 6)), v4);
  *a1 = v5;
  *(a1 + 2 * a2) = result;
  *(a1 + 4 * a2) = v7;
  *(a1 + 6 * a2) = v8;
  return result;
}

int8x16_t vpx_highbd_d63_predictor_8x8_neon(int8x16_t *a1, uint64_t a2, uint64_t a3)
{
  v3 = *(a3 + 2);
  v4 = *(a3 + 4);
  v5 = vdupq_laneq_s16(v4, 5);
  v6 = vrhaddq_u16(*a3, v3);
  v7 = vrhaddq_u16(vhaddq_u16(*a3, v4), v3);
  v8 = vextq_s8(v6, v6, 0xEuLL);
  v9 = vextq_s8(v7, v7, 0xEuLL);
  *a1 = v6;
  *(a1 + 2 * a2) = v7;
  *(a1 + 4 * a2) = vextq_s8(v8, v5, 4uLL);
  *(a1 + 6 * a2) = vextq_s8(v9, v5, 4uLL);
  *(a1 + 8 * a2) = vextq_s8(v8, v5, 6uLL);
  *(a1 + 10 * a2) = vextq_s8(v9, v5, 6uLL);
  *(a1 + 12 * a2) = vextq_s8(v8, v5, 8uLL);
  result = vextq_s8(v9, v5, 8uLL);
  *(a1 + 14 * a2) = result;
  return result;
}

int8x16_t vpx_highbd_d63_predictor_16x16_neon(int8x16_t *a1, uint64_t a2, uint64_t a3)
{
  v3 = *(a3 + 16);
  v4 = *(a3 + 2);
  v5 = *(a3 + 18);
  v6 = *(a3 + 20);
  result = vdupq_laneq_s16(v6, 5);
  v8 = vrhaddq_u16(*a3, v4);
  v9 = vrhaddq_u16(v3, v5);
  v10 = vrhaddq_u16(vhaddq_u16(*a3, *(a3 + 4)), v4);
  v11 = vrhaddq_u16(vhaddq_u16(v3, v6), v5);
  v12 = vextq_s8(v9, v9, 0xEuLL);
  v13 = vextq_s8(v11, v11, 0xEuLL);
  *a1 = v8;
  a1[1] = v9;
  v14 = (a1 + 2 * a2);
  *v14 = v10;
  v14[1] = v11;
  v15 = (a1 + 4 * a2);
  *v15 = vextq_s8(v8, v9, 2uLL);
  v15[1] = vextq_s8(v12, result, 4uLL);
  v16 = (a1 + 6 * a2);
  *v16 = vextq_s8(v10, v11, 2uLL);
  v16[1] = vextq_s8(v13, result, 4uLL);
  v17 = (a1 + 8 * a2);
  *v17 = vextq_s8(v8, v9, 4uLL);
  v17[1] = vextq_s8(v12, result, 6uLL);
  v18 = (a1 + 10 * a2);
  *v18 = vextq_s8(v10, v11, 4uLL);
  v18[1] = vextq_s8(v13, result, 6uLL);
  v19 = (a1 + 12 * a2);
  *v19 = vextq_s8(v8, v9, 6uLL);
  v19[1] = vextq_s8(v12, result, 8uLL);
  v20 = (a1 + 14 * a2);
  *v20 = vextq_s8(v10, v11, 6uLL);
  v20[1] = vextq_s8(v13, result, 8uLL);
  v21 = &a1[a2];
  *v21 = vextq_s8(v8, v9, 8uLL);
  v21[1] = vextq_s8(v12, result, 0xAuLL);
  v22 = (a1 + 18 * a2);
  *v22 = vextq_s8(v10, v11, 8uLL);
  v22[1] = vextq_s8(v13, result, 0xAuLL);
  v23 = (a1 + 20 * a2);
  *v23 = vextq_s8(v8, v9, 0xAuLL);
  v23[1] = vextq_s8(v12, result, 0xCuLL);
  v24 = (a1 + 22 * a2);
  *v24 = vextq_s8(v10, v11, 0xAuLL);
  v24[1] = vextq_s8(v13, result, 0xCuLL);
  v25 = (a1 + 24 * a2);
  *v25 = vextq_s8(v8, v9, 0xCuLL);
  v25[1] = vextq_s8(v12, result, 0xEuLL);
  v26 = (a1 + 26 * a2);
  *v26 = vextq_s8(v10, v11, 0xCuLL);
  v26[1] = vextq_s8(v13, result, 0xEuLL);
  v27 = (a1 + 28 * a2);
  *v27 = vextq_s8(v8, v9, 0xEuLL);
  v27[1] = result;
  v28 = (a1 + 30 * a2);
  *v28 = vextq_s8(v10, v11, 0xEuLL);
  v28[1] = result;
  return result;
}

int8x16_t vpx_highbd_d63_predictor_32x32_neon(int8x16_t *a1, uint64_t a2, uint64_t a3)
{
  v3 = *(a3 + 16);
  v4 = *(a3 + 2);
  v5 = *(a3 + 18);
  v6 = *(a3 + 32);
  v7 = *(a3 + 48);
  v8 = *(a3 + 34);
  v9 = *(a3 + 50);
  v10 = *(a3 + 52);
  result = vdupq_laneq_s16(v10, 5);
  v12 = vrhaddq_u16(*a3, v4);
  v13 = vrhaddq_u16(v3, v5);
  v14 = vrhaddq_u16(v6, v8);
  v15 = vrhaddq_u16(v7, v9);
  v16 = vrhaddq_u16(vhaddq_u16(*a3, *(a3 + 4)), v4);
  v17 = vrhaddq_u16(vhaddq_u16(v3, *(a3 + 20)), v5);
  v18 = vrhaddq_u16(vhaddq_u16(v6, *(a3 + 36)), v8);
  v19 = vrhaddq_u16(vhaddq_u16(v7, v10), v9);
  v20 = vextq_s8(v15, v15, 0xEuLL);
  v21 = vextq_s8(v19, v19, 0xEuLL);
  *a1 = v12;
  a1[1] = v13;
  a1[2] = v14;
  a1[3] = v15;
  v22 = (a1 + 2 * a2);
  *v22 = v16;
  v22[1] = v17;
  v22[2] = v18;
  v22[3] = v19;
  v23 = (a1 + 4 * a2);
  v94 = vextq_s8(v13, v14, 2uLL);
  *v23 = vextq_s8(v12, v13, 2uLL);
  v23[1] = v94;
  v92 = vextq_s8(v20, result, 4uLL);
  v93 = vextq_s8(v14, v15, 2uLL);
  v23[2] = v93;
  v23[3] = v92;
  v24 = (a1 + 6 * a2);
  v91 = vextq_s8(v17, v18, 2uLL);
  *v24 = vextq_s8(v16, v17, 2uLL);
  v24[1] = v91;
  v89 = vextq_s8(v21, result, 4uLL);
  v90 = vextq_s8(v18, v19, 2uLL);
  v24[2] = v90;
  v24[3] = v89;
  v25 = (a1 + 8 * a2);
  v88 = vextq_s8(v13, v14, 4uLL);
  *v25 = vextq_s8(v12, v13, 4uLL);
  v25[1] = v88;
  v86 = vextq_s8(v20, result, 6uLL);
  v87 = vextq_s8(v14, v15, 4uLL);
  v25[2] = v87;
  v25[3] = v86;
  v26 = (a1 + 10 * a2);
  v85 = vextq_s8(v17, v18, 4uLL);
  *v26 = vextq_s8(v16, v17, 4uLL);
  v26[1] = v85;
  v83 = vextq_s8(v21, result, 6uLL);
  v84 = vextq_s8(v18, v19, 4uLL);
  v26[2] = v84;
  v26[3] = v83;
  v27 = (a1 + 12 * a2);
  v82 = vextq_s8(v13, v14, 6uLL);
  *v27 = vextq_s8(v12, v13, 6uLL);
  v27[1] = v82;
  v80 = vextq_s8(v20, result, 8uLL);
  v81 = vextq_s8(v14, v15, 6uLL);
  v27[2] = v81;
  v27[3] = v80;
  v28 = (a1 + 14 * a2);
  v79 = vextq_s8(v17, v18, 6uLL);
  *v28 = vextq_s8(v16, v17, 6uLL);
  v28[1] = v79;
  v29 = vextq_s8(v18, v19, 6uLL);
  v30 = vextq_s8(v21, result, 8uLL);
  v28[2] = v29;
  v28[3] = v30;
  v31 = &a1[a2];
  v32 = vextq_s8(v13, v14, 8uLL);
  *v31 = vextq_s8(v12, v13, 8uLL);
  v31[1] = v32;
  v33 = vextq_s8(v14, v15, 8uLL);
  v34 = vextq_s8(v20, result, 0xAuLL);
  v31[2] = v33;
  v31[3] = v34;
  v35 = (a1 + 18 * a2);
  v36 = vextq_s8(v17, v18, 8uLL);
  *v35 = vextq_s8(v16, v17, 8uLL);
  v35[1] = v36;
  v37 = vextq_s8(v18, v19, 8uLL);
  v38 = vextq_s8(v21, result, 0xAuLL);
  v35[2] = v37;
  v35[3] = v38;
  v39 = (a1 + 20 * a2);
  v40 = vextq_s8(v13, v14, 0xAuLL);
  *v39 = vextq_s8(v12, v13, 0xAuLL);
  v39[1] = v40;
  v41 = vextq_s8(v14, v15, 0xAuLL);
  v42 = vextq_s8(v20, result, 0xCuLL);
  v39[2] = v41;
  v39[3] = v42;
  v43 = (a1 + 22 * a2);
  v44 = vextq_s8(v17, v18, 0xAuLL);
  *v43 = vextq_s8(v16, v17, 0xAuLL);
  v43[1] = v44;
  v45 = vextq_s8(v18, v19, 0xAuLL);
  v46 = vextq_s8(v21, result, 0xCuLL);
  v43[2] = v45;
  v43[3] = v46;
  v47 = (a1 + 24 * a2);
  v48 = vextq_s8(v13, v14, 0xCuLL);
  *v47 = vextq_s8(v12, v13, 0xCuLL);
  v47[1] = v48;
  v49 = vextq_s8(v14, v15, 0xCuLL);
  v50 = vextq_s8(v20, result, 0xEuLL);
  v47[2] = v49;
  v47[3] = v50;
  v51 = (a1 + 26 * a2);
  v52 = vextq_s8(v17, v18, 0xCuLL);
  *v51 = vextq_s8(v16, v17, 0xCuLL);
  v51[1] = v52;
  v53 = vextq_s8(v18, v19, 0xCuLL);
  v54 = vextq_s8(v21, result, 0xEuLL);
  v51[2] = v53;
  v51[3] = v54;
  v55 = vextq_s8(v12, v13, 0xEuLL);
  v56 = (a1 + 28 * a2);
  v57 = vextq_s8(v13, v14, 0xEuLL);
  *v56 = v55;
  v56[1] = v57;
  v58 = vextq_s8(v14, v15, 0xEuLL);
  v56[2] = v58;
  v56[3] = result;
  v59 = vextq_s8(v16, v17, 0xEuLL);
  v60 = (a1 + 30 * a2);
  v61 = vextq_s8(v17, v18, 0xEuLL);
  *v60 = v59;
  v60[1] = v61;
  v62 = vextq_s8(v18, v19, 0xEuLL);
  v60[2] = v62;
  v60[3] = result;
  v63 = &a1[2 * a2];
  *v63 = v13;
  v63[1] = v14;
  v63[2] = vextq_s8(v20, result, 2uLL);
  v63[3] = result;
  v64 = (a1 + 34 * a2);
  *v64 = v17;
  v64[1] = v18;
  v64[2] = vextq_s8(v21, result, 2uLL);
  v64[3] = result;
  v65 = (a1 + 36 * a2);
  *v65 = v94;
  v65[1] = v93;
  v65[2] = v92;
  v65[3] = result;
  v66 = (a1 + 38 * a2);
  *v66 = v91;
  v66[1] = v90;
  v66[2] = v89;
  v66[3] = result;
  v67 = (a1 + 40 * a2);
  *v67 = v88;
  v67[1] = v87;
  v67[2] = v86;
  v67[3] = result;
  v68 = (a1 + 42 * a2);
  *v68 = v85;
  v68[1] = v84;
  v68[2] = v83;
  v68[3] = result;
  v69 = (a1 + 44 * a2);
  *v69 = v82;
  v69[1] = v81;
  v69[2] = v80;
  v69[3] = result;
  v70 = (a1 + 46 * a2);
  *v70 = v79;
  v70[1] = v29;
  v70[2] = v30;
  v70[3] = result;
  v71 = &a1[3 * a2];
  *v71 = v32;
  v71[1] = v33;
  v71[2] = v34;
  v71[3] = result;
  v72 = (a1 + 50 * a2);
  *v72 = v36;
  v72[1] = v37;
  v72[2] = v38;
  v72[3] = result;
  v73 = (a1 + 52 * a2);
  *v73 = v40;
  v73[1] = v41;
  v73[2] = v42;
  v73[3] = result;
  v74 = (a1 + 54 * a2);
  *v74 = v44;
  v74[1] = v45;
  v74[2] = v46;
  v74[3] = result;
  v75 = (a1 + 56 * a2);
  *v75 = v48;
  v75[1] = v49;
  v75[2] = v50;
  v75[3] = result;
  v76 = (a1 + 58 * a2);
  *v76 = v52;
  v76[1] = v53;
  v76[2] = v54;
  v76[3] = result;
  v77 = (a1 + 60 * a2);
  *v77 = v57;
  v77[1] = v58;
  v77[2] = result;
  v77[3] = result;
  v78 = (a1 + 62 * a2);
  *v78 = v61;
  v78[1] = v62;
  v78[2] = result;
  v78[3] = result;
  return result;
}

int8x8_t vpx_highbd_d117_predictor_4x4_neon(int8x8_t *a1, uint64_t a2, uint16x4_t *a3, int8x8_t *a4)
{
  v4 = *(&a3[-1] + 6);
  v5 = vld1_dup_s16(a4);
  v6 = vext_s8(vdup_lane_s16(v4, 0), *a4, 6uLL);
  v7 = vrhadd_u16(v4, *a3);
  v8 = vrhadd_u16(vhadd_u16(vext_s8(v5, v4, 6uLL), *a3), v4);
  v9 = vrhadd_u16(vhadd_u16(v6, vext_s8(*a4, *a4, 2uLL)), *a4);
  *a1 = v7;
  *(a1 + 2 * a2) = v8;
  *(a1 + 4 * a2) = vext_s8(vdup_lane_s16(v9, 0), v7, 6uLL);
  result = vext_s8(vdup_lane_s16(v9, 1), v8, 6uLL);
  *(a1 + 6 * a2) = result;
  return result;
}

int8x16_t vpx_highbd_d117_predictor_8x8_neon(int8x16_t *a1, uint64_t a2, uint16x8_t *a3, int8x16_t *a4)
{
  v4 = *(&a3[-1] + 14);
  v5 = vld1q_dup_s16(a4->i16);
  v6 = vextq_s8(vdupq_lane_s16(*v4.i8, 0), *a4, 0xEuLL);
  v7 = vrhaddq_u16(v4, *a3);
  v8 = vrhaddq_u16(vhaddq_u16(vextq_s8(v5, v4, 0xEuLL), *a3), v4);
  v9 = vrhaddq_u16(vhaddq_u16(v6, vextq_s8(*a4, *a4, 2uLL)), *a4);
  v10 = vrev64q_s16(vextq_s8(v9, v9, 8uLL));
  v11 = vuzp1q_s16(v10, v10);
  v12 = vuzp2q_s16(v10, v10);
  *a1 = v7;
  *(a1 + 2 * a2) = v8;
  *(a1 + 4 * a2) = vextq_s8(v10, v7, 0xEuLL);
  *(a1 + 6 * a2) = vextq_s8(v11, v8, 0xEuLL);
  *(a1 + 8 * a2) = vextq_s8(v12, v7, 0xCuLL);
  *(a1 + 10 * a2) = vextq_s8(v11, v8, 0xCuLL);
  *(a1 + 12 * a2) = vextq_s8(v12, v7, 0xAuLL);
  result = vextq_s8(v11, v8, 0xAuLL);
  *(a1 + 14 * a2) = result;
  return result;
}

int8x16_t vpx_highbd_d117_predictor_16x16_neon(int8x16_t *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v4 = *(a3 - 2);
  v5 = *(a3 + 16);
  v6 = *(a3 + 14);
  v7 = a4;
  v8 = vld1q_dup_s16(v7->i16);
  v7 = (v7 + 2);
  v9 = vextq_s8(v8, v4, 0xEuLL);
  v10 = vextq_s8(vdupq_lane_s16(*v4.i8, 0), *a4, 0xEuLL);
  v11 = vrhaddq_u16(v4, *a3);
  v12 = vrhaddq_u16(v6, v5);
  v13 = vrhaddq_u16(vhaddq_u16(v9, *a3), v4);
  v14 = vrhaddq_u16(vhaddq_u16(*(a3 + 12), v5), v6);
  v15 = vrhaddq_u16(vhaddq_u16(v10, *v7), *a4);
  v16 = vrhaddq_u16(vhaddq_u16(*(a4 + 14), vextq_s8(*(a4 + 16), *(a4 + 16), 2uLL)), *(a4 + 16));
  v17 = vrev64q_s16(vextq_s8(v15, v15, 8uLL));
  v18 = vrev64q_s16(vextq_s8(v16, v16, 8uLL));
  *a1 = v11;
  a1[1] = v12;
  v19 = vuzp1q_s16(v18, v17);
  v20 = vuzp2q_s16(v18, v17);
  v21 = (a1 + 2 * a2);
  *v21 = v13;
  v21[1] = v14;
  v22 = (a1 + 4 * a2);
  *v22 = vextq_s8(v20, v11, 0xEuLL);
  v22[1] = vextq_s8(v11, v12, 0xEuLL);
  v23 = (a1 + 6 * a2);
  *v23 = vextq_s8(v19, v13, 0xEuLL);
  v23[1] = vextq_s8(v13, v14, 0xEuLL);
  v24 = (a1 + 8 * a2);
  *v24 = vextq_s8(v20, v11, 0xCuLL);
  v24[1] = vextq_s8(v11, v12, 0xCuLL);
  v25 = (a1 + 10 * a2);
  *v25 = vextq_s8(v19, v13, 0xCuLL);
  v25[1] = vextq_s8(v13, v14, 0xCuLL);
  v26 = (a1 + 12 * a2);
  *v26 = vextq_s8(v20, v11, 0xAuLL);
  v26[1] = vextq_s8(v11, v12, 0xAuLL);
  v27 = (a1 + 14 * a2);
  *v27 = vextq_s8(v19, v13, 0xAuLL);
  v27[1] = vextq_s8(v13, v14, 0xAuLL);
  v28 = &a1[a2];
  *v28 = vextq_s8(v20, v11, 8uLL);
  v28[1] = vextq_s8(v11, v12, 8uLL);
  v29 = (a1 + 18 * a2);
  *v29 = vextq_s8(v19, v13, 8uLL);
  v29[1] = vextq_s8(v13, v14, 8uLL);
  v30 = (a1 + 20 * a2);
  *v30 = vextq_s8(v20, v11, 6uLL);
  v30[1] = vextq_s8(v11, v12, 6uLL);
  v31 = (a1 + 22 * a2);
  *v31 = vextq_s8(v19, v13, 6uLL);
  v31[1] = vextq_s8(v13, v14, 6uLL);
  v32 = (a1 + 24 * a2);
  *v32 = vextq_s8(v20, v11, 4uLL);
  v32[1] = vextq_s8(v11, v12, 4uLL);
  v33 = (a1 + 26 * a2);
  *v33 = vextq_s8(v19, v13, 4uLL);
  v33[1] = vextq_s8(v13, v14, 4uLL);
  v34 = (a1 + 28 * a2);
  *v34 = vextq_s8(v20, v11, 2uLL);
  v34[1] = vextq_s8(v11, v12, 2uLL);
  v35 = vextq_s8(v19, v13, 2uLL);
  v36 = (a1 + 30 * a2);
  result = vextq_s8(v13, v14, 2uLL);
  *v36 = v35;
  v36[1] = result;
  return result;
}

int8x16_t vpx_highbd_d117_predictor_32x32_neon(int8x16_t *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v4 = *(a3 - 2);
  v5 = *(a3 + 16);
  v6 = *(a3 + 14);
  v7 = *(a3 + 30);
  v8 = *(a3 + 32);
  v9 = *(a3 + 48);
  v10 = *(a3 + 46);
  v11 = a4;
  v12 = vld1q_dup_s16(v11->i16);
  v11 = (v11 + 2);
  v13 = vrhaddq_u16(v4, *a3);
  v14 = vrhaddq_u16(v6, v5);
  v15 = vrhaddq_u16(v7, v8);
  v16 = vrhaddq_u16(v10, v9);
  v17 = vrhaddq_u16(vhaddq_u16(vextq_s8(v12, v4, 0xEuLL), *a3), v4);
  v18 = vrhaddq_u16(vhaddq_u16(*(a3 + 12), v5), v6);
  v19 = vrhaddq_u16(vhaddq_u16(*(a3 + 28), v8), v7);
  v20 = vrhaddq_u16(vhaddq_u16(*(a3 + 44), v9), v10);
  v21 = vrhaddq_u16(vhaddq_u16(vextq_s8(vdupq_lane_s16(*v4.i8, 0), *a4, 0xEuLL), *v11), *a4);
  v22 = vrhaddq_u16(vhaddq_u16(*(a4 + 14), *(a4 + 18)), *(a4 + 16));
  v23 = vrhaddq_u16(vhaddq_u16(*(a4 + 30), *(a4 + 34)), *(a4 + 32));
  v24 = vrhaddq_u16(vhaddq_u16(*(a4 + 46), vextq_s8(*(a4 + 48), *(a4 + 48), 2uLL)), *(a4 + 48));
  v25 = vrev64q_s16(vextq_s8(v21, v21, 8uLL));
  v26 = vrev64q_s16(vextq_s8(v22, v22, 8uLL));
  v27 = vrev64q_s16(vextq_s8(v23, v23, 8uLL));
  v28 = vrev64q_s16(vextq_s8(v24, v24, 8uLL));
  v29 = vuzp1q_s16(v26, v25);
  v30 = vuzp2q_s16(v26, v25);
  v31 = vuzp1q_s16(v28, v27);
  v32 = vuzp2q_s16(v28, v27);
  *a1 = v13;
  a1[1] = v14;
  a1[2] = v15;
  a1[3] = v16;
  v33 = (a1 + 2 * a2);
  *v33 = v17;
  v33[1] = v18;
  v33[2] = v19;
  v33[3] = v20;
  v34 = (a1 + 4 * a2);
  v111 = vextq_s8(v13, v14, 0xEuLL);
  v112 = vextq_s8(v30, v13, 0xEuLL);
  *v34 = v112;
  v34[1] = v111;
  v35 = vextq_s8(v14, v15, 0xEuLL);
  v34[2] = v35;
  v34[3] = vextq_s8(v15, v16, 0xEuLL);
  v109 = vextq_s8(v29, v17, 0xEuLL);
  v110 = v35;
  v36 = (a1 + 6 * a2);
  v108 = vextq_s8(v17, v18, 0xEuLL);
  *v36 = v109;
  v36[1] = v108;
  v105 = vextq_s8(v18, v19, 0xEuLL);
  v36[2] = v105;
  v36[3] = vextq_s8(v19, v20, 0xEuLL);
  v37 = (a1 + 8 * a2);
  v106 = vextq_s8(v13, v14, 0xCuLL);
  v107 = vextq_s8(v30, v13, 0xCuLL);
  *v37 = v107;
  v37[1] = v106;
  v38 = vextq_s8(v14, v15, 0xCuLL);
  v37[2] = v38;
  v37[3] = vextq_s8(v15, v16, 0xCuLL);
  v103 = vextq_s8(v29, v17, 0xCuLL);
  v104 = v38;
  v39 = (a1 + 10 * a2);
  v102 = vextq_s8(v17, v18, 0xCuLL);
  *v39 = v103;
  v39[1] = v102;
  v40 = vextq_s8(v18, v19, 0xCuLL);
  v39[2] = v40;
  v39[3] = vextq_s8(v19, v20, 0xCuLL);
  v100 = vextq_s8(v30, v13, 0xAuLL);
  v101 = v40;
  v41 = (a1 + 12 * a2);
  v99 = vextq_s8(v13, v14, 0xAuLL);
  *v41 = v100;
  v41[1] = v99;
  v42 = vextq_s8(v14, v15, 0xAuLL);
  v41[2] = v42;
  v41[3] = vextq_s8(v15, v16, 0xAuLL);
  v97 = vextq_s8(v29, v17, 0xAuLL);
  v98 = v42;
  v43 = (a1 + 14 * a2);
  v96 = vextq_s8(v17, v18, 0xAuLL);
  *v43 = v97;
  v43[1] = v96;
  v95 = vextq_s8(v18, v19, 0xAuLL);
  v43[2] = v95;
  v43[3] = vextq_s8(v19, v20, 0xAuLL);
  v44 = &a1[a2];
  v45 = vextq_s8(v14, v15, 8uLL);
  v44[2] = v45;
  v44[3] = vextq_s8(v15, v16, 8uLL);
  v46 = vextq_s8(v30, v13, 8uLL);
  v93 = vextq_s8(v13, v14, 8uLL);
  v94 = v45;
  *v44 = v46;
  v44[1] = v93;
  v47 = (a1 + 18 * a2);
  v48 = vextq_s8(v18, v19, 8uLL);
  v47[2] = v48;
  v47[3] = vextq_s8(v19, v20, 8uLL);
  v49 = vextq_s8(v29, v17, 8uLL);
  v50 = vextq_s8(v17, v18, 8uLL);
  *v47 = v49;
  v47[1] = v50;
  v51 = (a1 + 20 * a2);
  v52 = vextq_s8(v14, v15, 6uLL);
  v51[2] = v52;
  v51[3] = vextq_s8(v15, v16, 6uLL);
  v53 = vextq_s8(v30, v13, 6uLL);
  v54 = vextq_s8(v13, v14, 6uLL);
  *v51 = v53;
  v51[1] = v54;
  v55 = (a1 + 22 * a2);
  v56 = vextq_s8(v18, v19, 6uLL);
  v55[2] = v56;
  v55[3] = vextq_s8(v19, v20, 6uLL);
  v57 = vextq_s8(v29, v17, 6uLL);
  v58 = vextq_s8(v17, v18, 6uLL);
  *v55 = v57;
  v55[1] = v58;
  v59 = (a1 + 24 * a2);
  v60 = vextq_s8(v14, v15, 4uLL);
  v59[2] = v60;
  v59[3] = vextq_s8(v15, v16, 4uLL);
  v61 = vextq_s8(v30, v13, 4uLL);
  v62 = vextq_s8(v13, v14, 4uLL);
  *v59 = v61;
  v59[1] = v62;
  v63 = (a1 + 26 * a2);
  v64 = vextq_s8(v18, v19, 4uLL);
  v63[2] = v64;
  v63[3] = vextq_s8(v19, v20, 4uLL);
  v65 = vextq_s8(v29, v17, 4uLL);
  v66 = vextq_s8(v17, v18, 4uLL);
  *v63 = v65;
  v63[1] = v66;
  v67 = (a1 + 28 * a2);
  v68 = vextq_s8(v15, v16, 2uLL);
  v69 = vextq_s8(v14, v15, 2uLL);
  v67[2] = v69;
  v67[3] = v68;
  v70 = vextq_s8(v30, v13, 2uLL);
  v71 = vextq_s8(v13, v14, 2uLL);
  *v67 = v70;
  v67[1] = v71;
  v72 = (a1 + 30 * a2);
  v73 = vextq_s8(v18, v19, 2uLL);
  v72[2] = v73;
  v72[3] = vextq_s8(v19, v20, 2uLL);
  v74 = vextq_s8(v29, v17, 2uLL);
  v75 = vextq_s8(v17, v18, 2uLL);
  *v72 = v74;
  v72[1] = v75;
  v76 = &a1[2 * a2];
  v76[1] = v13;
  v76[2] = v14;
  v76[3] = v15;
  *v76 = v30;
  v77 = (a1 + 34 * a2);
  v77[1] = v17;
  v77[2] = v18;
  v77[3] = v19;
  *v77 = v29;
  v78 = (a1 + 36 * a2);
  *v78 = vextq_s8(v32, v30, 0xEuLL);
  v78[1] = v112;
  v78[2] = v111;
  v78[3] = v110;
  v79 = (a1 + 38 * a2);
  *v79 = vextq_s8(v31, v29, 0xEuLL);
  v79[1] = v109;
  v79[2] = v108;
  v79[3] = v105;
  v80 = (a1 + 40 * a2);
  *v80 = vextq_s8(v32, v30, 0xCuLL);
  v80[1] = v107;
  v80[2] = v106;
  v80[3] = v104;
  v81 = (a1 + 42 * a2);
  *v81 = vextq_s8(v31, v29, 0xCuLL);
  v81[1] = v103;
  v81[2] = v102;
  v81[3] = v101;
  v82 = (a1 + 44 * a2);
  *v82 = vextq_s8(v32, v30, 0xAuLL);
  v82[1] = v100;
  v82[2] = v99;
  v82[3] = v98;
  v83 = (a1 + 46 * a2);
  *v83 = vextq_s8(v31, v29, 0xAuLL);
  v83[1] = v97;
  v83[2] = v96;
  v83[3] = v95;
  v84 = &a1[3 * a2];
  *v84 = vextq_s8(v32, v30, 8uLL);
  v84[1] = v46;
  v84[2] = v93;
  v84[3] = v94;
  v85 = (a1 + 50 * a2);
  *v85 = vextq_s8(v31, v29, 8uLL);
  v85[1] = v49;
  v85[2] = v50;
  v85[3] = v48;
  v86 = (a1 + 52 * a2);
  *v86 = vextq_s8(v32, v30, 6uLL);
  v86[1] = v53;
  v86[2] = v54;
  v86[3] = v52;
  v87 = (a1 + 54 * a2);
  *v87 = vextq_s8(v31, v29, 6uLL);
  v87[1] = v57;
  v87[2] = v58;
  v87[3] = v56;
  v88 = (a1 + 56 * a2);
  *v88 = vextq_s8(v32, v30, 4uLL);
  v88[1] = v61;
  v88[2] = v62;
  v88[3] = v60;
  v89 = (a1 + 58 * a2);
  *v89 = vextq_s8(v31, v29, 4uLL);
  v89[1] = v65;
  v89[2] = v66;
  v89[3] = v64;
  v90 = (a1 + 60 * a2);
  *v90 = vextq_s8(v32, v30, 2uLL);
  v90[1] = v70;
  v90[2] = v71;
  v90[3] = v69;
  result = vextq_s8(v31, v29, 2uLL);
  v92 = (a1 + 62 * a2);
  *v92 = result;
  v92[1] = v74;
  v92[2] = v75;
  v92[3] = v73;
  return result;
}

int8x8_t vpx_highbd_d153_predictor_4x4_neon(int8x8_t *a1, uint64_t a2, uint16x4_t *a3, int8x8_t *a4)
{
  v4 = *(&a3[-1] + 6);
  v5 = vld1_dup_s16(a4);
  v6 = vext_s8(vdup_lane_s16(v4, 0), *a4, 6uLL);
  v7 = vrhadd_u16(vhadd_u16(vext_s8(v5, v4, 6uLL), *a3), v4);
  v8 = vrev64_s16(vrhadd_u16(vhadd_u16(v6, vext_s8(*a4, *a4, 2uLL)), *a4));
  v9 = vrev64_s16(vrhadd_u16(v6, *a4));
  v10 = vzip1_s16(v8, v9);
  v11 = vzip2_s16(v8, v9);
  *a1 = vext_s8(v11, v7, 6uLL);
  *(a1 + 2 * a2) = vext_s8(v11, v7, 2uLL);
  *(a1 + 4 * a2) = vext_s8(v10, v11, 6uLL);
  result = vext_s8(v10, v11, 2uLL);
  *(a1 + 6 * a2) = result;
  return result;
}

int8x16_t vpx_highbd_d153_predictor_8x8_neon(int8x16_t *a1, uint64_t a2, uint16x8_t *a3, int8x16_t *a4)
{
  v4 = *(&a3[-1] + 14);
  v5 = vld1q_dup_s16(a4->i16);
  v6 = vextq_s8(vdupq_lane_s16(*v4.i8, 0), *a4, 0xEuLL);
  v7 = vrhaddq_u16(v6, *a4);
  v8 = vrhaddq_u16(vhaddq_u16(vextq_s8(v5, v4, 0xEuLL), *a3), v4);
  v9 = vrhaddq_u16(vhaddq_u16(v6, vextq_s8(*a4, *a4, 2uLL)), *a4);
  v10 = vrev64q_s16(vextq_s8(v7, v7, 8uLL));
  v11 = vrev64q_s16(vextq_s8(v9, v9, 8uLL));
  v12 = vzip1q_s16(v11, v10);
  v13 = vzip2q_s16(v11, v10);
  *a1 = vextq_s8(v13, v8, 0xEuLL);
  *(a1 + 2 * a2) = vextq_s8(v13, v8, 0xAuLL);
  *(a1 + 4 * a2) = vextq_s8(v13, v8, 6uLL);
  *(a1 + 6 * a2) = vextq_s8(v13, v8, 2uLL);
  *(a1 + 8 * a2) = vextq_s8(v12, v13, 0xEuLL);
  *(a1 + 10 * a2) = vextq_s8(v12, v13, 0xAuLL);
  *(a1 + 12 * a2) = vextq_s8(v12, v13, 6uLL);
  result = vextq_s8(v12, v13, 2uLL);
  *(a1 + 14 * a2) = result;
  return result;
}

int8x16_t vpx_highbd_d153_predictor_16x16_neon(int8x16_t *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v4 = *(a3 - 2);
  v5 = a4;
  v6 = vld1q_dup_s16(v5->i16);
  v5 = (v5 + 2);
  v7 = *(a4 + 16);
  v8 = *(a4 + 14);
  v9 = vextq_s8(vdupq_lane_s16(*v4.i8, 0), *a4, 0xEuLL);
  v10 = vrhaddq_u16(v9, *a4);
  v11 = vrhaddq_u16(vhaddq_u16(vextq_s8(v6, v4, 0xEuLL), *a3), v4);
  v12 = vrhaddq_u16(vhaddq_u16(*(a3 + 12), *(a3 + 16)), *(a3 + 14));
  v13 = vrhaddq_u16(vhaddq_u16(v9, *v5), *a4);
  v14 = vrev64q_s16(vextq_s8(v10, v10, 8uLL));
  v15 = vrev64q_s16(vextq_s8(v13, v13, 8uLL));
  v16 = vzip2q_s16(v15, v14);
  v17 = vextq_s8(v16, v11, 0xEuLL);
  *a1 = v17;
  a1[1] = vextq_s8(v11, v12, 0xEuLL);
  v18 = vrhaddq_u16(v8, v7);
  v19 = vextq_s8(v16, v11, 0xAuLL);
  v20 = (a1 + 2 * a2);
  *v20 = v19;
  v20[1] = vextq_s8(v11, v12, 0xAuLL);
  v21 = vextq_s8(v16, v11, 6uLL);
  v22 = (a1 + 4 * a2);
  *v22 = v21;
  v22[1] = vextq_s8(v11, v12, 6uLL);
  v23 = vextq_s8(v16, v11, 2uLL);
  v24 = (a1 + 6 * a2);
  *v24 = v23;
  v24[1] = vextq_s8(v11, v12, 2uLL);
  v25 = vrhaddq_u16(vhaddq_u16(v8, vextq_s8(v7, v7, 2uLL)), v7);
  v26 = vzip1q_s16(v15, v14);
  v27 = vextq_s8(v26, v16, 0xEuLL);
  v28 = (a1 + 8 * a2);
  *v28 = v27;
  v28[1] = v17;
  v29 = vextq_s8(v26, v16, 0xAuLL);
  v30 = (a1 + 10 * a2);
  *v30 = v29;
  v30[1] = v19;
  v31 = vrev64q_s16(vextq_s8(v18, v18, 8uLL));
  v32 = vextq_s8(v26, v16, 6uLL);
  v33 = (a1 + 12 * a2);
  *v33 = v32;
  v33[1] = v21;
  v34 = vextq_s8(v26, v16, 2uLL);
  v35 = (a1 + 14 * a2);
  *v35 = v34;
  v35[1] = v23;
  v36 = vrev64q_s16(vextq_s8(v25, v25, 8uLL));
  v37 = vzip2q_s16(v36, v31);
  v38 = vextq_s8(v37, v26, 0xEuLL);
  v39 = &a1[a2];
  *v39 = v38;
  v39[1] = v27;
  v40 = vzip1q_s16(v36, v31);
  v41 = vextq_s8(v37, v26, 0xAuLL);
  v42 = (a1 + 18 * a2);
  *v42 = v41;
  v42[1] = v29;
  v43 = vextq_s8(v37, v26, 6uLL);
  v44 = (a1 + 20 * a2);
  *v44 = v43;
  v44[1] = v32;
  v45 = vextq_s8(v37, v26, 2uLL);
  v46 = (a1 + 22 * a2);
  *v46 = v45;
  v46[1] = v34;
  v47 = (a1 + 24 * a2);
  *v47 = vextq_s8(v40, v37, 0xEuLL);
  v47[1] = v38;
  v48 = (a1 + 26 * a2);
  *v48 = vextq_s8(v40, v37, 0xAuLL);
  v48[1] = v41;
  v49 = (a1 + 28 * a2);
  *v49 = vextq_s8(v40, v37, 6uLL);
  v49[1] = v43;
  result = vextq_s8(v40, v37, 2uLL);
  v51 = (a1 + 30 * a2);
  *v51 = result;
  v51[1] = v45;
  return result;
}

int8x16_t vpx_highbd_d153_predictor_32x32_neon(int8x16_t *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v4 = *(a3 - 2);
  v5 = a4;
  v6 = vld1q_dup_s16(v5->i16);
  v5 = (v5 + 2);
  v7 = *(a4 + 16);
  v8 = *(a4 + 14);
  v9 = *(a4 + 30);
  v10 = *(a4 + 32);
  v11 = *(a4 + 48);
  v12 = *(a4 + 46);
  v13 = vextq_s8(vdupq_lane_s16(*v4.i8, 0), *a4, 0xEuLL);
  v14 = vrhaddq_u16(v13, *a4);
  v15 = vrhaddq_u16(v8, v7);
  v16 = vrhaddq_u16(v9, v10);
  v17 = vrhaddq_u16(v12, v11);
  v18 = vrhaddq_u16(vhaddq_u16(vextq_s8(v6, v4, 0xEuLL), *a3), v4);
  v19 = vrhaddq_u16(vhaddq_u16(*(a3 + 12), *(a3 + 16)), *(a3 + 14));
  v20 = vrhaddq_u16(vhaddq_u16(*(a3 + 28), *(a3 + 32)), *(a3 + 30));
  v21 = vrhaddq_u16(vhaddq_u16(*(a3 + 44), *(a3 + 48)), *(a3 + 46));
  v22 = vrhaddq_u16(vhaddq_u16(v13, *v5), *a4);
  v23 = vrhaddq_u16(vhaddq_u16(v8, *(a4 + 18)), v7);
  v24 = vrhaddq_u16(vhaddq_u16(v9, *(a4 + 34)), v10);
  v25 = vrhaddq_u16(vhaddq_u16(v12, vextq_s8(v11, v11, 2uLL)), v11);
  v26 = vrev64q_s16(vextq_s8(v14, v14, 8uLL));
  v27 = vrev64q_s16(vextq_s8(v15, v15, 8uLL));
  v28 = vrev64q_s16(vextq_s8(v16, v16, 8uLL));
  v29 = vrev64q_s16(vextq_s8(v22, v22, 8uLL));
  v30 = vzip2q_s16(v29, v26);
  v31 = vextq_s8(v30, v18, 0xEuLL);
  v32 = vextq_s8(v18, v19, 0xEuLL);
  *a1 = v31;
  a1[1] = v32;
  v33 = vrev64q_s16(vextq_s8(v17, v17, 8uLL));
  v34 = vextq_s8(v19, v20, 0xEuLL);
  a1[2] = v34;
  a1[3] = vextq_s8(v20, v21, 0xEuLL);
  v35 = vextq_s8(v23, v23, 8uLL);
  v36 = vextq_s8(v30, v18, 0xAuLL);
  v37 = (a1 + 2 * a2);
  v38 = vextq_s8(v18, v19, 0xAuLL);
  *v37 = v36;
  v37[1] = v38;
  v39 = vrev64q_s16(v35);
  v40 = vextq_s8(v19, v20, 0xAuLL);
  v37[2] = v40;
  v37[3] = vextq_s8(v20, v21, 0xAuLL);
  v41 = vextq_s8(v24, v24, 8uLL);
  v42 = vextq_s8(v30, v18, 6uLL);
  v43 = (a1 + 4 * a2);
  v44 = vextq_s8(v18, v19, 6uLL);
  *v43 = v42;
  v43[1] = v44;
  v45 = vrev64q_s16(v41);
  v46 = vextq_s8(v19, v20, 6uLL);
  v43[2] = v46;
  v43[3] = vextq_s8(v20, v21, 6uLL);
  v47 = vextq_s8(v25, v25, 8uLL);
  v48 = vextq_s8(v30, v18, 2uLL);
  v49 = (a1 + 6 * a2);
  v50 = vextq_s8(v18, v19, 2uLL);
  *v49 = v48;
  v49[1] = v50;
  v51 = vrev64q_s16(v47);
  v52 = vextq_s8(v19, v20, 2uLL);
  v49[2] = v52;
  v49[3] = vextq_s8(v20, v21, 2uLL);
  v53 = vzip1q_s16(v51, v33);
  v54 = vzip1q_s16(v29, v26);
  v55 = vextq_s8(v54, v30, 0xEuLL);
  v56 = (a1 + 8 * a2);
  *v56 = v55;
  v56[1] = v31;
  v57 = vzip2q_s16(v51, v33);
  v58 = vzip1q_s16(v45, v28);
  v59 = vzip2q_s16(v45, v28);
  v56[2] = v32;
  v56[3] = v34;
  v60 = vzip1q_s16(v39, v27);
  v61 = vzip2q_s16(v39, v27);
  v62 = vextq_s8(v54, v30, 0xAuLL);
  v63 = (a1 + 10 * a2);
  *v63 = v62;
  v63[1] = v36;
  v63[2] = v38;
  v63[3] = v40;
  v64 = vextq_s8(v54, v30, 6uLL);
  v65 = (a1 + 12 * a2);
  *v65 = v64;
  v65[1] = v42;
  v65[2] = v44;
  v65[3] = v46;
  v66 = vextq_s8(v54, v30, 2uLL);
  v67 = (a1 + 14 * a2);
  *v67 = v66;
  v67[1] = v48;
  v67[2] = v50;
  v67[3] = v52;
  v68 = vextq_s8(v61, v54, 0xEuLL);
  v69 = &a1[a2];
  *v69 = v68;
  v69[1] = v55;
  v69[2] = v31;
  v69[3] = v32;
  v70 = vextq_s8(v61, v54, 0xAuLL);
  v71 = (a1 + 18 * a2);
  *v71 = v70;
  v71[1] = v62;
  v71[2] = v36;
  v71[3] = v38;
  v72 = vextq_s8(v61, v54, 6uLL);
  v73 = (a1 + 20 * a2);
  *v73 = v72;
  v73[1] = v64;
  v73[2] = v42;
  v73[3] = v44;
  v74 = vextq_s8(v61, v54, 2uLL);
  v75 = (a1 + 22 * a2);
  *v75 = v74;
  v75[1] = v66;
  v75[2] = v48;
  v75[3] = v50;
  v76 = vextq_s8(v60, v61, 0xEuLL);
  v77 = (a1 + 24 * a2);
  *v77 = v76;
  v77[1] = v68;
  v77[2] = v55;
  v77[3] = v31;
  v78 = vextq_s8(v60, v61, 0xAuLL);
  v79 = (a1 + 26 * a2);
  *v79 = v78;
  v79[1] = v70;
  v79[2] = v62;
  v79[3] = v36;
  v80 = vextq_s8(v60, v61, 6uLL);
  v81 = (a1 + 28 * a2);
  *v81 = v80;
  v81[1] = v72;
  v81[2] = v64;
  v81[3] = v42;
  v82 = vextq_s8(v60, v61, 2uLL);
  v83 = (a1 + 30 * a2);
  *v83 = v82;
  v83[1] = v74;
  v83[2] = v66;
  v83[3] = v48;
  v84 = vextq_s8(v59, v60, 0xEuLL);
  v85 = &a1[2 * a2];
  *v85 = v84;
  v85[1] = v76;
  v85[2] = v68;
  v85[3] = v55;
  v86 = vextq_s8(v59, v60, 0xAuLL);
  v87 = (a1 + 34 * a2);
  *v87 = v86;
  v87[1] = v78;
  v87[2] = v70;
  v87[3] = v62;
  v88 = vextq_s8(v59, v60, 6uLL);
  v89 = (a1 + 36 * a2);
  *v89 = v88;
  v89[1] = v80;
  v89[2] = v72;
  v89[3] = v64;
  v90 = vextq_s8(v59, v60, 2uLL);
  v91 = (a1 + 38 * a2);
  *v91 = v90;
  v91[1] = v82;
  v91[2] = v74;
  v91[3] = v66;
  v92 = vextq_s8(v58, v59, 0xEuLL);
  v93 = (a1 + 40 * a2);
  *v93 = v92;
  v93[1] = v84;
  v93[2] = v76;
  v93[3] = v68;
  v94 = vextq_s8(v58, v59, 0xAuLL);
  v95 = (a1 + 42 * a2);
  *v95 = v94;
  v95[1] = v86;
  v95[2] = v78;
  v95[3] = v70;
  v96 = vextq_s8(v58, v59, 6uLL);
  v97 = (a1 + 44 * a2);
  *v97 = v96;
  v97[1] = v88;
  v97[2] = v80;
  v97[3] = v72;
  v98 = vextq_s8(v58, v59, 2uLL);
  v99 = (a1 + 46 * a2);
  *v99 = v98;
  v99[1] = v90;
  v99[2] = v82;
  v99[3] = v74;
  v100 = vextq_s8(v57, v58, 0xEuLL);
  v101 = &a1[3 * a2];
  *v101 = v100;
  v101[1] = v92;
  v101[2] = v84;
  v101[3] = v76;
  v102 = vextq_s8(v57, v58, 0xAuLL);
  v103 = (a1 + 50 * a2);
  *v103 = v102;
  v103[1] = v94;
  v103[2] = v86;
  v103[3] = v78;
  v104 = vextq_s8(v57, v58, 6uLL);
  v105 = (a1 + 52 * a2);
  *v105 = v104;
  v105[1] = v96;
  v105[2] = v88;
  v105[3] = v80;
  v106 = vextq_s8(v57, v58, 2uLL);
  v107 = (a1 + 54 * a2);
  *v107 = v106;
  v107[1] = v98;
  v107[2] = v90;
  v107[3] = v82;
  v108 = (a1 + 56 * a2);
  *v108 = vextq_s8(v53, v57, 0xEuLL);
  v108[1] = v100;
  v108[2] = v92;
  v108[3] = v84;
  v109 = (a1 + 58 * a2);
  *v109 = vextq_s8(v53, v57, 0xAuLL);
  v109[1] = v102;
  v109[2] = v94;
  v109[3] = v86;
  v110 = (a1 + 60 * a2);
  *v110 = vextq_s8(v53, v57, 6uLL);
  v110[1] = v104;
  v110[2] = v96;
  v110[3] = v88;
  result = vextq_s8(v53, v57, 2uLL);
  v112 = (a1 + 62 * a2);
  *v112 = result;
  v112[1] = v106;
  v112[2] = v98;
  v112[3] = v90;
  return result;
}

int8x16_t vpx_highbd_d135_predictor_4x4_neon(void *a1, uint64_t a2, uint64_t a3, int16x4_t *a4)
{
  *v4.i8 = *a4;
  *v5.i8 = vrev64_s16(*a4);
  v4.i64[1] = v5.i64[0];
  v5.i64[1] = *(a3 - 2);
  result = vrhaddq_u16(vhaddq_u16(v5, vextq_s8(v4, *(a3 - 2), 0xCuLL)), vextq_s8(v4, *(a3 - 2), 0xAuLL));
  *a1 = vextq_s8(result, result, 6uLL).u64[0];
  v7 = (a1 + 2 * a2);
  *v7 = vextq_s8(result, result, 4uLL).u64[0];
  v8 = (v7 + 2 * a2);
  *v8 = vextq_s8(result, result, 2uLL).u64[0];
  *(v8 + 2 * a2) = result.i64[0];
  return result;
}

int8x16_t vpx_highbd_d135_predictor_8x8_neon(int8x16_t *a1, uint64_t a2, uint64_t a3, int8x16_t *a4)
{
  v4 = *(a3 - 2);
  *v5.i8 = vrev64_s16(*&vextq_s8(*a4, *a4, 8uLL));
  v5.u64[1] = vrev64_s16(*a4->i8);
  v6 = vrhaddq_u16(vhaddq_u16(v5, vextq_s8(v5, v4, 4uLL)), vextq_s8(v5, v4, 2uLL));
  v7 = vrhaddq_u16(vhaddq_u16(v4, *(a3 + 2)), *a3);
  v8 = vextq_s8(v6, v7, 0xEuLL);
  v9 = vextq_s8(v6, v7, 0xCuLL);
  v10 = vextq_s8(v6, v7, 0xAuLL);
  v11 = vextq_s8(v6, v7, 8uLL);
  v12 = vextq_s8(v6, v7, 6uLL);
  v13 = vextq_s8(v6, v7, 4uLL);
  result = vextq_s8(v6, v7, 2uLL);
  *a1 = v8;
  v15 = (a1 + 2 * a2);
  *v15 = v9;
  v16 = (v15 + 2 * a2);
  *v16 = v10;
  v17 = (v16 + 2 * a2);
  *v17 = v11;
  v18 = (v17 + 2 * a2);
  *v18 = v12;
  v19 = (v18 + 2 * a2);
  *v19 = v13;
  v20 = (v19 + 2 * a2);
  *v20 = result;
  *(v20 + 2 * a2) = v6;
  return result;
}

int8x16_t *vpx_highbd_d135_predictor_16x16_neon(int8x16_t *a1, uint64_t a2, uint64_t a3, int8x16_t *a4)
{
  v4 = a4[1];
  *v5.i8 = vrev64_s16(*&vextq_s8(*a4, *a4, 8uLL));
  v6 = vrev64_s16(*v4.i8);
  *v4.i8 = vrev64_s16(*&vextq_s8(v4, v4, 8uLL));
  v5.u64[1] = vrev64_s16(*a4->i8);
  v4.u64[1] = v6;
  v7 = vrhaddq_u16(vhaddq_u16(v4, vextq_s8(v4, v5, 4uLL)), vextq_s8(v4, v5, 2uLL));
  v8 = *(a3 - 2);
  v9 = vrhaddq_u16(vhaddq_u16(v5, vextq_s8(v5, v8, 4uLL)), vextq_s8(v5, v8, 2uLL));
  v10 = vrhaddq_u16(vhaddq_u16(v8, *(a3 + 2)), *a3);
  v11 = vrhaddq_u16(vhaddq_u16(*(a3 + 14), *(a3 + 18)), *(a3 + 16));
  v12 = vextq_s8(v9, v10, 0xEuLL);
  v13 = vextq_s8(v9, v10, 0xCuLL);
  v14 = vextq_s8(v9, v10, 0xAuLL);
  v15 = vextq_s8(v9, v10, 8uLL);
  v16 = vextq_s8(v9, v10, 6uLL);
  v17 = vextq_s8(v9, v10, 4uLL);
  v18 = vextq_s8(v9, v10, 2uLL);
  *a1 = v12;
  a1[1] = vextq_s8(v10, v11, 0xEuLL);
  result = a1 + 1;
  v20 = (result + 2 * a2);
  v20[-1] = v13;
  *v20 = vextq_s8(v10, v11, 0xCuLL);
  v21 = (v20 + 2 * a2);
  v21[-1] = v14;
  *v21 = vextq_s8(v10, v11, 0xAuLL);
  v22 = (v21 + 2 * a2);
  v22[-1] = v15;
  *v22 = vextq_s8(v10, v11, 8uLL);
  v23 = (v22 + 2 * a2);
  v23[-1] = v16;
  *v23 = vextq_s8(v10, v11, 6uLL);
  v24 = (v23 + 2 * a2);
  v24[-1] = v17;
  *v24 = vextq_s8(v10, v11, 4uLL);
  v25 = (v24 + 2 * a2);
  v25[-1] = v18;
  *v25 = vextq_s8(v10, v11, 2uLL);
  v26 = (v25 + 2 * a2);
  v26[-1] = v9;
  *v26 = v10;
  v27 = (v26 + 2 * a2);
  v27[-1] = vextq_s8(v7, v9, 0xEuLL);
  *v27 = v12;
  v28 = (v27 + 2 * a2);
  v28[-1] = vextq_s8(v7, v9, 0xCuLL);
  *v28 = v13;
  v29 = (v28 + 2 * a2);
  v29[-1] = vextq_s8(v7, v9, 0xAuLL);
  *v29 = v14;
  v30 = (v29 + 2 * a2);
  v30[-1] = vextq_s8(v7, v9, 8uLL);
  *v30 = v15;
  v31 = (v30 + 2 * a2);
  v31[-1] = vextq_s8(v7, v9, 6uLL);
  *v31 = v16;
  v32 = (v31 + 2 * a2);
  v32[-1] = vextq_s8(v7, v9, 4uLL);
  *v32 = v17;
  v33 = (v32 + 2 * a2);
  v33[-1] = vextq_s8(v7, v9, 2uLL);
  *v33 = v18;
  v34 = (v33 + 2 * a2);
  v34[-1] = v7;
  *v34 = v9;
  return result;
}

int8x16_t vpx_highbd_d135_predictor_32x32_neon(uint64_t a1, uint64_t a2, uint64_t a3, int8x16_t *a4)
{
  v4 = a4[2];
  v5 = a4[3];
  v6 = vrev64_s16(*v4.i8);
  *v4.i8 = vrev64_s16(*&vextq_s8(v4, v4, 8uLL));
  v4.u64[1] = v6;
  v7 = vrev64_s16(*v5.i8);
  *v5.i8 = vrev64_s16(*&vextq_s8(v5, v5, 8uLL));
  v5.u64[1] = v7;
  v8 = a4[1];
  *v9.i8 = vrev64_s16(*&vextq_s8(*a4, *a4, 8uLL));
  v9.u64[1] = vrev64_s16(*a4->i8);
  *v10.i8 = vrev64_s16(*&vextq_s8(v8, v8, 8uLL));
  v10.u64[1] = vrev64_s16(*v8.i8);
  v11 = *(a3 - 2);
  v12 = *(a3 + 14);
  v13 = *(a3 + 16);
  v14 = vrhaddq_u16(vhaddq_u16(v5, vextq_s8(v5, v4, 4uLL)), vextq_s8(v5, v4, 2uLL));
  v15 = *(a3 + 18);
  v16 = *(a3 + 30);
  v17 = vrhaddq_u16(vhaddq_u16(v4, vextq_s8(v4, v10, 4uLL)), vextq_s8(v4, v10, 2uLL));
  v18 = *(a3 + 34);
  v19 = vrhaddq_u16(vhaddq_u16(v10, vextq_s8(v10, v9, 4uLL)), vextq_s8(v10, v9, 2uLL));
  v20 = vrhaddq_u16(vhaddq_u16(v9, vextq_s8(v9, v11, 4uLL)), vextq_s8(v9, v11, 2uLL));
  v21 = *(a3 + 46);
  v22 = vhaddq_u16(v11, *(a3 + 2));
  v23 = *(a3 + 32);
  v24 = *(a3 + 48);
  v25 = vrhaddq_u16(v22, *a3);
  v26 = *(a3 + 50);
  v27 = (a1 + 62 * a2);
  *v27 = v14;
  v27[1] = v17;
  v27[2] = v19;
  v28 = vrhaddq_u16(vhaddq_u16(v21, v26), v24);
  v27[3] = v20;
  v29 = 2 * (-24 - a2);
  v30 = (v27 + v29 + 48);
  v31 = vextq_s8(v14, v17, 2uLL);
  v32 = vextq_s8(v17, v19, 2uLL);
  v33 = vextq_s8(v19, v20, 2uLL);
  *v30 = v31;
  v30[1] = v32;
  v34 = vextq_s8(v20, v25, 2uLL);
  v30[2] = v33;
  v30[3] = v34;
  v35 = (v30 + v29 + 48);
  v36 = vextq_s8(v31, v32, 2uLL);
  v37 = vextq_s8(v32, v33, 2uLL);
  v38 = vextq_s8(v33, v34, 2uLL);
  v39 = vextq_s8(v34, vdupq_lane_s16(*v25.i8, 1), 2uLL);
  *v35 = v36;
  v35[1] = v37;
  v35[2] = v38;
  v35[3] = v39;
  v40 = (v35 + v29 + 48);
  v41 = vextq_s8(v36, v37, 2uLL);
  v42 = vextq_s8(v37, v38, 2uLL);
  v43 = vextq_s8(v38, v39, 2uLL);
  v44 = vextq_s8(v39, vdupq_lane_s16(*v25.i8, 2), 2uLL);
  *v40 = v41;
  v40[1] = v42;
  v40[2] = v43;
  v40[3] = v44;
  v45 = (v40 + v29 + 48);
  v46 = vextq_s8(v41, v42, 2uLL);
  v47 = vextq_s8(v42, v43, 2uLL);
  v48 = vextq_s8(v43, v44, 2uLL);
  v49 = vextq_s8(v44, vdupq_lane_s16(*v25.i8, 3), 2uLL);
  *v45 = v46;
  v45[1] = v47;
  v45[2] = v48;
  v45[3] = v49;
  v50 = (v45 + v29 + 48);
  v51 = vextq_s8(v46, v47, 2uLL);
  v52 = vextq_s8(v47, v48, 2uLL);
  v53 = vextq_s8(v48, v49, 2uLL);
  v54 = vextq_s8(v49, vdupq_laneq_s16(v25, 4), 2uLL);
  *v50 = v51;
  v50[1] = v52;
  v50[2] = v53;
  v50[3] = v54;
  v55 = (v50 + v29 + 48);
  v56 = vextq_s8(v51, v52, 2uLL);
  v57 = vextq_s8(v52, v53, 2uLL);
  v58 = vextq_s8(v53, v54, 2uLL);
  v59 = vextq_s8(v54, vdupq_laneq_s16(v25, 5), 2uLL);
  *v55 = v56;
  v55[1] = v57;
  v55[2] = v58;
  v55[3] = v59;
  v60 = (v55 + v29 + 48);
  v61 = vextq_s8(v56, v57, 2uLL);
  v62 = vextq_s8(v57, v58, 2uLL);
  v63 = vextq_s8(v58, v59, 2uLL);
  v64 = vextq_s8(v59, vdupq_laneq_s16(v25, 6), 2uLL);
  *v60 = v61;
  v60[1] = v62;
  v60[2] = v63;
  v60[3] = v64;
  v65 = vextq_s8(v64, vdupq_laneq_s16(v25, 7), 2uLL);
  v66 = vextq_s8(v63, v64, 2uLL);
  v67 = vextq_s8(v62, v63, 2uLL);
  v68 = vextq_s8(v61, v62, 2uLL);
  v69 = vhaddq_u16(v16, v18);
  v70 = vrhaddq_u16(vhaddq_u16(v12, v15), v13);
  v71 = (v60 + v29 + 48);
  *v71 = v68;
  v71[1] = v67;
  v71[2] = v66;
  v71[3] = v65;
  v72 = (v71 + v29 + 48);
  v73 = vextq_s8(v68, v67, 2uLL);
  v74 = vextq_s8(v67, v66, 2uLL);
  v75 = vextq_s8(v66, v65, 2uLL);
  v76 = vextq_s8(v65, v70, 2uLL);
  *v72 = v73;
  v72[1] = v74;
  v72[2] = v75;
  v72[3] = v76;
  v77 = (v72 + v29 + 48);
  v78 = vextq_s8(v73, v74, 2uLL);
  v79 = vextq_s8(v74, v75, 2uLL);
  v80 = vextq_s8(v75, v76, 2uLL);
  v81 = vextq_s8(v76, vdupq_lane_s16(*v70.i8, 1), 2uLL);
  *v77 = v78;
  v77[1] = v79;
  v77[2] = v80;
  v77[3] = v81;
  v82 = (v77 + v29 + 48);
  v83 = vextq_s8(v78, v79, 2uLL);
  v84 = vextq_s8(v79, v80, 2uLL);
  v85 = vextq_s8(v80, v81, 2uLL);
  v86 = vextq_s8(v81, vdupq_lane_s16(*v70.i8, 2), 2uLL);
  *v82 = v83;
  v82[1] = v84;
  v82[2] = v85;
  v82[3] = v86;
  v87 = (v82 + v29 + 48);
  v88 = vextq_s8(v83, v84, 2uLL);
  v89 = vextq_s8(v84, v85, 2uLL);
  v90 = vextq_s8(v85, v86, 2uLL);
  v91 = vextq_s8(v86, vdupq_lane_s16(*v70.i8, 3), 2uLL);
  *v87 = v88;
  v87[1] = v89;
  v87[2] = v90;
  v87[3] = v91;
  v92 = (v87 + v29 + 48);
  v93 = vextq_s8(v88, v89, 2uLL);
  v94 = vextq_s8(v89, v90, 2uLL);
  v95 = vextq_s8(v90, v91, 2uLL);
  v96 = vextq_s8(v91, vdupq_laneq_s16(v70, 4), 2uLL);
  *v92 = v93;
  v92[1] = v94;
  v92[2] = v95;
  v92[3] = v96;
  v97 = (v92 + v29 + 48);
  v98 = vextq_s8(v93, v94, 2uLL);
  v99 = vextq_s8(v94, v95, 2uLL);
  v100 = vextq_s8(v95, v96, 2uLL);
  v101 = vextq_s8(v96, vdupq_laneq_s16(v70, 5), 2uLL);
  *v97 = v98;
  v97[1] = v99;
  v97[2] = v100;
  v97[3] = v101;
  v102 = (v97 + v29 + 48);
  v103 = vextq_s8(v98, v99, 2uLL);
  v104 = vextq_s8(v99, v100, 2uLL);
  v105 = vextq_s8(v100, v101, 2uLL);
  v106 = vextq_s8(v101, vdupq_laneq_s16(v70, 6), 2uLL);
  *v102 = v103;
  v102[1] = v104;
  v102[2] = v105;
  v102[3] = v106;
  v107 = vrhaddq_u16(v69, v23);
  v108 = vextq_s8(v103, v104, 2uLL);
  v109 = vextq_s8(v104, v105, 2uLL);
  v110 = vextq_s8(v105, v106, 2uLL);
  v111 = vextq_s8(v106, vdupq_laneq_s16(v70, 7), 2uLL);
  v112 = (v102 + v29 + 48);
  *v112 = v108;
  v112[1] = v109;
  v112[2] = v110;
  v112[3] = v111;
  v113 = (v112 + v29 + 48);
  v114 = vextq_s8(v108, v109, 2uLL);
  v115 = vextq_s8(v109, v110, 2uLL);
  v116 = vextq_s8(v110, v111, 2uLL);
  v117 = vextq_s8(v111, v107, 2uLL);
  *v113 = v114;
  v113[1] = v115;
  v113[2] = v116;
  v113[3] = v117;
  v118 = (v113 + v29 + 48);
  v119 = vextq_s8(v114, v115, 2uLL);
  v120 = vextq_s8(v115, v116, 2uLL);
  v121 = vextq_s8(v116, v117, 2uLL);
  v122 = vextq_s8(v117, vdupq_lane_s16(*v107.i8, 1), 2uLL);
  *v118 = v119;
  v118[1] = v120;
  v118[2] = v121;
  v118[3] = v122;
  v123 = (v118 + v29 + 48);
  v124 = vextq_s8(v119, v120, 2uLL);
  v125 = vextq_s8(v120, v121, 2uLL);
  v126 = vextq_s8(v121, v122, 2uLL);
  v127 = vextq_s8(v122, vdupq_lane_s16(*v107.i8, 2), 2uLL);
  *v123 = v124;
  v123[1] = v125;
  v123[2] = v126;
  v123[3] = v127;
  v128 = (v123 + v29 + 48);
  v129 = vextq_s8(v124, v125, 2uLL);
  v130 = vextq_s8(v125, v126, 2uLL);
  v131 = vextq_s8(v126, v127, 2uLL);
  v132 = vextq_s8(v127, vdupq_lane_s16(*v107.i8, 3), 2uLL);
  *v128 = v129;
  v128[1] = v130;
  v128[2] = v131;
  v128[3] = v132;
  v133 = (v128 + v29 + 48);
  v134 = vextq_s8(v129, v130, 2uLL);
  v135 = vextq_s8(v130, v131, 2uLL);
  v136 = vextq_s8(v131, v132, 2uLL);
  v137 = vextq_s8(v132, vdupq_laneq_s16(v107, 4), 2uLL);
  *v133 = v134;
  v133[1] = v135;
  v133[2] = v136;
  v133[3] = v137;
  v138 = (v133 + v29 + 48);
  v139 = vextq_s8(v134, v135, 2uLL);
  v140 = vextq_s8(v135, v136, 2uLL);
  v141 = vextq_s8(v136, v137, 2uLL);
  v142 = vextq_s8(v137, vdupq_laneq_s16(v107, 5), 2uLL);
  *v138 = v139;
  v138[1] = v140;
  v138[2] = v141;
  v138[3] = v142;
  v143 = (v138 + v29 + 48);
  v144 = vextq_s8(v139, v140, 2uLL);
  v145 = vextq_s8(v140, v141, 2uLL);
  v146 = vextq_s8(v141, v142, 2uLL);
  v147 = vextq_s8(v142, vdupq_laneq_s16(v107, 6), 2uLL);
  *v143 = v144;
  v143[1] = v145;
  v143[2] = v146;
  v143[3] = v147;
  v148 = vextq_s8(v144, v145, 2uLL);
  v149 = vextq_s8(v145, v146, 2uLL);
  v150 = vextq_s8(v146, v147, 2uLL);
  v151 = vextq_s8(v147, vdupq_laneq_s16(v107, 7), 2uLL);
  v152 = (v143 + v29 + 48);
  *v152 = v148;
  v152[1] = v149;
  v152[2] = v150;
  v152[3] = v151;
  v153 = (v152 + v29 + 48);
  v154 = vextq_s8(v148, v149, 2uLL);
  v155 = vextq_s8(v149, v150, 2uLL);
  v156 = vextq_s8(v150, v151, 2uLL);
  v157 = vextq_s8(v151, v28, 2uLL);
  *v153 = v154;
  v153[1] = v155;
  v153[2] = v156;
  v153[3] = v157;
  v158 = (v153 + v29 + 48);
  v159 = vextq_s8(v154, v155, 2uLL);
  v160 = vextq_s8(v155, v156, 2uLL);
  v161 = vextq_s8(v156, v157, 2uLL);
  v162 = vextq_s8(v157, vdupq_lane_s16(*v28.i8, 1), 2uLL);
  *v158 = v159;
  v158[1] = v160;
  v158[2] = v161;
  v158[3] = v162;
  v163 = (v158 + v29 + 48);
  v164 = vextq_s8(v159, v160, 2uLL);
  v165 = vextq_s8(v160, v161, 2uLL);
  v166 = vextq_s8(v161, v162, 2uLL);
  v167 = vextq_s8(v162, vdupq_lane_s16(*v28.i8, 2), 2uLL);
  *v163 = v164;
  v163[1] = v165;
  v163[2] = v166;
  v163[3] = v167;
  v168 = (v163 + v29 + 48);
  v169 = vextq_s8(v164, v165, 2uLL);
  v170 = vextq_s8(v165, v166, 2uLL);
  v171 = vextq_s8(v166, v167, 2uLL);
  v172 = vextq_s8(v167, vdupq_lane_s16(*v28.i8, 3), 2uLL);
  *v168 = v169;
  v168[1] = v170;
  v168[2] = v171;
  v168[3] = v172;
  v173 = (v168 + v29 + 48);
  v174 = vextq_s8(v169, v170, 2uLL);
  v175 = vextq_s8(v170, v171, 2uLL);
  v176 = vextq_s8(v171, v172, 2uLL);
  v177 = vextq_s8(v172, vdupq_laneq_s16(v28, 4), 2uLL);
  *v173 = v174;
  v173[1] = v175;
  v173[2] = v176;
  v173[3] = v177;
  v178 = (v173 + v29 + 48);
  v179 = vextq_s8(v174, v175, 2uLL);
  v180 = vextq_s8(v175, v176, 2uLL);
  v181 = vextq_s8(v176, v177, 2uLL);
  v182 = vextq_s8(v177, vdupq_laneq_s16(v28, 5), 2uLL);
  *v178 = v179;
  v178[1] = v180;
  v178[2] = v181;
  v178[3] = v182;
  v183 = (v178 + v29 + 48);
  result = vextq_s8(v182, vdupq_laneq_s16(v28, 6), 2uLL);
  *v183 = vextq_s8(v179, v180, 2uLL);
  v183[1] = vextq_s8(v180, v181, 2uLL);
  v183[2] = vextq_s8(v181, v182, 2uLL);
  v183[3] = result;
  return result;
}

int8x8_t vpx_highbd_d207_predictor_4x4_neon(int8x8_t *a1, uint64_t a2, uint64_t a3, int16x4_t *a4)
{
  v4 = vdup_lane_s16(*a4, 3);
  v5 = vext_s8(*a4, v4, 2uLL);
  v6 = vrhadd_u16(*a4, v5);
  v7 = vrhadd_u16(vhadd_u16(*a4, vext_s8(*a4, v4, 4uLL)), v5);
  v8 = vzip1_s16(v6, v7);
  v9 = vzip2_s16(v6, v7);
  *a1 = v8;
  *(a1 + 2 * a2) = vext_s8(v8, v9, 4uLL);
  *(a1 + 4 * a2) = v9;
  result = vext_s8(v9, v4, 4uLL);
  *(a1 + 6 * a2) = result;
  return result;
}

int8x16_t vpx_highbd_d207_predictor_8x8_neon(int8x16_t *a1, uint64_t a2, uint64_t a3, int16x8_t *a4)
{
  v4 = vdupq_laneq_s16(*a4, 7);
  v5 = vextq_s8(*a4, v4, 2uLL);
  v6 = vrhaddq_u16(*a4, v5);
  v7 = vrhaddq_u16(vhaddq_u16(*a4, vextq_s8(*a4, v4, 4uLL)), v5);
  v8 = vzip1q_s16(v6, v7);
  v9 = vzip2q_s16(v6, v7);
  *a1 = v8;
  *(a1 + 2 * a2) = vextq_s8(v8, v9, 4uLL);
  *(a1 + 4 * a2) = vextq_s8(v8, v9, 8uLL);
  *(a1 + 6 * a2) = vextq_s8(v8, v9, 0xCuLL);
  *(a1 + 8 * a2) = v9;
  *(a1 + 10 * a2) = vextq_s8(v9, v4, 4uLL);
  *(a1 + 12 * a2) = vextq_s8(v9, v4, 8uLL);
  result = vextq_s8(v9, v4, 0xCuLL);
  *(a1 + 14 * a2) = result;
  return result;
}

int8x16_t vpx_highbd_d207_predictor_16x16_neon(int8x16_t *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v4 = *(a4 + 16);
  v5 = *(a4 + 2);
  result = vdupq_laneq_s16(v4, 7);
  v7 = vextq_s8(v4, result, 2uLL);
  v8 = vrhaddq_u16(*a4, v5);
  v9 = vrhaddq_u16(v4, v7);
  v10 = vrhaddq_u16(vhaddq_u16(*a4, *(a4 + 4)), v5);
  v11 = vzip1q_s16(v8, v10);
  v12 = vzip2q_s16(v8, v10);
  *a1 = v11;
  a1[1] = v12;
  v13 = vrhaddq_u16(vhaddq_u16(v4, vextq_s8(v4, result, 4uLL)), v7);
  v14 = vzip1q_s16(v9, v13);
  v15 = (a1 + 2 * a2);
  v16 = vextq_s8(v12, v14, 4uLL);
  *v15 = vextq_s8(v11, v12, 4uLL);
  v15[1] = v16;
  v17 = (a1 + 4 * a2);
  v18 = vextq_s8(v12, v14, 8uLL);
  *v17 = vextq_s8(v11, v12, 8uLL);
  v17[1] = v18;
  v19 = (a1 + 6 * a2);
  v20 = vextq_s8(v12, v14, 0xCuLL);
  *v19 = vextq_s8(v11, v12, 0xCuLL);
  v19[1] = v20;
  v21 = vzip2q_s16(v9, v13);
  v22 = (a1 + 8 * a2);
  *v22 = v12;
  v22[1] = v14;
  v23 = (a1 + 10 * a2);
  v24 = vextq_s8(v14, v21, 4uLL);
  *v23 = v16;
  v23[1] = v24;
  v25 = (a1 + 12 * a2);
  v26 = vextq_s8(v14, v21, 8uLL);
  *v25 = v18;
  v25[1] = v26;
  v27 = (a1 + 14 * a2);
  v28 = vextq_s8(v14, v21, 0xCuLL);
  *v27 = v20;
  v27[1] = v28;
  v29 = &a1[a2];
  *v29 = v14;
  v29[1] = v21;
  v30 = (a1 + 18 * a2);
  v31 = vextq_s8(v21, result, 4uLL);
  *v30 = v24;
  v30[1] = v31;
  v32 = (a1 + 20 * a2);
  v33 = vextq_s8(v21, result, 8uLL);
  *v32 = v26;
  v32[1] = v33;
  v34 = (a1 + 22 * a2);
  v35 = vextq_s8(v21, result, 0xCuLL);
  *v34 = v28;
  v34[1] = v35;
  v36 = (a1 + 24 * a2);
  *v36 = v21;
  v36[1] = result;
  v37 = (a1 + 26 * a2);
  *v37 = v31;
  v37[1] = result;
  v38 = (a1 + 28 * a2);
  *v38 = v33;
  v38[1] = result;
  v39 = (a1 + 30 * a2);
  *v39 = v35;
  v39[1] = result;
  return result;
}

int8x16_t vpx_highbd_d207_predictor_32x32_neon(int8x16_t *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v4 = *(a4 + 16);
  v5 = *(a4 + 2);
  v6 = *(a4 + 18);
  v7 = *(a4 + 32);
  v8 = *(a4 + 48);
  v9 = *(a4 + 34);
  result = vdupq_laneq_s16(v8, 7);
  v11 = vextq_s8(v8, result, 2uLL);
  v12 = vrhaddq_u16(*a4, v5);
  v13 = vrhaddq_u16(v4, v6);
  v14 = vrhaddq_u16(v7, v9);
  v15 = vrhaddq_u16(v8, v11);
  v16 = vrhaddq_u16(vhaddq_u16(*a4, *(a4 + 4)), v5);
  v17 = vrhaddq_u16(vhaddq_u16(v4, *(a4 + 20)), v6);
  v18 = vrhaddq_u16(vhaddq_u16(v7, *(a4 + 36)), v9);
  v19 = vrhaddq_u16(vhaddq_u16(v8, vextq_s8(v8, result, 4uLL)), v11);
  v20 = vzip1q_s16(v12, v16);
  v21 = vzip2q_s16(v12, v16);
  v22 = vzip1q_s16(v13, v17);
  v23 = vzip2q_s16(v13, v17);
  *a1 = v20;
  a1[1] = v21;
  v24 = vzip1q_s16(v14, v18);
  v25 = vzip2q_s16(v14, v18);
  v26 = vzip1q_s16(v15, v19);
  a1[2] = v22;
  a1[3] = v23;
  v27 = vzip2q_s16(v15, v19);
  v28 = (a1 + 2 * a2);
  v29 = vextq_s8(v21, v22, 4uLL);
  *v28 = vextq_s8(v20, v21, 4uLL);
  v28[1] = v29;
  v30 = vextq_s8(v22, v23, 4uLL);
  v31 = vextq_s8(v23, v24, 4uLL);
  v28[2] = v30;
  v28[3] = v31;
  v32 = (a1 + 4 * a2);
  v33 = vextq_s8(v21, v22, 8uLL);
  *v32 = vextq_s8(v20, v21, 8uLL);
  v32[1] = v33;
  v34 = vextq_s8(v22, v23, 8uLL);
  v35 = vextq_s8(v23, v24, 8uLL);
  v32[2] = v34;
  v32[3] = v35;
  v36 = (a1 + 6 * a2);
  v37 = vextq_s8(v21, v22, 0xCuLL);
  *v36 = vextq_s8(v20, v21, 0xCuLL);
  v36[1] = v37;
  v38 = vextq_s8(v22, v23, 0xCuLL);
  v39 = vextq_s8(v23, v24, 0xCuLL);
  v36[2] = v38;
  v36[3] = v39;
  v40 = (a1 + 8 * a2);
  *v40 = v21;
  v40[1] = v22;
  v40[2] = v23;
  v40[3] = v24;
  v41 = (a1 + 10 * a2);
  *v41 = v29;
  v41[1] = v30;
  v42 = vextq_s8(v24, v25, 4uLL);
  v41[2] = v31;
  v41[3] = v42;
  v43 = (a1 + 12 * a2);
  *v43 = v33;
  v43[1] = v34;
  v44 = vextq_s8(v24, v25, 8uLL);
  v43[2] = v35;
  v43[3] = v44;
  v45 = (a1 + 14 * a2);
  *v45 = v37;
  v45[1] = v38;
  v46 = vextq_s8(v24, v25, 0xCuLL);
  v45[2] = v39;
  v45[3] = v46;
  v47 = &a1[a2];
  *v47 = v22;
  v47[1] = v23;
  v47[2] = v24;
  v47[3] = v25;
  v48 = (a1 + 18 * a2);
  *v48 = v30;
  v48[1] = v31;
  v49 = vextq_s8(v25, v26, 4uLL);
  v48[2] = v42;
  v48[3] = v49;
  v50 = (a1 + 20 * a2);
  *v50 = v34;
  v50[1] = v35;
  v51 = vextq_s8(v25, v26, 8uLL);
  v50[2] = v44;
  v50[3] = v51;
  v52 = (a1 + 22 * a2);
  *v52 = v38;
  v52[1] = v39;
  v53 = vextq_s8(v25, v26, 0xCuLL);
  v52[2] = v46;
  v52[3] = v53;
  v54 = (a1 + 24 * a2);
  *v54 = v23;
  v54[1] = v24;
  v54[2] = v25;
  v54[3] = v26;
  v55 = (a1 + 26 * a2);
  *v55 = v31;
  v55[1] = v42;
  v56 = vextq_s8(v26, v27, 4uLL);
  v55[2] = v49;
  v55[3] = v56;
  v57 = (a1 + 28 * a2);
  *v57 = v35;
  v57[1] = v44;
  v58 = vextq_s8(v26, v27, 8uLL);
  v57[2] = v51;
  v57[3] = v58;
  v59 = (a1 + 30 * a2);
  *v59 = v39;
  v59[1] = v46;
  v60 = vextq_s8(v26, v27, 0xCuLL);
  v59[2] = v53;
  v59[3] = v60;
  v61 = &a1[2 * a2];
  *v61 = v24;
  v61[1] = v25;
  v61[2] = v26;
  v61[3] = v27;
  v62 = (a1 + 34 * a2);
  *v62 = v42;
  v62[1] = v49;
  v63 = vextq_s8(v27, result, 4uLL);
  v62[2] = v56;
  v62[3] = v63;
  v64 = (a1 + 36 * a2);
  *v64 = v44;
  v64[1] = v51;
  v65 = vextq_s8(v27, result, 8uLL);
  v64[2] = v58;
  v64[3] = v65;
  v66 = (a1 + 38 * a2);
  *v66 = v46;
  v66[1] = v53;
  v67 = vextq_s8(v27, result, 0xCuLL);
  v66[2] = v60;
  v66[3] = v67;
  v68 = (a1 + 40 * a2);
  *v68 = v25;
  v68[1] = v26;
  v68[2] = v27;
  v68[3] = result;
  v69 = (a1 + 42 * a2);
  *v69 = v49;
  v69[1] = v56;
  v69[2] = v63;
  v69[3] = result;
  v70 = (a1 + 44 * a2);
  *v70 = v51;
  v70[1] = v58;
  v70[2] = v65;
  v70[3] = result;
  v71 = (a1 + 46 * a2);
  *v71 = v53;
  v71[1] = v60;
  v71[2] = v67;
  v71[3] = result;
  v72 = &a1[3 * a2];
  *v72 = v26;
  v72[1] = v27;
  v72[2] = result;
  v72[3] = result;
  v73 = (a1 + 50 * a2);
  *v73 = v56;
  v73[1] = v63;
  v73[2] = result;
  v73[3] = result;
  v74 = (a1 + 52 * a2);
  *v74 = v58;
  v74[1] = v65;
  v74[2] = result;
  v74[3] = result;
  v75 = (a1 + 54 * a2);
  *v75 = v60;
  v75[1] = v67;
  v75[2] = result;
  v75[3] = result;
  v76 = (a1 + 56 * a2);
  *v76 = v27;
  v76[1] = result;
  v76[2] = result;
  v76[3] = result;
  v77 = (a1 + 58 * a2);
  *v77 = v63;
  v77[1] = result;
  v77[2] = result;
  v77[3] = result;
  v78 = (a1 + 60 * a2);
  *v78 = v65;
  v78[1] = result;
  v78[2] = result;
  v78[3] = result;
  v79 = (a1 + 62 * a2);
  *v79 = v67;
  v79[1] = result;
  v79[2] = result;
  v79[3] = result;
  return result;
}

double vpx_highbd_v_predictor_4x4_neon(void *a1, uint64_t a2, double *a3)
{
  result = *a3;
  *a1 = *a3;
  v4 = (a1 + 2 * a2);
  *v4 = result;
  v5 = (v4 + 2 * a2);
  *v5 = result;
  *(v5 + 2 * a2) = result;
  return result;
}

__n128 vpx_highbd_v_predictor_8x8_neon(__n128 *a1, uint64_t a2, __n128 *a3)
{
  result = *a3;
  *a1 = *a3;
  v4 = (a1 + 2 * a2);
  *v4 = result;
  v5 = (v4 + 2 * a2);
  *v5 = result;
  v6 = (v5 + 2 * a2);
  *v6 = result;
  v7 = (v6 + 2 * a2);
  *v7 = result;
  v8 = (v7 + 2 * a2);
  *v8 = result;
  v9 = (v8 + 2 * a2);
  *v9 = result;
  *(v9 + 2 * a2) = result;
  return result;
}

__n128 vpx_highbd_v_predictor_16x16_neon(_OWORD *a1, uint64_t a2, uint64_t a3)
{
  result = *a3;
  v4 = *(a3 + 16);
  *a1 = *a3;
  a1[1] = v4;
  v5 = a1 + 2 * a2;
  *v5 = result;
  *(v5 + 1) = v4;
  v6 = &v5[2 * a2];
  *v6 = result;
  *(v6 + 1) = v4;
  v7 = &v6[2 * a2];
  *v7 = result;
  *(v7 + 1) = v4;
  v8 = &v7[2 * a2];
  *v8 = result;
  *(v8 + 1) = v4;
  v9 = &v8[2 * a2];
  *v9 = result;
  *(v9 + 1) = v4;
  v10 = &v9[2 * a2];
  *v10 = result;
  *(v10 + 1) = v4;
  v11 = &v10[2 * a2];
  *v11 = result;
  *(v11 + 1) = v4;
  v12 = &v11[2 * a2];
  *v12 = result;
  *(v12 + 1) = v4;
  v13 = &v12[2 * a2];
  *v13 = result;
  *(v13 + 16) = v4;
  v14 = v13 + 2 * a2;
  *v14 = result;
  *(v14 + 16) = v4;
  v15 = v14 + 2 * a2;
  *v15 = result;
  *(v15 + 16) = v4;
  v16 = v15 + 2 * a2;
  *v16 = result;
  *(v16 + 16) = v4;
  v17 = v16 + 2 * a2;
  *v17 = result;
  *(v17 + 16) = v4;
  v18 = v17 + 2 * a2;
  *v18 = result;
  *(v18 + 16) = v4;
  v19 = v18 + 2 * a2;
  *v19 = result;
  *(v19 + 16) = v4;
  return result;
}

__n128 vpx_highbd_v_predictor_32x32_neon(_OWORD *a1, uint64_t a2, uint64_t a3)
{
  result = *a3;
  v4 = *(a3 + 16);
  v5 = *(a3 + 32);
  v6 = *(a3 + 48);
  *a1 = *a3;
  a1[1] = v4;
  a1[2] = v5;
  a1[3] = v6;
  v7 = a1 + 2 * a2;
  *v7 = result;
  *(v7 + 1) = v4;
  *(v7 + 2) = v5;
  *(v7 + 3) = v6;
  v8 = &v7[2 * a2];
  *v8 = result;
  *(v8 + 1) = v4;
  *(v8 + 2) = v5;
  *(v8 + 3) = v6;
  v9 = &v8[2 * a2];
  *v9 = result;
  *(v9 + 1) = v4;
  *(v9 + 2) = v5;
  *(v9 + 3) = v6;
  v10 = &v9[2 * a2];
  *v10 = result;
  *(v10 + 1) = v4;
  *(v10 + 2) = v5;
  *(v10 + 3) = v6;
  v11 = &v10[2 * a2];
  *v11 = result;
  *(v11 + 1) = v4;
  *(v11 + 2) = v5;
  *(v11 + 3) = v6;
  v12 = &v11[2 * a2];
  *v12 = result;
  *(v12 + 1) = v4;
  *(v12 + 2) = v5;
  *(v12 + 3) = v6;
  v13 = &v12[2 * a2];
  *v13 = result;
  *(v13 + 1) = v4;
  *(v13 + 2) = v5;
  *(v13 + 3) = v6;
  v14 = &v13[2 * a2];
  *v14 = result;
  *(v14 + 1) = v4;
  *(v14 + 2) = v5;
  *(v14 + 3) = v6;
  v15 = &v14[2 * a2];
  *v15 = result;
  *(v15 + 16) = v4;
  *(v15 + 32) = v5;
  *(v15 + 48) = v6;
  v16 = v15 + 2 * a2;
  *v16 = result;
  *(v16 + 16) = v4;
  *(v16 + 32) = v5;
  *(v16 + 48) = v6;
  v17 = v16 + 2 * a2;
  *v17 = result;
  *(v17 + 16) = v4;
  *(v17 + 32) = v5;
  *(v17 + 48) = v6;
  v18 = v17 + 2 * a2;
  *v18 = result;
  *(v18 + 16) = v4;
  *(v18 + 32) = v5;
  *(v18 + 48) = v6;
  v19 = v18 + 2 * a2;
  *v19 = result;
  *(v19 + 16) = v4;
  *(v19 + 32) = v5;
  *(v19 + 48) = v6;
  v20 = v19 + 2 * a2;
  *v20 = result;
  *(v20 + 16) = v4;
  *(v20 + 32) = v5;
  *(v20 + 48) = v6;
  v21 = v20 + 2 * a2;
  *v21 = result;
  *(v21 + 16) = v4;
  *(v21 + 32) = v5;
  *(v21 + 48) = v6;
  v22 = v21 + 2 * a2;
  *v22 = result;
  *(v22 + 16) = v4;
  *(v22 + 32) = v5;
  *(v22 + 48) = v6;
  v23 = v22 + 2 * a2;
  *v23 = result;
  *(v23 + 16) = v4;
  *(v23 + 32) = v5;
  *(v23 + 48) = v6;
  v24 = v23 + 2 * a2;
  *v24 = result;
  *(v24 + 16) = v4;
  *(v24 + 32) = v5;
  *(v24 + 48) = v6;
  v25 = v24 + 2 * a2;
  *v25 = result;
  *(v25 + 16) = v4;
  *(v25 + 32) = v5;
  *(v25 + 48) = v6;
  v26 = v25 + 2 * a2;
  *v26 = result;
  *(v26 + 16) = v4;
  *(v26 + 32) = v5;
  *(v26 + 48) = v6;
  v27 = v26 + 2 * a2;
  *v27 = result;
  *(v27 + 16) = v4;
  *(v27 + 32) = v5;
  *(v27 + 48) = v6;
  v28 = v27 + 2 * a2;
  *v28 = result;
  *(v28 + 16) = v4;
  *(v28 + 32) = v5;
  *(v28 + 48) = v6;
  v29 = v28 + 2 * a2;
  *v29 = result;
  *(v29 + 16) = v4;
  *(v29 + 32) = v5;
  *(v29 + 48) = v6;
  v30 = v29 + 2 * a2;
  *v30 = result;
  *(v30 + 16) = v4;
  *(v30 + 32) = v5;
  *(v30 + 48) = v6;
  v31 = v30 + 2 * a2;
  *v31 = result;
  *(v31 + 16) = v4;
  *(v31 + 32) = v5;
  *(v31 + 48) = v6;
  v32 = v31 + 2 * a2;
  *v32 = result;
  *(v32 + 16) = v4;
  *(v32 + 32) = v5;
  *(v32 + 48) = v6;
  v33 = v32 + 2 * a2;
  *v33 = result;
  *(v33 + 16) = v4;
  *(v33 + 32) = v5;
  *(v33 + 48) = v6;
  v34 = v33 + 2 * a2;
  *v34 = result;
  *(v34 + 16) = v4;
  *(v34 + 32) = v5;
  *(v34 + 48) = v6;
  v35 = v34 + 2 * a2;
  *v35 = result;
  *(v35 + 16) = v4;
  *(v35 + 32) = v5;
  *(v35 + 48) = v6;
  v36 = v35 + 2 * a2;
  *v36 = result;
  *(v36 + 16) = v4;
  *(v36 + 32) = v5;
  *(v36 + 48) = v6;
  v37 = v36 + 2 * a2;
  *v37 = result;
  *(v37 + 16) = v4;
  *(v37 + 32) = v5;
  *(v37 + 48) = v6;
  return result;
}

int16x4_t vpx_highbd_h_predictor_4x4_neon(int16x4_t *a1, uint64_t a2, uint64_t a3, int16x4_t *a4)
{
  v4 = *a4;
  *a1 = vdup_lane_s16(*a4, 0);
  v5 = (a1 + 2 * a2);
  *v5 = vdup_lane_s16(v4, 1);
  v6 = (v5 + 2 * a2);
  *v6 = vdup_lane_s16(v4, 2);
  result = vdup_lane_s16(v4, 3);
  *(v6 + 2 * a2) = result;
  return result;
}

int16x8_t vpx_highbd_h_predictor_8x8_neon(int16x8_t *a1, uint64_t a2, uint64_t a3, int16x4_t *a4)
{
  v4 = *a4->i8;
  *a1 = vdupq_lane_s16(*a4, 0);
  v5 = (a1 + 2 * a2);
  *v5 = vdupq_lane_s16(*v4.i8, 1);
  v6 = (v5 + 2 * a2);
  *v6 = vdupq_lane_s16(*v4.i8, 2);
  v7 = (v6 + 2 * a2);
  *v7 = vdupq_lane_s16(*v4.i8, 3);
  v8 = (v7 + 2 * a2);
  *v8 = vdupq_laneq_s16(v4, 4);
  v9 = (v8 + 2 * a2);
  *v9 = vdupq_laneq_s16(v4, 5);
  v10 = (v9 + 2 * a2);
  *v10 = vdupq_laneq_s16(v4, 6);
  result = vdupq_laneq_s16(v4, 7);
  *(v10 + 2 * a2) = result;
  return result;
}

int16x8_t vpx_highbd_h_predictor_16x16_neon(int16x8_t *a1, uint64_t a2, uint64_t a3, int16x8_t *a4)
{
  v4 = *a4;
  v5 = vdupq_lane_s16(*a4->i8, 0);
  *a1 = v5;
  a1[1] = v5;
  v6 = (a1 + 2 * a2 + 16);
  v7 = vdupq_lane_s16(*v4.i8, 1);
  v6[-1] = v7;
  *v6 = v7;
  v8 = (v6 + 2 * a2);
  v9 = vdupq_lane_s16(*v4.i8, 2);
  v8[-1] = v9;
  *v8 = v9;
  v10 = (v8 + 2 * a2);
  v11 = vdupq_lane_s16(*v4.i8, 3);
  v10[-1] = v11;
  *v10 = v11;
  v12 = (v10 + 2 * a2);
  v13 = vdupq_laneq_s16(v4, 4);
  v12[-1] = v13;
  *v12 = v13;
  v14 = vdupq_laneq_s16(v4, 5);
  v15 = (v12 + 2 * a2);
  v15[-1] = v14;
  *v15 = v14;
  v16 = (v15 + 2 * a2);
  v17 = vdupq_laneq_s16(v4, 6);
  v16[-1] = v17;
  *v16 = v17;
  v18 = (v16 + 2 * a2);
  v19 = vdupq_laneq_s16(v4, 7);
  v18[-1] = v19;
  *v18 = v19;
  v20 = (v18 + 2 * a2);
  v21 = a4[1];
  v22 = vdupq_lane_s16(*v21.i8, 0);
  v20[-1] = v22;
  *v20 = v22;
  v23 = (v20 + 2 * a2);
  v24 = vdupq_lane_s16(*v21.i8, 1);
  v23[-1] = v24;
  *v23 = v24;
  v25 = (v23 + 2 * a2);
  v26 = vdupq_lane_s16(*v21.i8, 2);
  v25[-1] = v26;
  *v25 = v26;
  v27 = (v25 + 2 * a2);
  v28 = vdupq_lane_s16(*v21.i8, 3);
  v27[-1] = v28;
  *v27 = v28;
  v29 = vdupq_laneq_s16(v21, 4);
  v30 = (v27 + 2 * a2);
  v30[-1] = v29;
  *v30 = v29;
  v31 = (v30 + 2 * a2);
  v32 = vdupq_laneq_s16(v21, 5);
  v31[-1] = v32;
  *v31 = v32;
  v33 = (v31 + 2 * a2);
  v34 = vdupq_laneq_s16(v21, 6);
  v33[-1] = v34;
  *v33 = v34;
  v35 = (v33 + 2 * a2);
  result = vdupq_laneq_s16(v21, 7);
  v35[-1] = result;
  *v35 = result;
  return result;
}

int16x8_t vpx_highbd_h_predictor_32x32_neon(int16x8_t *a1, uint64_t a2, uint64_t a3, int16x8_t *a4)
{
  v4 = *a4;
  v5 = vdupq_lane_s16(*a4->i8, 0);
  *a1 = v5;
  a1[1] = v5;
  a1[2] = v5;
  a1[3] = v5;
  v6 = (a1 + 2 * a2 + 48);
  v7 = vdupq_lane_s16(*v4.i8, 1);
  v6[-3] = v7;
  v6[-2] = v7;
  v6[-1] = v7;
  *v6 = v7;
  v8 = (v6 + 2 * a2);
  v9 = vdupq_lane_s16(*v4.i8, 2);
  v8[-3] = v9;
  v8[-2] = v9;
  v8[-1] = v9;
  *v8 = v9;
  v10 = vdupq_lane_s16(*v4.i8, 3);
  v11 = (v8 + 2 * a2);
  v11[-3] = v10;
  v11[-2] = v10;
  v11[-1] = v10;
  *v11 = v10;
  v12 = (v11 + 2 * a2);
  v13 = vdupq_laneq_s16(v4, 4);
  v12[-3] = v13;
  v12[-2] = v13;
  v12[-1] = v13;
  *v12 = v13;
  v14 = (v12 + 2 * a2);
  v15 = vdupq_laneq_s16(v4, 5);
  v14[-3] = v15;
  v14[-2] = v15;
  v14[-1] = v15;
  *v14 = v15;
  v16 = (v14 + 2 * a2);
  v17 = vdupq_laneq_s16(v4, 6);
  v16[-3] = v17;
  v16[-2] = v17;
  v16[-1] = v17;
  *v16 = v17;
  v18 = (v16 + 2 * a2);
  v19 = vdupq_laneq_s16(v4, 7);
  v18[-3] = v19;
  v18[-2] = v19;
  v18[-1] = v19;
  *v18 = v19;
  v20 = a4[1];
  v21 = vdupq_lane_s16(*v20.i8, 0);
  v22 = (v18 + 2 * a2);
  v22[-3] = v21;
  v22[-2] = v21;
  v22[-1] = v21;
  *v22 = v21;
  v23 = (v22 + 2 * a2);
  v24 = vdupq_lane_s16(*v20.i8, 1);
  v23[-3] = v24;
  v23[-2] = v24;
  v23[-1] = v24;
  *v23 = v24;
  v25 = (v23 + 2 * a2);
  v26 = vdupq_lane_s16(*v20.i8, 2);
  v25[-3] = v26;
  v25[-2] = v26;
  v25[-1] = v26;
  *v25 = v26;
  v27 = (v25 + 2 * a2);
  v28 = vdupq_lane_s16(*v20.i8, 3);
  v27[-3] = v28;
  v27[-2] = v28;
  v27[-1] = v28;
  *v27 = v28;
  v29 = (v27 + 2 * a2);
  v30 = vdupq_laneq_s16(v20, 4);
  v29[-3] = v30;
  v29[-2] = v30;
  v29[-1] = v30;
  *v29 = v30;
  v31 = (v29 + 2 * a2);
  v32 = vdupq_laneq_s16(v20, 5);
  v31[-3] = v32;
  v31[-2] = v32;
  v31[-1] = v32;
  *v31 = v32;
  v33 = vdupq_laneq_s16(v20, 6);
  v34 = (v31 + 2 * a2);
  v34[-3] = v33;
  v34[-2] = v33;
  v34[-1] = v33;
  *v34 = v33;
  v35 = (v34 + 2 * a2);
  v36 = vdupq_laneq_s16(v20, 7);
  v35[-3] = v36;
  v35[-2] = v36;
  v35[-1] = v36;
  *v35 = v36;
  v37 = (v35 + 2 * a2);
  v38 = a4[2];
  v39 = vdupq_lane_s16(*v38.i8, 0);
  v37[-3] = v39;
  v37[-2] = v39;
  v37[-1] = v39;
  *v37 = v39;
  v40 = (v37 + 2 * a2);
  v41 = vdupq_lane_s16(*v38.i8, 1);
  v40[-3] = v41;
  v40[-2] = v41;
  v40[-1] = v41;
  *v40 = v41;
  v42 = (v40 + 2 * a2);
  v43 = vdupq_lane_s16(*v38.i8, 2);
  v42[-3] = v43;
  v42[-2] = v43;
  v42[-1] = v43;
  *v42 = v43;
  v44 = vdupq_lane_s16(*v38.i8, 3);
  v45 = (v42 + 2 * a2);
  v45[-3] = v44;
  v45[-2] = v44;
  v45[-1] = v44;
  *v45 = v44;
  v46 = (v45 + 2 * a2);
  v47 = vdupq_laneq_s16(v38, 4);
  v46[-3] = v47;
  v46[-2] = v47;
  v46[-1] = v47;
  *v46 = v47;
  v48 = (v46 + 2 * a2);
  v49 = vdupq_laneq_s16(v38, 5);
  v48[-3] = v49;
  v48[-2] = v49;
  v48[-1] = v49;
  *v48 = v49;
  v50 = (v48 + 2 * a2);
  v51 = vdupq_laneq_s16(v38, 6);
  v50[-3] = v51;
  v50[-2] = v51;
  v50[-1] = v51;
  *v50 = v51;
  v52 = (v50 + 2 * a2);
  v53 = vdupq_laneq_s16(v38, 7);
  v52[-3] = v53;
  v52[-2] = v53;
  v52[-1] = v53;
  *v52 = v53;
  v54 = a4[3];
  v55 = vdupq_lane_s16(*v54.i8, 0);
  v56 = (v52 + 2 * a2);
  v56[-3] = v55;
  v56[-2] = v55;
  v56[-1] = v55;
  *v56 = v55;
  v57 = (v56 + 2 * a2);
  v58 = vdupq_lane_s16(*v54.i8, 1);
  v57[-3] = v58;
  v57[-2] = v58;
  v57[-1] = v58;
  *v57 = v58;
  v59 = (v57 + 2 * a2);
  v60 = vdupq_lane_s16(*v54.i8, 2);
  v59[-3] = v60;
  v59[-2] = v60;
  v59[-1] = v60;
  *v59 = v60;
  v61 = (v59 + 2 * a2);
  v62 = vdupq_lane_s16(*v54.i8, 3);
  v61[-3] = v62;
  v61[-2] = v62;
  v61[-1] = v62;
  *v61 = v62;
  v63 = (v61 + 2 * a2);
  v64 = vdupq_laneq_s16(v54, 4);
  v63[-3] = v64;
  v63[-2] = v64;
  v63[-1] = v64;
  *v63 = v64;
  v65 = (v63 + 2 * a2);
  v66 = vdupq_laneq_s16(v54, 5);
  v65[-3] = v66;
  v65[-2] = v66;
  v65[-1] = v66;
  *v65 = v66;
  v67 = vdupq_laneq_s16(v54, 6);
  v68 = (v65 + 2 * a2);
  v68[-3] = v67;
  v68[-2] = v67;
  v68[-1] = v67;
  *v68 = v67;
  v69 = (v68 + 2 * a2);
  result = vdupq_laneq_s16(v54, 7);
  v69[-3] = result;
  v69[-2] = result;
  v69[-1] = result;
  *v69 = result;
  return result;
}

double vpx_highbd_tm_predictor_4x4_neon(void *a1, uint64_t a2, uint64_t *a3, unint64_t *a4, char a5, double a6, double a7, double a8, int8x16_t a9)
{
  v9 = vdupq_n_s16(~(-1 << a5));
  v10 = a3 - 1;
  v11 = vld1q_dup_s16(v10);
  v12.i64[0] = *a3;
  v12.i64[1] = *a3;
  a9.i64[0] = *a4;
  v13 = vsubq_s16(v12, v11);
  v14 = vqshluq_n_s16(vminq_s16(vaddq_s16(vqtbl1q_s8(a9, xmmword_273BC6180), v13), v9), 0);
  *a1 = v14.i64[0];
  v15 = (a1 + 2 * a2);
  *v15 = vextq_s8(v14, v14, 8uLL).u64[0];
  v16 = (v15 + 2 * a2);
  v17 = vqshluq_n_s16(vminq_s16(vaddq_s16(vqtbl1q_s8(a9, xmmword_273BC6190), v13), v9), 0);
  *v16 = v17.i64[0];
  *&result = vextq_s8(v17, v17, 8uLL).u64[0];
  *(v16 + 2 * a2) = result;
  return result;
}

int16x8_t vpx_highbd_tm_predictor_8x8_neon(int16x8_t *a1, uint64_t a2, int16x8_t *a3, int16x4_t *a4, char a5)
{
  v5 = vdupq_n_s16(~(-1 << a5));
  v6 = &a3[-1].i16[7];
  v7 = vld1q_dup_s16(v6);
  v8 = *a4->i8;
  v9 = vsubq_s16(*a3, v7);
  *a1 = vqshluq_n_s16(vminq_s16(vaddq_s16(vdupq_lane_s16(*a4, 0), v9), v5), 0);
  v10 = (a1 + 2 * a2);
  *v10 = vqshluq_n_s16(vminq_s16(vaddq_s16(vdupq_lane_s16(*v8.i8, 1), v9), v5), 0);
  v11 = (v10 + 2 * a2);
  *v11 = vqshluq_n_s16(vminq_s16(vaddq_s16(vdupq_lane_s16(*v8.i8, 2), v9), v5), 0);
  v12 = (v11 + 2 * a2);
  *v12 = vqshluq_n_s16(vminq_s16(vaddq_s16(vdupq_lane_s16(*v8.i8, 3), v9), v5), 0);
  v13 = (v12 + 2 * a2);
  *v13 = vqshluq_n_s16(vminq_s16(vaddq_s16(vdupq_laneq_s16(v8, 4), v9), v5), 0);
  v14 = (v13 + 2 * a2);
  *v14 = vqshluq_n_s16(vminq_s16(vaddq_s16(vdupq_laneq_s16(v8, 5), v9), v5), 0);
  v15 = (v14 + 2 * a2);
  *v15 = vqshluq_n_s16(vminq_s16(vaddq_s16(vdupq_laneq_s16(v8, 6), v9), v5), 0);
  result = vqshluq_n_s16(vminq_s16(vaddq_s16(vdupq_laneq_s16(v8, 7), v9), v5), 0);
  *(v15 + 2 * a2) = result;
  return result;
}

int16x8_t vpx_highbd_tm_predictor_16x16_neon(int16x8_t *a1, uint64_t a2, int16x8_t *a3, int16x8_t *a4, char a5)
{
  v5 = vdupq_n_s16(~(-1 << a5));
  v6 = &a3[-1].i16[7];
  v7 = vld1q_dup_s16(v6);
  v8 = vsubq_s16(*a3, v7);
  v9 = vsubq_s16(a3[1], v7);
  v10 = *a4;
  v11 = vdupq_lane_s16(*a4->i8, 0);
  *a1 = vqshluq_n_s16(vminq_s16(vaddq_s16(v11, v8), v5), 0);
  v12 = vdupq_lane_s16(*v10.i8, 1);
  a1[1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v11, v9), v5), 0);
  v13 = (a1 + 2 * a2 + 16);
  v13[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v12, v8), v5), 0);
  *v13 = vqshluq_n_s16(vminq_s16(vaddq_s16(v12, v9), v5), 0);
  v14 = (v13 + 2 * a2);
  v15 = vdupq_lane_s16(*v10.i8, 2);
  v14[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v15, v8), v5), 0);
  *v14 = vqshluq_n_s16(vminq_s16(vaddq_s16(v15, v9), v5), 0);
  v16 = (v14 + 2 * a2);
  v17 = vdupq_lane_s16(*v10.i8, 3);
  v16[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v17, v8), v5), 0);
  *v16 = vqshluq_n_s16(vminq_s16(vaddq_s16(v17, v9), v5), 0);
  v18 = (v16 + 2 * a2);
  v19 = vdupq_laneq_s16(v10, 4);
  v18[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v19, v8), v5), 0);
  *v18 = vqshluq_n_s16(vminq_s16(vaddq_s16(v19, v9), v5), 0);
  v20 = vdupq_laneq_s16(v10, 5);
  v21 = (v18 + 2 * a2);
  v21[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v20, v8), v5), 0);
  *v21 = vqshluq_n_s16(vminq_s16(vaddq_s16(v20, v9), v5), 0);
  v22 = (v21 + 2 * a2);
  v23 = vdupq_laneq_s16(v10, 6);
  v22[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v23, v8), v5), 0);
  *v22 = vqshluq_n_s16(vminq_s16(vaddq_s16(v23, v9), v5), 0);
  v24 = (v22 + 2 * a2);
  v25 = vdupq_laneq_s16(v10, 7);
  v24[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v25, v8), v5), 0);
  *v24 = vqshluq_n_s16(vminq_s16(vaddq_s16(v25, v9), v5), 0);
  v26 = (v24 + 2 * a2);
  v27 = a4[1];
  v28 = vdupq_lane_s16(*v27.i8, 0);
  v26[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v28, v8), v5), 0);
  *v26 = vqshluq_n_s16(vminq_s16(vaddq_s16(v28, v9), v5), 0);
  v29 = (v26 + 2 * a2);
  v30 = vdupq_lane_s16(*v27.i8, 1);
  v29[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v30, v8), v5), 0);
  *v29 = vqshluq_n_s16(vminq_s16(vaddq_s16(v30, v9), v5), 0);
  v31 = (v29 + 2 * a2);
  v32 = vdupq_lane_s16(*v27.i8, 2);
  v31[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v32, v8), v5), 0);
  *v31 = vqshluq_n_s16(vminq_s16(vaddq_s16(v32, v9), v5), 0);
  v33 = (v31 + 2 * a2);
  v34 = vdupq_lane_s16(*v27.i8, 3);
  v33[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v34, v8), v5), 0);
  *v33 = vqshluq_n_s16(vminq_s16(vaddq_s16(v34, v9), v5), 0);
  v35 = vdupq_laneq_s16(v27, 4);
  v36 = (v33 + 2 * a2);
  v36[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v35, v8), v5), 0);
  *v36 = vqshluq_n_s16(vminq_s16(vaddq_s16(v35, v9), v5), 0);
  v37 = (v36 + 2 * a2);
  v38 = vdupq_laneq_s16(v27, 5);
  v37[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v38, v8), v5), 0);
  *v37 = vqshluq_n_s16(vminq_s16(vaddq_s16(v38, v9), v5), 0);
  v39 = (v37 + 2 * a2);
  v40 = vdupq_laneq_s16(v27, 6);
  v39[-1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v40, v8), v5), 0);
  *v39 = vqshluq_n_s16(vminq_s16(vaddq_s16(v40, v9), v5), 0);
  v41 = (v39 + 2 * a2);
  v42 = vdupq_laneq_s16(v27, 7);
  v43 = vminq_s16(vaddq_s16(v42, v8), v5);
  result = vqshluq_n_s16(vminq_s16(vaddq_s16(v42, v9), v5), 0);
  v41[-1] = vqshluq_n_s16(v43, 0);
  *v41 = result;
  return result;
}

int16x8_t *vpx_highbd_tm_predictor_32x32_neon(int16x8_t *result, uint64_t a2, int16x8_t *a3, uint64_t a4, char a5)
{
  v5 = 0;
  v6 = vdupq_n_s16(~(-1 << a5));
  v7 = &a3[-1].i16[7];
  v8 = vld1q_dup_s16(v7);
  v9 = vsubq_s16(*a3, v8);
  v10 = vsubq_s16(a3[1], v8);
  v11 = vsubq_s16(a3[2], v8);
  v12 = vsubq_s16(a3[3], v8);
  v13 = 2 * a2;
  do
  {
    v14 = *(a4 + v5);
    v15 = vdupq_lane_s16(*v14.i8, 0);
    *result = vqshluq_n_s16(vminq_s16(vaddq_s16(v15, v9), v6), 0);
    result[1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v15, v10), v6), 0);
    result[2] = vqshluq_n_s16(vminq_s16(vaddq_s16(v15, v11), v6), 0);
    result[3] = vqshluq_n_s16(vminq_s16(vaddq_s16(v15, v12), v6), 0);
    v16 = vdupq_lane_s16(*v14.i8, 1);
    v17 = (result + v13);
    *v17 = vqshluq_n_s16(vminq_s16(vaddq_s16(v16, v9), v6), 0);
    v17[1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v16, v10), v6), 0);
    v17[2] = vqshluq_n_s16(vminq_s16(vaddq_s16(v16, v11), v6), 0);
    v17[3] = vqshluq_n_s16(vminq_s16(vaddq_s16(v16, v12), v6), 0);
    v18 = vdupq_lane_s16(*v14.i8, 2);
    v19 = (result + v13 + v13);
    *v19 = vqshluq_n_s16(vminq_s16(vaddq_s16(v18, v9), v6), 0);
    v19[1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v18, v10), v6), 0);
    v19[2] = vqshluq_n_s16(vminq_s16(vaddq_s16(v18, v11), v6), 0);
    v19[3] = vqshluq_n_s16(vminq_s16(vaddq_s16(v18, v12), v6), 0);
    v20 = vdupq_lane_s16(*v14.i8, 3);
    v21 = (v19 + v13);
    *v21 = vqshluq_n_s16(vminq_s16(vaddq_s16(v20, v9), v6), 0);
    v21[1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v20, v10), v6), 0);
    v22 = vdupq_laneq_s16(v14, 4);
    v21[2] = vqshluq_n_s16(vminq_s16(vaddq_s16(v20, v11), v6), 0);
    v21[3] = vqshluq_n_s16(vminq_s16(vaddq_s16(v20, v12), v6), 0);
    v23 = (v21 + v13);
    *v23 = vqshluq_n_s16(vminq_s16(vaddq_s16(v22, v9), v6), 0);
    v23[1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v22, v10), v6), 0);
    v23[2] = vqshluq_n_s16(vminq_s16(vaddq_s16(v22, v11), v6), 0);
    v23[3] = vqshluq_n_s16(vminq_s16(vaddq_s16(v22, v12), v6), 0);
    v24 = vdupq_laneq_s16(v14, 5);
    v25 = (v23 + v13);
    *v25 = vqshluq_n_s16(vminq_s16(vaddq_s16(v24, v9), v6), 0);
    v25[1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v24, v10), v6), 0);
    v25[2] = vqshluq_n_s16(vminq_s16(vaddq_s16(v24, v11), v6), 0);
    v25[3] = vqshluq_n_s16(vminq_s16(vaddq_s16(v24, v12), v6), 0);
    v26 = vdupq_laneq_s16(v14, 6);
    v27 = (v25 + v13);
    *v27 = vqshluq_n_s16(vminq_s16(vaddq_s16(v26, v9), v6), 0);
    v27[1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v26, v10), v6), 0);
    v27[2] = vqshluq_n_s16(vminq_s16(vaddq_s16(v26, v11), v6), 0);
    v27[3] = vqshluq_n_s16(vminq_s16(vaddq_s16(v26, v12), v6), 0);
    v28 = vdupq_laneq_s16(v14, 7);
    v29 = (v27 + v13);
    *v29 = vqshluq_n_s16(vminq_s16(vaddq_s16(v28, v9), v6), 0);
    v29[1] = vqshluq_n_s16(vminq_s16(vaddq_s16(v28, v10), v6), 0);
    v29[2] = vqshluq_n_s16(vminq_s16(vaddq_s16(v28, v11), v6), 0);
    v29[3] = vqshluq_n_s16(vminq_s16(vaddq_s16(v28, v12), v6), 0);
    v5 += 16;
    result = (v29 + v13);
  }

  while (v5 != 64);
  return result;
}

int16x8_t vpx_highbd_lpf_horizontal_4_neon(uint64_t a1, signed int a2, const char *a3, const char *a4, const char *a5, int a6)
{
  v6 = vdupq_n_s16(a6 - 8);
  v7 = vld1_dup_s8(a3);
  v8 = vld1_dup_s8(a4);
  v9 = vld1_dup_s8(a5);
  v10 = vmovl_u8(v9);
  v11 = vshlq_u16(vmovl_u8(v7), v6);
  v12 = vshlq_u16(vmovl_u8(v8), v6);
  v13 = vshl_u32(vdup_n_s32(a2), 0x100000002);
  v14 = -1 << (a6 - 1);
  v15 = vdupq_n_s16(~v14);
  v16 = vdupq_n_s16(v14);
  v17 = vdupq_n_s16(-128 << (a6 - 8));
  v18 = vdupq_n_s16(128 << (a6 - 8));
  v19.i64[0] = v13.i32[0];
  v19.i64[1] = v13.i32[1];
  v20 = vnegq_s64(v19);
  v21 = v20.i64[1];
  v22 = vshlq_u16(v10, v6);
  v23 = (a1 + 2 * v20.i64[0]);
  v24 = *v23;
  v25 = 2 * a2;
  v26 = (v23 + v25);
  v27 = *v26;
  v28 = &v26->i8[v25];
  v29 = *v28;
  v30 = (v28 + v25);
  v31 = *v30;
  v32 = (v30 + v25);
  v33 = *v32;
  v34 = &v32->i8[v25];
  v35 = vmaxq_u16(vabdq_u16(v29, v31), vabdq_u16(*v34, v33));
  v36 = vcgtq_u16(v35, v22);
  v37 = vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v35, vabdq_u16(v24, v27)), vabdq_u16(v27, v29)), vabdq_u16(*(v34 + v25), *v34)), vabdq_u16(*(v34 + v25 + v25), *(v34 + v25)));
  v38 = vabdq_u16(v31, v33);
  v39 = vandq_s8(vcgeq_u16(v11, vsraq_n_u16(vaddq_s16(v38, v38), vabdq_u16(v29, *v34), 1uLL)), vcgeq_u16(v12, v37));
  v40 = vaddq_s16(v29, v17);
  v41 = vaddq_s16(v31, v17);
  v42 = vaddq_s16(v33, v17);
  v43 = vsubq_s16(v33, v31);
  v33.i64[0] = 0x3000300030003;
  v33.i64[1] = 0x3000300030003;
  v44 = vandq_s8(vminq_s16(vmaxq_s16(vmlaq_s16(vandq_s8(vminq_s16(vmaxq_s16(vsubq_s16(v29, *v34), v16), v15), v36), v43, v33), v16), v15), v39);
  v29.i64[0] = 0x4000400040004;
  v29.i64[1] = 0x4000400040004;
  v45 = vshrq_n_s16(vminq_s16(vaddq_s16(v44, v29), v15), 3uLL);
  v46 = vminq_s16(vmaxq_s16(vsraq_n_s16(v41, vminq_s16(vaddq_s16(v44, v33), v15), 3uLL), v16), v15);
  v47 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(v42, v45), v16), v15), v18);
  v48 = vbicq_s8(vrshrq_n_s16(v45, 1uLL), v36);
  v49 = vminq_s16(vmaxq_s16(vsubq_s16(vaddq_s16(*v34, v17), v48), v16), v15);
  result = vaddq_s16(vminq_s16(vmaxq_s16(vaddq_s16(v40, v48), v16), v15), v18);
  v51 = (a1 + 2 * v21);
  *v51 = result;
  v52 = (v51 + v25);
  *v52 = vaddq_s16(v46, v18);
  v53 = (v52 + v25);
  *v53 = v47;
  *(v53 + v25) = vaddq_s16(v49, v18);
  return result;
}

int16x8_t vpx_highbd_lpf_horizontal_4_dual_neon(uint64_t a1, signed int a2, const char *a3, const char *a4, const char *a5, const char *a6, const char *a7, const char *a8, int a9)
{
  v9 = vdupq_n_s16(a9 - 8);
  v10 = vld1_dup_s8(a3);
  v11 = vld1_dup_s8(a4);
  v12 = vld1_dup_s8(a5);
  v13 = vshlq_u16(vmovl_u8(v10), v9);
  v14 = vshlq_u16(vmovl_u8(v11), v9);
  v15 = vshlq_u16(vmovl_u8(v12), v9);
  v16 = -1 << (a9 - 1);
  v17 = vdupq_n_s16(~v16);
  v18 = vdupq_n_s16(v16);
  v19 = vdupq_n_s16(-128 << (a9 - 8));
  v20 = vshl_u32(vdup_n_s32(a2), 0x100000002);
  v21 = vdupq_n_s16(128 << (a9 - 8));
  v22.i64[0] = v20.i32[0];
  v22.i64[1] = v20.i32[1];
  v23 = vnegq_s64(v22);
  v24 = v23.i64[1];
  v25 = 2 * v23.i64[0];
  v26 = (a1 + 2 * v23.i64[0]);
  v27 = *v26;
  v28 = 2 * a2;
  v29 = (v26 + v28);
  v30 = *v29;
  v31 = &v29->i8[v28];
  v32 = *v31;
  v33 = (v31 + v28);
  v34 = *v33;
  v35 = (v33 + v28);
  v36 = *v35;
  v37 = &v35->i8[v28];
  v38 = vmaxq_u16(vabdq_u16(v32, v34), vabdq_u16(*v37, v36));
  v39 = vcgtq_u16(v38, v15);
  v40 = vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v38, vabdq_u16(v27, v30)), vabdq_u16(v30, v32)), vabdq_u16(*(v37 + v28), *v37)), vabdq_u16(*(v37 + v28 + v28), *(v37 + v28)));
  v41 = vabdq_u16(v34, v36);
  v42 = vandq_s8(vcgeq_u16(v13, vsraq_n_u16(vaddq_s16(v41, v41), vabdq_u16(v32, *v37), 1uLL)), vcgeq_u16(v14, v40));
  v43 = vaddq_s16(v32, v19);
  v44 = vaddq_s16(v34, v19);
  v13.i64[0] = 0x3000300030003;
  v13.i64[1] = 0x3000300030003;
  v45 = vandq_s8(vminq_s16(vmaxq_s16(vmlaq_s16(vandq_s8(vminq_s16(vmaxq_s16(vsubq_s16(v32, *v37), v18), v17), v39), vsubq_s16(v36, v34), v13), v18), v17), v42);
  v42.i64[0] = 0x4000400040004;
  v42.i64[1] = 0x4000400040004;
  v46 = vshrq_n_s16(vminq_s16(vaddq_s16(v45, v42), v17), 3uLL);
  v47 = vminq_s16(vmaxq_s16(vsraq_n_s16(v44, vminq_s16(vaddq_s16(v45, v13), v17), 3uLL), v18), v17);
  v48 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(vaddq_s16(v36, v19), v46), v18), v17), v21);
  v49 = vbicq_s8(vrshrq_n_s16(v46, 1uLL), v39);
  v50 = vminq_s16(vmaxq_s16(vsubq_s16(vaddq_s16(*v37, v19), v49), v18), v17);
  v24 *= 2;
  *(a1 + v24) = vaddq_s16(vminq_s16(vmaxq_s16(vaddq_s16(v43, v49), v18), v17), v21);
  v51 = (a1 + v24 + v28);
  *v51 = vaddq_s16(v47, v21);
  v52 = (v51 + v28);
  *v52 = v48;
  *(v52 + v28) = vaddq_s16(v50, v21);
  *v49.i8 = vld1_dup_s8(a6);
  *v43.i8 = vld1_dup_s8(a7);
  *v47.i8 = vld1_dup_s8(a8);
  v53 = vshlq_u16(vmovl_u8(*v49.i8), v9);
  v54 = vshlq_u16(vmovl_u8(*v43.i8), v9);
  v55 = vshlq_u16(vmovl_u8(*v47.i8), v9);
  v56 = (a1 + 16 + v25);
  v57 = *v56;
  v58 = (v56 + v28);
  v59 = *v58;
  v60 = &v58->i8[v28];
  v61 = *v60;
  v62 = (v60 + v28);
  v63 = *v62;
  v64 = (v62 + v28);
  v65 = *v64;
  v66 = &v64->i8[v28];
  v67 = vmaxq_u16(vabdq_u16(v61, v63), vabdq_u16(*v66, v65));
  v68 = vcgtq_u16(v67, v55);
  v69 = vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v67, vabdq_u16(v57, v59)), vabdq_u16(v59, v61)), vabdq_u16(*(v66 + v28), *v66)), vabdq_u16(*(v66 + v28 + v28), *(v66 + v28)));
  v70 = vabdq_u16(v63, v65);
  v71 = vandq_s8(vcgeq_u16(v53, vsraq_n_u16(vaddq_s16(v70, v70), vabdq_u16(v61, *v66), 1uLL)), vcgeq_u16(v54, v69));
  v72 = vaddq_s16(v61, v19);
  v73 = vandq_s8(vminq_s16(vmaxq_s16(vmlaq_s16(vandq_s8(vminq_s16(vmaxq_s16(vsubq_s16(v61, *v66), v18), v17), v68), vsubq_s16(v65, v63), v13), v18), v17), v71);
  v74 = vshrq_n_s16(vminq_s16(vaddq_s16(v73, v42), v17), 3uLL);
  v75 = vminq_s16(vaddq_s16(v73, v13), v17);
  v76 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(vaddq_s16(v65, v19), v74), v18), v17), v21);
  v77 = vaddq_s16(vminq_s16(vmaxq_s16(vsraq_n_s16(vaddq_s16(v63, v19), v75, 3uLL), v18), v17), v21);
  v78 = vbicq_s8(vrshrq_n_s16(v74, 1uLL), v68);
  v79 = vminq_s16(vmaxq_s16(vsubq_s16(vaddq_s16(*v66, v19), v78), v18), v17);
  result = vaddq_s16(vminq_s16(vmaxq_s16(vaddq_s16(v72, v78), v18), v17), v21);
  v81 = (a1 + 16 + v24);
  *v81 = result;
  v82 = (v81 + v28);
  *v82 = v77;
  v83 = (v82 + v28);
  *v83 = v76;
  *(v83 + v28) = vaddq_s16(v79, v21);
  return result;
}

int16x8_t vpx_highbd_lpf_vertical_4_neon(uint64_t a1, int a2, const char *a3, const char *a4, const char *a5, int a6)
{
  v7 = *(a1 - 8);
  v6 = (a1 - 8);
  v8 = a2;
  v9 = *&v6[v8];
  v10 = &v6[v8 + v8];
  v11 = *v10;
  v12 = (v10 + v8 * 2);
  v13 = *v12;
  v14 = (v12 + v8 * 2);
  v15 = *v14;
  v16 = (v14 + v8 * 2);
  v17 = *v16;
  v18 = (v16 + v8 * 2);
  v19 = *(v18 + v8 * 2);
  v20 = vtrn1q_s16(v7, v9);
  v21 = vtrn2q_s16(v7, v9);
  v22 = vtrn1q_s16(v11, v13);
  v23 = vtrn2q_s16(v11, v13);
  v24 = vtrn1q_s16(v15, v17);
  v25 = vtrn2q_s16(v15, v17);
  v26 = vtrn1q_s16(*v18, v19);
  v27 = vtrn2q_s16(*v18, v19);
  v28 = vtrn1q_s32(v20, v22);
  v29 = vtrn2q_s32(v20, v22);
  v30 = vtrn1q_s32(v21, v23);
  v31 = vtrn2q_s32(v21, v23);
  v32 = vtrn1q_s32(v24, v26);
  v33 = vtrn2q_s32(v24, v26);
  v34 = vtrn1q_s32(v25, v27);
  v35 = vzip2q_s64(v28, v32);
  v28.i64[1] = v32.i64[0];
  v36 = vzip2q_s64(v30, v34);
  v30.i64[1] = v34.i64[0];
  v37 = vtrn2q_s32(v25, v27);
  v38 = vzip2q_s64(v29, v33);
  v29.i64[1] = v33.i64[0];
  v39 = vzip2q_s64(v31, v37);
  v31.i64[1] = v37.i64[0];
  v40 = vdupq_n_s16(a6 - 8);
  *v27.i8 = vld1_dup_s8(a3);
  v41 = vld1_dup_s8(a4);
  v42 = vld1_dup_s8(a5);
  v43 = vshlq_u16(vmovl_u8(*v27.i8), v40);
  v44 = vshlq_u16(vmovl_u8(v41), v40);
  v45 = vshlq_u16(vmovl_u8(v42), v40);
  v46 = vmaxq_u16(vabdq_u16(v29, v31), vabdq_u16(v36, v35));
  v47 = vcgtq_u16(v46, v45);
  v48 = vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v46, vabdq_u16(v28, v30)), vabdq_u16(v30, v29)), vabdq_u16(v38, v36)), vabdq_u16(v39, v38));
  v49 = vabdq_u16(v31, v35);
  v50 = vcgeq_u16(v43, vsraq_n_u16(vaddq_s16(v49, v49), vabdq_u16(v29, v36), 1uLL));
  v51 = -1 << (a6 - 1);
  v52 = vdupq_n_s16(~v51);
  v53 = vdupq_n_s16(v51);
  v54 = vdupq_n_s16(-128 << (a6 - 8));
  v55 = vandq_s8(v50, vcgeq_u16(v44, v48));
  v56 = vaddq_s16(v54, v29);
  v57 = vaddq_s16(v54, v31);
  v58 = vaddq_s16(v54, v35);
  v59 = vaddq_s16(v54, v36);
  v44.i64[0] = 0x3000300030003;
  v44.i64[1] = 0x3000300030003;
  v60 = vandq_s8(vminq_s16(vmaxq_s16(vmlaq_s16(vandq_s8(vminq_s16(vmaxq_s16(vsubq_s16(v56, v59), v53), v52), v47), vsubq_s16(v58, v57), v44), v53), v52), v55);
  v29.i64[0] = 0x4000400040004;
  v29.i64[1] = 0x4000400040004;
  v61 = vshrq_n_s16(vminq_s16(vaddq_s16(v60, v29), v52), 3uLL);
  v62 = vmaxq_s16(vsraq_n_s16(v57, vminq_s16(vaddq_s16(v60, v44), v52), 3uLL), v53);
  v63 = vdupq_n_s16(128 << (a6 - 8));
  v64 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(v58, v61), v53), v52), v63);
  v65 = vaddq_s16(vminq_s16(v62, v52), v63);
  v66 = vbicq_s8(vrshrq_n_s16(v61, 1uLL), v47);
  v67 = vminq_s16(vmaxq_s16(vsubq_s16(v59, v66), v53), v52);
  result = vminq_s16(vmaxq_s16(vaddq_s16(v66, v56), v53), v52);
  v69 = vaddq_s16(v67, v63);
  v70 = vaddq_s16(result, v63);
  v6[2] = v70.i16[0];
  v6[3] = v65.i16[0];
  v6[4] = v64.i16[0];
  v6[5] = v69.i16[0];
  v71 = &v6[v8 + 2];
  *v71 = v70.i16[1];
  v71[1] = v65.i16[1];
  v71[2] = v64.i16[1];
  v71[3] = v69.i16[1];
  v72 = &v71[v8];
  *v72 = v70.i16[2];
  v72[1] = v65.i16[2];
  v72[2] = v64.i16[2];
  v72[3] = v69.i16[2];
  v73 = &v72[v8];
  *v73 = v70.i16[3];
  v73[1] = v65.i16[3];
  v73[2] = v64.i16[3];
  v73[3] = v69.i16[3];
  v74 = &v73[v8];
  *v74 = v70.i16[4];
  v74[1] = v65.i16[4];
  v74[2] = v64.i16[4];
  v74[3] = v69.i16[4];
  v75 = &v74[v8];
  *v75 = v70.i16[5];
  v75[1] = v65.i16[5];
  v75[2] = v64.i16[5];
  v75[3] = v69.i16[5];
  v76 = &v75[v8];
  *v76 = v70.i16[6];
  v76[1] = v65.i16[6];
  v76[2] = v64.i16[6];
  v76[3] = v69.i16[6];
  v77 = &v76[v8];
  *v77 = v70.i16[7];
  v77[1] = v65.i16[7];
  v77[2] = v64.i16[7];
  v77[3] = v69.i16[7];
  return result;
}

int16x8_t vpx_highbd_lpf_vertical_4_dual_neon(_WORD *a1, int a2, const char *a3, const char *a4, const char *a5, const char *a6, const char *a7, const char *a8, int a9)
{
  v9 = a1 - 2;
  v10 = *(a1 - 4);
  v11 = a2;
  v12 = &a1[v11 - 4];
  v13 = *v12;
  v14 = (v12 + v11 * 2);
  v15 = *v14;
  v16 = (v14 + v11 * 2);
  v17 = *v16;
  v18 = (v16 + v11 * 2);
  v19 = *v18;
  v20 = (v18 + v11 * 2);
  v21 = *v20;
  v22 = (v20 + v11 * 2);
  v23 = *(v22 + v11 * 2);
  v24 = vtrn1q_s16(v10, v13);
  v25 = vtrn2q_s16(v10, v13);
  v26 = vtrn1q_s16(v15, v17);
  v27 = vtrn2q_s16(v15, v17);
  v28 = vtrn1q_s16(v19, v21);
  v29 = vtrn2q_s16(v19, v21);
  v30 = vtrn1q_s16(*v22, v23);
  v31 = vtrn2q_s16(*v22, v23);
  v32 = vtrn1q_s32(v24, v26);
  v33 = vtrn2q_s32(v24, v26);
  v34 = vtrn1q_s32(v25, v27);
  v35 = vtrn2q_s32(v25, v27);
  v36 = vtrn1q_s32(v28, v30);
  v37 = vtrn2q_s32(v28, v30);
  v38 = vtrn1q_s32(v29, v31);
  v39 = vtrn2q_s32(v29, v31);
  v40 = vzip2q_s64(v32, v36);
  v32.i64[1] = v36.i64[0];
  v41 = vzip2q_s64(v34, v38);
  v34.i64[1] = v38.i64[0];
  v42 = vzip2q_s64(v33, v37);
  v43.i64[0] = v33.i64[0];
  v43.i64[1] = v37.i64[0];
  v44 = vzip2q_s64(v35, v39);
  v37.i64[0] = v35.i64[0];
  v37.i64[1] = v39.i64[0];
  *v35.i8 = vld1_dup_s8(a3);
  v45 = vdupq_n_s16(a9 - 8);
  *v39.i8 = vld1_dup_s8(a4);
  v46 = vld1_dup_s8(a5);
  v47 = vmaxq_u16(vabdq_u16(v43, v37), vabdq_u16(v41, v40));
  v48 = vcgtq_u16(v47, vshlq_u16(vmovl_u8(v46), v45));
  v49 = vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v47, vabdq_u16(v32, v34)), vabdq_u16(v34, v43)), vabdq_u16(v42, v41)), vabdq_u16(v44, v42));
  v50 = vabdq_u16(v37, v40);
  v51 = vandq_s8(vcgeq_u16(vshlq_u16(vmovl_u8(*v35.i8), v45), vsraq_n_u16(vaddq_s16(v50, v50), vabdq_u16(v43, v41), 1uLL)), vcgeq_u16(vshlq_u16(vmovl_u8(*v39.i8), v45), v49));
  v52 = -1 << (a9 - 1);
  v53 = vdupq_n_s16(~v52);
  v54 = vdupq_n_s16(v52);
  v55 = vdupq_n_s16(-128 << (a9 - 8));
  v56 = vaddq_s16(v55, v43);
  v57 = vaddq_s16(v55, v37);
  v58 = vaddq_s16(v55, v40);
  v59 = vaddq_s16(v55, v41);
  v39.i64[0] = 0x3000300030003;
  v39.i64[1] = 0x3000300030003;
  v60 = vandq_s8(vminq_s16(vmaxq_s16(vmlaq_s16(vandq_s8(vminq_s16(vmaxq_s16(vsubq_s16(v56, v59), v54), v53), v48), vsubq_s16(v58, v57), v39), v54), v53), v51);
  v40.i64[0] = 0x4000400040004;
  v40.i64[1] = 0x4000400040004;
  v61 = vshrq_n_s16(vminq_s16(vaddq_s16(v60, v40), v53), 3uLL);
  v62 = vminq_s16(vmaxq_s16(vsraq_n_s16(v57, vminq_s16(vaddq_s16(v60, v39), v53), 3uLL), v54), v53);
  v63 = vdupq_n_s16(128 << (a9 - 8));
  v64 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(v58, v61), v54), v53), v63);
  v65 = vaddq_s16(v62, v63);
  v66 = vbicq_s8(vrshrq_n_s16(v61, 1uLL), v48);
  v67 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(v59, v66), v54), v53), v63);
  v68 = vaddq_s16(vminq_s16(vmaxq_s16(vaddq_s16(v66, v56), v54), v53), v63);
  *v9 = v68.i16[0];
  v9[1] = v65.i16[0];
  *a1 = v64.i16[0];
  a1[1] = v67.i16[0];
  v69 = &a1[v11 - 2];
  *v69 = v68.i16[1];
  v69[1] = v65.i16[1];
  v69[2] = v64.i16[1];
  v69[3] = v67.i16[1];
  v70 = &v69[v11];
  *v70 = v68.i16[2];
  v70[1] = v65.i16[2];
  v70[2] = v64.i16[2];
  v70[3] = v67.i16[2];
  v71 = &v70[v11];
  *v71 = v68.i16[3];
  v71[1] = v65.i16[3];
  v71[2] = v64.i16[3];
  v71[3] = v67.i16[3];
  v72 = &v71[v11];
  *v72 = v68.i16[4];
  v72[1] = v65.i16[4];
  v72[2] = v64.i16[4];
  v72[3] = v67.i16[4];
  v73 = &v72[v11];
  *v73 = v68.i16[5];
  v73[1] = v65.i16[5];
  v73[2] = v64.i16[5];
  v73[3] = v67.i16[5];
  v74 = &v73[v11];
  *v74 = v68.i16[6];
  v74[1] = v65.i16[6];
  v74[2] = v64.i16[6];
  v74[3] = v67.i16[6];
  v75 = &v74[v11];
  v76 = &a1[8 * a2];
  *v75 = v68.i16[7];
  v75[1] = v65.i16[7];
  v75[2] = v64.i16[7];
  v75[3] = v67.i16[7];
  v77 = *(v76 - 4);
  v76 -= 4;
  v78 = *&v76[v11];
  v79 = &v76[v11 + v11];
  v80 = *v79;
  v81 = (v79 + v11 * 2);
  v82 = *v81;
  v83 = (v81 + v11 * 2);
  v84 = *v83;
  v85 = (v83 + v11 * 2);
  v86 = *v85;
  v87 = (v85 + v11 * 2);
  v88 = *(v87 + v11 * 2);
  v89 = vtrn1q_s16(v77, v78);
  v90 = vtrn2q_s16(v77, v78);
  v91 = vtrn1q_s16(v80, v82);
  v92 = vtrn2q_s16(v80, v82);
  v93 = vtrn1q_s16(v84, v86);
  v94 = vtrn2q_s16(v84, v86);
  v95 = vtrn1q_s16(*v87, v88);
  v96 = vtrn2q_s16(*v87, v88);
  v97 = vtrn1q_s32(v89, v91);
  v98 = vtrn2q_s32(v89, v91);
  v99 = vtrn1q_s32(v90, v92);
  v100 = vtrn2q_s32(v90, v92);
  v101 = vtrn1q_s32(v93, v95);
  v102 = vtrn2q_s32(v93, v95);
  v103 = vtrn1q_s32(v94, v96);
  v104 = vtrn2q_s32(v94, v96);
  v105 = vzip2q_s64(v97, v101);
  v97.i64[1] = v101.i64[0];
  v106 = vzip2q_s64(v99, v103);
  v99.i64[1] = v103.i64[0];
  v107 = vzip2q_s64(v98, v102);
  v98.i64[1] = v102.i64[0];
  v108 = vzip2q_s64(v100, v104);
  v100.i64[1] = v104.i64[0];
  *v104.i8 = vld1_dup_s8(a6);
  *v67.i8 = vld1_dup_s8(a7);
  v109 = vld1_dup_s8(a8);
  v110 = vshlq_u16(vmovl_u8(*v104.i8), v45);
  v111 = vshlq_u16(vmovl_u8(*v67.i8), v45);
  v112 = vshlq_u16(vmovl_u8(v109), v45);
  v113 = vmaxq_u16(vabdq_u16(v98, v100), vabdq_u16(v106, v105));
  v114 = vcgtq_u16(v113, v112);
  v115 = vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v113, vabdq_u16(v97, v99)), vabdq_u16(v99, v98)), vabdq_u16(v107, v106)), vabdq_u16(v108, v107));
  v116 = vabdq_u16(v100, v105);
  v117 = vandq_s8(vcgeq_u16(v110, vsraq_n_u16(vaddq_s16(v116, v116), vabdq_u16(v98, v106), 1uLL)), vcgeq_u16(v111, v115));
  v118 = vaddq_s16(v55, v98);
  v119 = vaddq_s16(v55, v100);
  v120 = vaddq_s16(v55, v105);
  v121 = vaddq_s16(v55, v106);
  v122 = vandq_s8(vminq_s16(vmaxq_s16(vmlaq_s16(vandq_s8(vminq_s16(vmaxq_s16(vsubq_s16(v118, v121), v54), v53), v114), vsubq_s16(v120, v119), v39), v54), v53), v117);
  v123 = vshrq_n_s16(vminq_s16(vaddq_s16(v122, v40), v53), 3uLL);
  v124 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(v120, v123), v54), v53), v63);
  v125 = vaddq_s16(vminq_s16(vmaxq_s16(vsraq_n_s16(v119, vminq_s16(vaddq_s16(v122, v39), v53), 3uLL), v54), v53), v63);
  v126 = vbicq_s8(vrshrq_n_s16(v123, 1uLL), v114);
  v127 = vminq_s16(vmaxq_s16(vsubq_s16(v121, v126), v54), v53);
  result = vminq_s16(vmaxq_s16(vaddq_s16(v126, v118), v54), v53);
  v129 = vaddq_s16(v127, v63);
  v130 = vaddq_s16(result, v63);
  v76[2] = v130.i16[0];
  v76[3] = v125.i16[0];
  v76[4] = v124.i16[0];
  v76[5] = v129.i16[0];
  v131 = &v76[v11 + 2];
  *v131 = v130.i16[1];
  v131[1] = v125.i16[1];
  v131[2] = v124.i16[1];
  v131[3] = v129.i16[1];
  v132 = &v131[v11];
  *v132 = v130.i16[2];
  v132[1] = v125.i16[2];
  v132[2] = v124.i16[2];
  v132[3] = v129.i16[2];
  v133 = &v132[v11];
  *v133 = v130.i16[3];
  v133[1] = v125.i16[3];
  v133[2] = v124.i16[3];
  v133[3] = v129.i16[3];
  v134 = &v133[v11];
  *v134 = v130.i16[4];
  v134[1] = v125.i16[4];
  v134[2] = v124.i16[4];
  v134[3] = v129.i16[4];
  v135 = &v134[v11];
  *v135 = v130.i16[5];
  v135[1] = v125.i16[5];
  v135[2] = v124.i16[5];
  v135[3] = v129.i16[5];
  v136 = &v135[v11];
  *v136 = v130.i16[6];
  v136[1] = v125.i16[6];
  v136[2] = v124.i16[6];
  v136[3] = v129.i16[6];
  v137 = &v136[v11];
  *v137 = v130.i16[7];
  v137[1] = v125.i16[7];
  v137[2] = v124.i16[7];
  v137[3] = v129.i16[7];
  return result;
}

uint64_t vpx_highbd_lpf_horizontal_8_neon(uint64_t result, int a2, const char *a3, const char *a4, const char *a5, int a6)
{
  v6 = a6 - 8;
  v7 = vdupq_n_s16(a6 - 8);
  v8 = vld1_dup_s8(a3);
  v9 = vld1_dup_s8(a4);
  v10 = vshlq_u16(vmovl_u8(v8), v7);
  v11 = vshlq_u16(vmovl_u8(v9), v7);
  v12 = (result - 8 * a2);
  v13 = *v12;
  v14 = 2 * a2;
  v15 = (v12 + v14);
  v16 = *v15;
  v17 = (v15 + v14);
  v18 = *v17;
  v19 = (v17 + v14);
  v20 = *v19;
  v21 = (v19 + v14);
  v22 = *v21;
  v23 = (v21 + v14);
  v24 = *v23;
  v25 = &v23->i8[v14];
  v26 = *v25;
  v27 = *(v25 + v14);
  v28 = vmaxq_u16(vabdq_u16(v18, v20), vabdq_u16(v24, v22));
  v29 = vabdq_u16(v20, v22);
  v30 = vandq_s8(vcgeq_u16(v10, vsraq_n_u16(vaddq_s16(v29, v29), vabdq_u16(v18, v24), 1uLL)), vcgeq_u16(v11, vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v28, vabdq_u16(v13, v16)), vabdq_u16(v16, v18)), vabdq_u16(*v25, v24)), vabdq_u16(v27, *v25))));
  v31 = vandq_s8(vcgeq_u16(vdupq_n_s16(1 << (a6 - 8)), vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v28, vabdq_u16(v16, v20)), vabdq_u16(*v25, v22)), vabdq_u16(v13, v20)), vabdq_u16(v27, v22))), v30);
  v32 = vpaddl_u32((v31.i64[0] + vextq_s8(v31, v31, 8uLL).u64[0]));
  if (v32 == -4)
  {
    v33.i64[0] = 0x3000300030003;
    v33.i64[1] = 0x3000300030003;
    v34 = vaddq_s16(vmlaq_s16(vaddq_s16(v16, v16), v13, v33), vaddq_s16(vaddq_s16(v18, v20), v22));
    v35 = vrshrq_n_u16(v34, 3uLL);
    v36 = vaddq_s16(v24, v18);
    v37 = vaddq_s16(vsubq_s16(v36, vaddq_s16(v16, v13)), v34);
    v38 = vrshrq_n_u16(v37, 3uLL);
    v39 = vaddq_s16(vaddq_s16(vsubq_s16(v20, vaddq_s16(v18, v13)), v26), v37);
    v40 = vrshrq_n_u16(v39, 3uLL);
    v41 = vaddq_s16(vaddq_s16(vsubq_s16(v22, vaddq_s16(v20, v13)), v27), v39);
    v42 = vrshrq_n_u16(v41, 3uLL);
    v43 = vaddq_s16(vaddq_s16(vsubq_s16(v24, vaddq_s16(v22, v16)), v27), v41);
    v44 = vrshrq_n_u16(v43, 3uLL);
    v26 = vrshrq_n_u16(vaddq_s16(vaddq_s16(vsubq_s16(v26, v36), v27), v43), 3uLL);
    v16 = v35;
  }

  else
  {
    v45 = vld1_dup_s8(a5);
    v46 = vshlq_u16(vmovl_u8(v45), v7);
    v47 = -1 << (a6 - 1);
    v48 = vdupq_n_s16(~v47);
    v49 = vdupq_n_s16(v47);
    v50 = vcgtq_u16(v28, v46);
    v51 = vdupq_n_s16(-128 << v6);
    v52 = vaddq_s16(v18, v51);
    v53 = vaddq_s16(v20, v51);
    v54 = vaddq_s16(v22, v51);
    v55 = vaddq_s16(v24, v51);
    v56.i64[0] = 0x3000300030003;
    v56.i64[1] = 0x3000300030003;
    v57 = vandq_s8(vminq_s16(vmaxq_s16(vmlaq_s16(vandq_s8(vminq_s16(vmaxq_s16(vsubq_s16(v18, v24), v49), v48), v50), vsubq_s16(v22, v20), v56), v49), v48), vmovl_s8(vmovn_s16(v30)));
    v58.i64[0] = 0x4000400040004;
    v58.i64[1] = 0x4000400040004;
    v59 = vshrq_n_s16(vminq_s16(vaddq_s16(v57, v58), v48), 3uLL);
    v60 = vminq_s16(vmaxq_s16(vsraq_n_s16(v53, vminq_s16(vaddq_s16(v57, v56), v48), 3uLL), v49), v48);
    v61 = vdupq_n_s16(128 << v6);
    v42 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(v54, v59), v49), v48), v61);
    v40 = vaddq_s16(v60, v61);
    v62 = vbicq_s8(vrshrq_n_s16(v59, 1uLL), v50);
    v63 = vminq_s16(vmaxq_s16(vsubq_s16(v55, v62), v49), v48);
    v64 = vminq_s16(vmaxq_s16(vaddq_s16(v52, v62), v49), v48);
    v44 = vaddq_s16(v63, v61);
    v38 = vaddq_s16(v64, v61);
    if (v32)
    {
      v65 = vaddq_s16(vmlaq_s16(vaddq_s16(v16, v16), v13, v56), vaddq_s16(vaddq_s16(v18, v20), v22));
      v66 = vrshrq_n_u16(v65, 3uLL);
      v67 = vaddq_s16(v24, v18);
      v68 = vaddq_s16(vsubq_s16(v67, vaddq_s16(v16, v13)), v65);
      v69 = vaddq_s16(vaddq_s16(vsubq_s16(v20, vaddq_s16(v18, v13)), v26), v68);
      v70 = vaddq_s16(vaddq_s16(vsubq_s16(v22, vaddq_s16(v20, v13)), v27), v69);
      v71 = vrshrq_n_u16(v70, 3uLL);
      v72 = vaddq_s16(vaddq_s16(vsubq_s16(v24, vaddq_s16(v22, v16)), v27), v70);
      v73 = vrshrq_n_u16(v72, 3uLL);
      v74 = vrshrq_n_u16(vaddq_s16(vaddq_s16(vsubq_s16(v26, v67), v27), v72), 3uLL);
      v75 = vmovl_s8(vmovn_s16(v31));
      v16 = vbslq_s8(v75, v66, v16);
      v38 = vbslq_s8(v75, vrshrq_n_u16(v68, 3uLL), v38);
      v40 = vbslq_s8(v75, vrshrq_n_u16(v69, 3uLL), v40);
      v42 = vbslq_s8(v75, v71, v42);
      v44 = vbslq_s8(v75, v73, v44);
      v26 = vbslq_s8(v75, v74, v26);
    }
  }

  v76 = (result - 6 * a2);
  *v76 = v16;
  v77 = 2 * a2;
  v78 = &v76->i8[v77];
  *v78 = v38;
  v79 = (v78 + v77);
  *v79 = v40;
  v80 = (v79 + v77);
  *v80 = v42;
  v81 = (v80 + v77);
  *v81 = v44;
  *(v81 + v77) = v26;
  return result;
}

_WORD *vpx_highbd_lpf_vertical_8_neon(_WORD *a1, int a2, const char *a3, const char *a4, const char *a5, int a6)
{
  v6 = *(a1 - 4);
  v7 = 2 * a2;
  v8 = &a1[v7 / 2 - 4];
  v9 = *v8;
  v10 = (v8 + v7);
  v11 = *v10;
  v12 = (v10 + v7);
  v13 = *v12;
  v14 = (v12 + v7);
  v15 = *v14;
  v16 = (v14 + v7);
  v17 = *v16;
  v18 = (v16 + v7);
  v19 = *(v18 + v7);
  v20 = vtrn1q_s16(v6, v9);
  v21 = vtrn2q_s16(v6, v9);
  v22 = vtrn1q_s16(v11, v13);
  v23 = vtrn2q_s16(v11, v13);
  v24 = vtrn1q_s16(v15, v17);
  v25 = vtrn2q_s16(v15, v17);
  v26 = vtrn1q_s16(*v18, v19);
  v27 = vtrn2q_s16(*v18, v19);
  v28 = vtrn1q_s32(v20, v22);
  v39 = vtrn2q_s32(v20, v22);
  v29 = vtrn1q_s32(v21, v23);
  v41 = vtrn2q_s32(v21, v23);
  v30 = vtrn1q_s32(v24, v26);
  v31 = vtrn2q_s32(v24, v26);
  v32 = vtrn1q_s32(v25, v27);
  v33 = vtrn2q_s32(v25, v27);
  v34 = vzip2q_s64(v28, v30);
  v35.i64[0] = v28.i64[0];
  v35.i64[1] = v30.i64[0];
  v36 = vzip2q_s64(v29, v32);
  v37.i64[0] = v29.i64[0];
  v37.i64[1] = v32.i64[0];
  v38 = vzip2q_s64(v39, v31);
  v39.i64[1] = v31.i64[0];
  v40 = vzip2q_s64(v41, v33);
  v41.i64[1] = v33.i64[0];
  v42 = a6 - 8;
  v43 = vdupq_n_s16(a6 - 8);
  *v33.i8 = vld1_dup_s8(a3);
  v44 = vld1_dup_s8(a4);
  v45 = vshlq_u16(vmovl_u8(v44), v43);
  v46 = vmaxq_u16(vabdq_u16(v39, v41), vabdq_u16(v36, v34));
  v47 = vabdq_u16(v41, v34);
  v48 = vandq_s8(vcgeq_u16(vshlq_u16(vmovl_u8(*v33.i8), v43), vsraq_n_u16(vaddq_s16(v47, v47), vabdq_u16(v39, v36), 1uLL)), vcgeq_u16(v45, vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v46, vabdq_u16(v35, v37)), vabdq_u16(v37, v39)), vabdq_u16(v38, v36)), vabdq_u16(v40, v38))));
  v49 = vandq_s8(vcgeq_u16(vdupq_n_s16(1 << (a6 - 8)), vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v46, vabdq_u16(v37, v41)), vabdq_u16(v38, v34)), vabdq_u16(v35, v41)), vabdq_u16(v40, v34))), v48);
  v50 = vpaddl_u32((v49.i64[0] + vextq_s8(v49, v49, 8uLL).u64[0]));
  if (v50 == -4)
  {
    v51.i64[0] = 0x3000300030003;
    v51.i64[1] = 0x3000300030003;
    v52 = vaddq_s16(vmlaq_s16(vaddq_s16(vaddq_s16(v41, v39), v34), v35, v51), vaddq_s16(v37, v37));
    v53 = vrshrq_n_u16(v52, 3uLL);
    v54 = vaddq_s16(v36, v39);
    v55 = vaddq_s16(vsubq_s16(v54, vaddq_s16(v35, v37)), v52);
    v56 = vrshrq_n_u16(v55, 3uLL);
    v57 = vaddq_s16(vaddq_s16(vsubq_s16(v41, vaddq_s16(v39, v35)), v38), v55);
    v58 = vrshrq_n_u16(v57, 3uLL);
    v59 = vaddq_s16(vaddq_s16(vsubq_s16(v34, vaddq_s16(v41, v35)), v40), v57);
    v60 = vrshrq_n_u16(v59, 3uLL);
    v61 = vaddq_s16(vaddq_s16(vsubq_s16(v36, vaddq_s16(v34, v37)), v40), v59);
    v62 = vrshrq_n_u16(v61, 3uLL);
    v38 = vrshrq_n_u16(vaddq_s16(vsubq_s16(vaddq_s16(v40, v38), v54), v61), 3uLL);
    v37 = v53;
  }

  else
  {
    v63 = vld1_dup_s8(a5);
    v64 = vshlq_u16(vmovl_u8(v63), v43);
    v65 = -1 << (a6 - 1);
    v66 = vdupq_n_s16(~v65);
    v67 = vdupq_n_s16(v65);
    v68 = vcgtq_u16(v46, v64);
    v69 = vdupq_n_s16(-128 << v42);
    v70 = vaddq_s16(v69, v39);
    v71 = vaddq_s16(v69, v41);
    v72 = vaddq_s16(v69, v34);
    v73 = vaddq_s16(v69, v36);
    v74.i64[0] = 0x3000300030003;
    v74.i64[1] = 0x3000300030003;
    v75 = vandq_s8(vminq_s16(vmaxq_s16(vmlaq_s16(vandq_s8(vminq_s16(vmaxq_s16(vsubq_s16(v70, v73), v67), v66), v68), vsubq_s16(v72, v71), v74), v67), v66), vmovl_s8(vmovn_s16(v48)));
    v76.i64[0] = 0x4000400040004;
    v76.i64[1] = 0x4000400040004;
    v77 = vshrq_n_s16(vminq_s16(vaddq_s16(v75, v76), v66), 3uLL);
    v78 = vminq_s16(vmaxq_s16(vsraq_n_s16(v71, vminq_s16(vaddq_s16(v75, v74), v66), 3uLL), v67), v66);
    v79 = vdupq_n_s16(128 << v42);
    v60 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(v72, v77), v67), v66), v79);
    v58 = vaddq_s16(v78, v79);
    v80 = vbicq_s8(vrshrq_n_s16(v77, 1uLL), v68);
    v62 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(v73, v80), v67), v66), v79);
    v56 = vaddq_s16(vminq_s16(vmaxq_s16(vaddq_s16(v80, v70), v67), v66), v79);
    if (v50)
    {
      v81 = vaddq_s16(vmlaq_s16(vaddq_s16(vaddq_s16(v41, v39), v34), v35, v74), vaddq_s16(v37, v37));
      v82 = vrshrq_n_u16(v81, 3uLL);
      v83 = vaddq_s16(v36, v39);
      v84 = vaddq_s16(vsubq_s16(v83, vaddq_s16(v35, v37)), v81);
      v85 = vaddq_s16(vaddq_s16(vsubq_s16(v41, vaddq_s16(v39, v35)), v38), v84);
      v86 = vaddq_s16(vaddq_s16(vsubq_s16(v34, vaddq_s16(v41, v35)), v40), v85);
      v87 = vrshrq_n_u16(v86, 3uLL);
      v88 = vaddq_s16(vaddq_s16(vsubq_s16(v36, vaddq_s16(v34, v37)), v40), v86);
      v89 = vmovl_s8(vmovn_s16(v49));
      v37 = vbslq_s8(v89, v82, v37);
      v56 = vbslq_s8(v89, vrshrq_n_u16(v84, 3uLL), v56);
      v58 = vbslq_s8(v89, vrshrq_n_u16(v85, 3uLL), v58);
      v60 = vbslq_s8(v89, v87, v60);
      v62 = vbslq_s8(v89, vrshrq_n_u16(v88, 3uLL), v62);
      v38 = vbslq_s8(v89, vrshrq_n_u16(vaddq_s16(vsubq_s16(vaddq_s16(v40, v38), v83), v88), 3uLL), v38);
    }
  }

  v90 = a1 - 3;
  *v90 = v37.i16[0];
  v90[1] = v56.i16[0];
  v90[2] = v58.i16[0];
  v91 = a2;
  *a1 = v60.i16[0];
  a1[1] = v62.i16[0];
  a1[2] = v38.i16[0];
  v92 = &a1[v91];
  v93 = v92 - 3;
  *v93 = v37.i16[1];
  v93[1] = v56.i16[1];
  v93[2] = v58.i16[1];
  *v92 = v60.i16[1];
  v92[1] = v62.i16[1];
  v92[2] = v38.i16[1];
  v94 = &v92[v91];
  v95 = v94 - 3;
  *v95 = v37.i16[2];
  v95[1] = v56.i16[2];
  v95[2] = v58.i16[2];
  *v94 = v60.i16[2];
  v94[1] = v62.i16[2];
  v94[2] = v38.i16[2];
  v96 = &v94[v91];
  v97 = v96 - 3;
  *v97 = v37.i16[3];
  v97[1] = v56.i16[3];
  v97[2] = v58.i16[3];
  *v96 = v60.i16[3];
  v96[1] = v62.i16[3];
  v96[2] = v38.i16[3];
  v98 = &v96[v91];
  v99 = v98 - 3;
  *v99 = v37.i16[4];
  v99[1] = v56.i16[4];
  v99[2] = v58.i16[4];
  *v98 = v60.i16[4];
  v98[1] = v62.i16[4];
  v98[2] = v38.i16[4];
  v100 = &v98[v91];
  v101 = v100 - 3;
  *v101 = v37.i16[5];
  v101[1] = v56.i16[5];
  v101[2] = v58.i16[5];
  *v100 = v60.i16[5];
  v100[1] = v62.i16[5];
  v100[2] = v38.i16[5];
  v102 = &v100[v91];
  v103 = v102 - 3;
  *v103 = v37.i16[6];
  v103[1] = v56.i16[6];
  v103[2] = v58.i16[6];
  *v102 = v60.i16[6];
  v102[1] = v62.i16[6];
  v102[2] = v38.i16[6];
  result = &v102[v91];
  v105 = result - 3;
  *v105 = v37.i16[7];
  v105[1] = v56.i16[7];
  v105[2] = v58.i16[7];
  *result = v60.i16[7];
  result[1] = v62.i16[7];
  result[2] = v38.i16[7];
  return result;
}

void lpf_horizontal_16_kernel(int8x16_t *a1, int a2, char a3, uint16x8_t a4, uint16x8_t a5, uint16x8_t a6)
{
  v75 = 0u;
  v76 = 0u;
  v73 = 0u;
  v74 = 0u;
  v71 = 0u;
  v72 = 0u;
  v69 = 0u;
  v70 = 0u;
  v67 = 0u;
  v68 = 0u;
  v65 = 0u;
  v66 = 0u;
  v63 = 0u;
  v64 = 0u;
  v8 = 8 * a2;
  v9 = &a1[-a2];
  v10 = *v9;
  v11 = a2;
  v12 = 2 * a2;
  v13 = (v9 + v12);
  v14 = *v13;
  v15 = (v13 + v12);
  v16 = *v15;
  v17 = (v15 + v12);
  v18 = *v17;
  v19 = (v17 + v12);
  v20 = *v19;
  v21 = (v19 + v12);
  v22 = *v21;
  v23 = (v21 + v12);
  v24 = *v23;
  v25 = (v23 + v12);
  v26 = *v25;
  v27 = (v25 + v12);
  v28 = *v27;
  v29 = (v27 + v12);
  v30 = *v29;
  v31 = (v29 + v12);
  v32 = *v31;
  v33 = (v31 + v12);
  v34 = *v33;
  v35 = (v33 + v12);
  v36 = *v35;
  v37 = (v35 + v12);
  v38 = *v37;
  v39 = &v37->i8[v12];
  v40 = vmaxq_u16(vabdq_u16(v24, v26), vabdq_u16(v30, v28));
  v41 = vcgtq_u16(v40, a6);
  v42 = vabdq_u16(v26, v28);
  v43 = vandq_s8(vcgeq_u16(a4, vsraq_n_u16(vaddq_s16(v42, v42), vabdq_u16(v24, v30), 1uLL)), vcgeq_u16(a5, vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v40, vabdq_u16(v20, v22)), vabdq_u16(v22, v24)), vabdq_u16(v32, v30)), vabdq_u16(v34, v32))));
  v44 = vdupq_n_s16(1 << (a3 - 8));
  v45 = vandq_s8(vcgeq_u16(v44, vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v40, vabdq_u16(v22, v26)), vabdq_u16(v32, v28)), vabdq_u16(v20, v26)), vabdq_u16(v34, v28))), v43);
  v46 = vpaddl_u32((v45.i64[0] + vextq_s8(v45, v45, 8uLL).u64[0]));
  v47 = vbicq_s8(v45, vcgtq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(vabdq_u16(v10, v26), vabdq_u16(v14, v26)), vabdq_u16(v16, v26)), vabdq_u16(v18, v26)), vabdq_u16(v36, v28)), vabdq_u16(v38, v28)), vabdq_u16(*v39, v28)), vabdq_u16(*(v39 + v12), v28)), v44));
  v48 = vpaddl_u32((v47.i64[0] + vextq_s8(v47, v47, 8uLL).u64[0]));
  filter16(v46, v48, &v76, &v75, &v74, &v73, &v72, &v71, v43, v45, v47, v41, v10, v14, v16, v18, v20, v22, v24, v26, v28, v30, v32, v34, v36, v38, *v39, *(v39 + v12), &v70, &v69, &v68, &v67, &v66, &v65, &v64, &v63, a3);
  v49 = v70;
  v50 = v71;
  v52 = v68;
  v51 = v69;
  if (v46)
  {
    v53 = v72;
    v54 = v67;
    if (v48)
    {
      v55 = v75;
      v57 = v73;
      v56 = v74;
      v59 = v65;
      v58 = v66;
      v60 = 3 * v11;
      *(a1 - 2 * (v8 - a2)) = v76;
      *(a1 - 12 * v11) = v55;
      *(a1 - 10 * v11) = v56;
      *(a1 - 8 * v11) = v57;
      *(a1 + 6 * v11) = v58;
      *(a1 + 8 * v11) = v59;
      v61 = v63;
      *(a1 + 10 * v11) = v64;
      *(a1 + 12 * v11) = v61;
    }

    else
    {
      v60 = 3 * a2;
    }

    *(a1 - 2 * v60) = v53;
    *(a1 + 4 * v11) = v54;
    v62 = 2 * v11;
  }

  else
  {
    v62 = 2 * a2;
  }

  *(a1 - 2 * v62) = v50;
  *(a1 - 2 * v11) = v49;
  *a1 = v51;
  *(a1 + 2 * v11) = v52;
}

__n128 lpf_vertical_16_kernel(int16x8_t *a1, int a2, char a3, uint16x8_t a4, uint16x8_t a5, uint16x8_t a6)
{
  v163 = 0u;
  v164 = 0u;
  v161 = 0u;
  v162 = 0u;
  v159 = 0u;
  v160 = 0u;
  v157 = 0u;
  v158 = 0u;
  v155 = 0u;
  v156 = 0u;
  v153 = 0u;
  v154 = 0u;
  v7 = a1[-1];
  v152 = 0u;
  v8 = 2 * a2;
  v9 = (a1 + v8 - 16);
  v10 = *v9;
  v11 = (v9 + v8);
  v12 = *v11;
  v13 = (v11 + v8);
  v14 = *v13;
  v15 = (v13 + v8);
  v16 = *v15;
  v17 = (v15 + v8);
  v18 = *v17;
  v19 = (v17 + v8);
  v20 = *(v19 + v8);
  v21 = vtrn1q_s16(v7, v10);
  v22 = vtrn2q_s16(v7, v10);
  v23 = vtrn1q_s16(v12, v14);
  v24 = vtrn2q_s16(v12, v14);
  v25 = vtrn1q_s16(v16, v18);
  v26 = vtrn2q_s16(v16, v18);
  v27 = vtrn1q_s16(*v19, v20);
  v28 = vtrn2q_s16(*v19, v20);
  v29 = vtrn1q_s32(v21, v23);
  v30 = vtrn2q_s32(v21, v23);
  v31 = vtrn1q_s32(v22, v24);
  v32 = vtrn2q_s32(v22, v24);
  v33 = vtrn1q_s32(v25, v27);
  v34 = vtrn2q_s32(v25, v27);
  v35 = vtrn1q_s32(v26, v28);
  v36 = vtrn2q_s32(v26, v28);
  v37 = vzip2q_s64(v29, v33);
  v22.i64[0] = v29.i64[0];
  v22.i64[1] = v33.i64[0];
  v38 = vzip2q_s64(v31, v35);
  v31.i64[1] = v35.i64[0];
  v39 = vzip2q_s64(v30, v34);
  v33.i64[0] = v30.i64[0];
  v33.i64[1] = v34.i64[0];
  v40 = vzip2q_s64(v32, v36);
  v34.i64[0] = v32.i64[0];
  v34.i64[1] = v36.i64[0];
  v151 = 0u;
  v41 = &a1->i16[v8 / 2];
  v42 = *(a1 + v8);
  v43 = (a1 + v8 + v8);
  v44 = &v43->i16[v8 / 2];
  v45 = *(v43 + v8);
  v46 = (v43 + v8 + v8);
  v47 = &v46->i16[v8 / 2];
  v48 = *(v46 + v8);
  v49 = (v46 + v8 + v8);
  v150 = &v49->i16[v8 / 2];
  v50 = *(v49 + v8);
  v51 = vtrn1q_s16(*a1, v42);
  v52 = vtrn2q_s16(*a1, v42);
  v53 = vtrn1q_s16(*v43, v45);
  v54 = vtrn2q_s16(*v43, v45);
  v55 = vtrn1q_s16(*v46, v48);
  v56 = vtrn2q_s16(*v46, v48);
  v57 = vtrn1q_s16(*v49, v50);
  v58 = vtrn2q_s16(*v49, v50);
  v59 = vtrn1q_s32(v51, v53);
  v60 = vtrn2q_s32(v51, v53);
  v61 = vtrn1q_s32(v52, v54);
  v62 = vtrn2q_s32(v52, v54);
  v63 = vtrn1q_s32(v55, v57);
  v64 = vtrn2q_s32(v55, v57);
  v65 = vtrn1q_s32(v56, v58);
  v66 = vtrn2q_s32(v56, v58);
  v67 = vzip2q_s64(v59, v63);
  v53.i64[0] = v59.i64[0];
  v53.i64[1] = v63.i64[0];
  v68 = vzip2q_s64(v61, v65);
  v63.i64[0] = v61.i64[0];
  v63.i64[1] = v65.i64[0];
  v69 = vzip2q_s64(v60, v64);
  v65.i64[0] = v60.i64[0];
  v65.i64[1] = v64.i64[0];
  v70 = vzip2q_s64(v62, v66);
  v64.i64[0] = v62.i64[0];
  v64.i64[1] = v66.i64[0];
  v71 = a2;
  v72 = vmaxq_u16(vabdq_u16(v39, v40), vabdq_u16(v63, v53));
  v73 = vcgtq_u16(v72, a6);
  v74 = vabdq_u16(v40, v53);
  v75 = vandq_s8(vcgeq_u16(a4, vsraq_n_u16(vaddq_s16(v74, v74), vabdq_u16(v39, v63), 1uLL)), vcgeq_u16(a5, vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v72, vabdq_u16(v37, v38)), vabdq_u16(v38, v39)), vabdq_u16(v65, v63)), vabdq_u16(v64, v65))));
  v76 = vdupq_n_s16(1 << (a3 - 8));
  v77 = vandq_s8(vcgeq_u16(v76, vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(v72, vabdq_u16(v38, v40)), vabdq_u16(v65, v53)), vabdq_u16(v37, v40)), vabdq_u16(v64, v53))), v75);
  v78 = vpaddl_u32((v77.i64[0] + vextq_s8(v77, v77, 8uLL).u64[0]));
  v79 = vbicq_s8(v77, vcgtq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(vmaxq_u16(vabdq_u16(v22, v40), vabdq_u16(v31, v40)), vabdq_u16(v33, v40)), vabdq_u16(v34, v40)), vabdq_u16(v67, v53)), vabdq_u16(v68, v53)), vabdq_u16(v69, v53)), vabdq_u16(v70, v53)), v76));
  v80 = vpaddl_u32((v79.i64[0] + vextq_s8(v79, v79, 8uLL).u64[0]));
  filter16(v78, v80, &v164, &v163, &v162, &v161, &v160, &v159, v75, v77, v79, v73, v22, v31, v33, v34, v37, v38, v39, v40, v53, v63, v65, v64, v67, v68, v69, v70, &v158, &v157, &v156, &v155, &v154, &v153, &v152, &v151, a3);
  if (v78)
  {
    if (v80)
    {
      v81 = &a1[-1].i16[5];
      v83 = v163;
      v82 = v164;
      v85 = v161;
      v84 = v162;
      v87 = v159;
      v86 = v160;
      v88 = &a1[-1].i16[1];
      *v88 = v164.i16[0];
      v88[1] = v83.i16[0];
      v88[2] = v84.i16[0];
      v88[3] = v85.i16[0];
      v89 = v158;
      *v81 = v86.n128_u16[0];
      v81[1] = v87.n128_u16[0];
      v81[2] = v89.i16[0];
      v90 = &a1[-1].i16[v71 + 5];
      v91 = v90 - 4;
      *v91 = v82.i16[1];
      v91[1] = v83.i16[1];
      v91[2] = v84.i16[1];
      v91[3] = v85.i16[1];
      *v90 = v86.n128_i16[1];
      v90[1] = v87.n128_i16[1];
      v90[2] = v89.i16[1];
      v92 = &v90[v71];
      v93 = v92 - 4;
      *v93 = v82.i16[2];
      v93[1] = v83.i16[2];
      v93[2] = v84.i16[2];
      v93[3] = v85.i16[2];
      *v92 = v86.n128_i16[2];
      v92[1] = v87.n128_i16[2];
      v92[2] = v89.i16[2];
      v94 = &v92[v71];
      v95 = v94 - 4;
      *v95 = v82.i16[3];
      v95[1] = v83.i16[3];
      v95[2] = v84.i16[3];
      v95[3] = v85.i16[3];
      *v94 = v86.n128_i16[3];
      v94[1] = v87.n128_i16[3];
      v94[2] = v89.i16[3];
      v96 = &v94[v71];
      v97 = v96 - 4;
      *v97 = v82.i16[4];
      v97[1] = v83.i16[4];
      v97[2] = v84.i16[4];
      v97[3] = v85.i16[4];
      *v96 = v86.n128_i16[4];
      v96[1] = v87.n128_i16[4];
      v96[2] = v89.i16[4];
      v98 = &v96[v71];
      v99 = v98 - 4;
      *v99 = v82.i16[5];
      v99[1] = v83.i16[5];
      v99[2] = v84.i16[5];
      v99[3] = v85.i16[5];
      *v98 = v86.n128_i16[5];
      v98[1] = v87.n128_i16[5];
      v98[2] = v89.i16[5];
      v100 = &v98[v71];
      v101 = v100 - 4;
      *v101 = v82.i16[6];
      v101[1] = v83.i16[6];
      v101[2] = v84.i16[6];
      v101[3] = v85.i16[6];
      *v100 = v86.n128_i16[6];
      v100[1] = v87.n128_i16[6];
      v100[2] = v89.i16[6];
      v102 = &v100[v71];
      v103 = v102 - 4;
      *v103 = v82.i16[7];
      v103[1] = v83.i16[7];
      v103[2] = v84.i16[7];
      v103[3] = v85.i16[7];
      *v102 = v86.n128_i16[7];
      v102[1] = v87.n128_i16[7];
      v102[2] = v89.i16[7];
      v105 = v156;
      result = v157;
      v106 = v155;
      v108 = v153;
      v107 = v154;
      a1->i16[0] = v157.n128_u16[0];
      a1->i16[1] = v105.i16[0];
      a1->i16[2] = v106.i16[0];
      a1->i16[3] = v107.i16[0];
      v109 = &a1->i16[4];
      v111 = v151;
      v110 = v152;
      *v109 = v108.i16[0];
      v109[1] = v110.i16[0];
      v109[2] = v111.i16[0];
      v112 = &v109[v71];
      v113 = v112 - 4;
      *v113 = result.n128_i16[1];
      v113[1] = v105.i16[1];
      v113[2] = v106.i16[1];
      v113[3] = v107.i16[1];
      *v112 = v108.i16[1];
      v112[1] = v110.i16[1];
      v112[2] = v111.i16[1];
      v114 = &v112[v71];
      v115 = v114 - 4;
      *v115 = result.n128_i16[2];
      v115[1] = v105.i16[2];
      v115[2] = v106.i16[2];
      v115[3] = v107.i16[2];
      *v114 = v108.i16[2];
      v114[1] = v110.i16[2];
      v114[2] = v111.i16[2];
      v116 = &v114[v71];
      v117 = v116 - 4;
      *v117 = result.n128_i16[3];
      v117[1] = v105.i16[3];
      v117[2] = v106.i16[3];
      v117[3] = v107.i16[3];
      *v116 = v108.i16[3];
      v116[1] = v110.i16[3];
      v116[2] = v111.i16[3];
      v118 = &v116[v71];
      v119 = v118 - 4;
      *v119 = result.n128_i16[4];
      v119[1] = v105.i16[4];
      v119[2] = v106.i16[4];
      v119[3] = v107.i16[4];
      *v118 = v108.i16[4];
      v118[1] = v110.i16[4];
      v118[2] = v111.i16[4];
      v120 = &v118[v71];
      v121 = v120 - 4;
      *v121 = result.n128_i16[5];
      v121[1] = v105.i16[5];
      v121[2] = v106.i16[5];
      v121[3] = v107.i16[5];
      *v120 = v108.i16[5];
      v120[1] = v110.i16[5];
      v120[2] = v111.i16[5];
      v122 = &v120[v71];
      v123 = v122 - 4;
      *v123 = result.n128_i16[6];
      v123[1] = v105.i16[6];
      v123[2] = v106.i16[6];
      v123[3] = v107.i16[6];
      *v122 = v108.i16[6];
      v122[1] = v110.i16[6];
      v122[2] = v111.i16[6];
      v124 = &v122[v71];
      v125 = v124 - 4;
      *v125 = result.n128_u16[7];
      v125[1] = v105.i16[7];
      v125[2] = v106.i16[7];
      v125[3] = v107.i16[7];
      *v124 = v108.i16[7];
      v124[1] = v110.i16[7];
      v124[2] = v111.i16[7];
    }

    else
    {
      v137 = v159;
      result = v160;
      v139 = v157;
      v138 = v158;
      v141 = v155;
      v140 = v156;
      v142 = &a1[-1].i16[5];
      *v142 = v160.n128_u16[0];
      v142[1] = v137.n128_u16[0];
      v142[2] = v138.i16[0];
      a1->i16[0] = v139.n128_u16[0];
      a1->i16[1] = v140.i16[0];
      a1->i16[2] = v141.i16[0];
      v143 = v41 - 3;
      *v143 = result.n128_i16[1];
      v143[1] = v137.n128_i16[1];
      v143[2] = v138.i16[1];
      *v41 = v139.n128_i16[1];
      v41[1] = v140.i16[1];
      v41[2] = v141.i16[1];
      v144 = &v43[-1].i16[5];
      *v144 = result.n128_i16[2];
      v144[1] = v137.n128_i16[2];
      v144[2] = v138.i16[2];
      v43->i16[0] = v139.n128_i16[2];
      v43->i16[1] = v140.i16[2];
      v43->i16[2] = v141.i16[2];
      v145 = v44 - 3;
      *v145 = result.n128_i16[3];
      v145[1] = v137.n128_i16[3];
      v145[2] = v138.i16[3];
      *v44 = v139.n128_i16[3];
      v44[1] = v140.i16[3];
      v44[2] = v141.i16[3];
      v146 = &v46[-1].i16[5];
      *v146 = result.n128_i16[4];
      v146[1] = v137.n128_i16[4];
      v146[2] = v138.i16[4];
      v46->i16[0] = v139.n128_i16[4];
      v46->i16[1] = v140.i16[4];
      v46->i16[2] = v141.i16[4];
      v147 = v47 - 3;
      *v147 = result.n128_i16[5];
      v147[1] = v137.n128_i16[5];
      v147[2] = v138.i16[5];
      *v47 = v139.n128_i16[5];
      v47[1] = v140.i16[5];
      v47[2] = v141.i16[5];
      v148 = &v49[-1].i16[5];
      *v148 = result.n128_i16[6];
      v148[1] = v137.n128_i16[6];
      v148[2] = v138.i16[6];
      v49->i16[0] = v139.n128_i16[6];
      v49->i16[1] = v140.i16[6];
      v49->i16[2] = v141.i16[6];
      v149 = v150 - 3;
      *v149 = result.n128_i16[7];
      v149[1] = v137.n128_i16[7];
      v149[2] = v138.i16[7];
      *v150 = v139.n128_i16[7];
      v150[1] = v140.i16[7];
      v150[2] = v141.i16[7];
    }
  }

  else
  {
    v126 = &a1[-1].i16[6];
    v127 = v158;
    result = v159;
    v129 = v156;
    v128 = v157;
    *v126 = v159.n128_u16[0];
    v126[1] = v127.i16[0];
    a1->i16[0] = v128.n128_u16[0];
    a1->i16[1] = v129.i16[0];
    v130 = &a1[-1].i16[v71 + 6];
    *v130 = result.n128_i16[1];
    v130[1] = v127.i16[1];
    v130[2] = v128.n128_i16[1];
    v130[3] = v129.i16[1];
    v131 = &v130[v71];
    *v131 = result.n128_i16[2];
    v131[1] = v127.i16[2];
    v131[2] = v128.n128_i16[2];
    v131[3] = v129.i16[2];
    v132 = &v131[v71];
    *v132 = result.n128_i16[3];
    v132[1] = v127.i16[3];
    v132[2] = v128.n128_i16[3];
    v132[3] = v129.i16[3];
    v133 = &v132[v71];
    *v133 = result.n128_i16[4];
    v133[1] = v127.i16[4];
    v133[2] = v128.n128_i16[4];
    v133[3] = v129.i16[4];
    v134 = &v133[v71];
    *v134 = result.n128_i16[5];
    v134[1] = v127.i16[5];
    v134[2] = v128.n128_i16[5];
    v134[3] = v129.i16[5];
    v135 = &v134[v71];
    *v135 = result.n128_i16[6];
    v135[1] = v127.i16[6];
    v135[2] = v128.n128_i16[6];
    v135[3] = v129.i16[6];
    v136 = &v135[v71];
    *v136 = result.n128_i16[7];
    v136[1] = v127.i16[7];
    v136[2] = v128.n128_i16[7];
    v136[3] = v129.i16[7];
  }

  return result;
}

int8x16_t filter16(int a1, int a2, int8x16_t *a3, int8x16_t *a4, int8x16_t *a5, int8x16_t *a6, int8x16_t *a7, int8x16_t *a8, int8x16_t a9, int8x16_t a10, int8x16_t a11, int8x16_t a12, int16x8_t a13, int16x8_t a14, int16x8_t a15, int16x8_t a16, int16x8_t a17, int16x8_t a18, int16x8_t a19, int16x8_t a20, int16x8_t a21, int16x8_t a22, int16x8_t a23, int16x8_t a24, int16x8_t a25, int16x8_t a26, int16x8_t a27, int16x8_t a28, int8x16_t *a29, int8x16_t *a30, int8x16_t *a31, int8x16_t *a32, int8x16_t *a33, int8x16_t *a34, int8x16_t *a35, int8x16_t *a36, char a37)
{
  if (a1 == -4 || (v37 = -1 << (a37 - 1), v38 = vdupq_n_s16(~v37), v39 = vdupq_n_s16(v37), v40 = vdupq_n_s16(-128 << (a37 - 8)), v41 = vaddq_s16(v40, a19), v42 = vaddq_s16(v40, a20), v43 = vaddq_s16(v40, a21), v44 = vaddq_s16(v40, a22), v45.i64[0] = 0x3000300030003, v45.i64[1] = 0x3000300030003, v46 = vandq_s8(vminq_s16(vmaxq_s16(vmlaq_s16(vandq_s8(vminq_s16(vmaxq_s16(vsubq_s16(v41, v44), v39), v38), a12), vsubq_s16(v43, v42), v45), v39), v38), a9), v47.i64[0] = 0x4000400040004, v47.i64[1] = 0x4000400040004, v48 = vshrq_n_s16(vminq_s16(vaddq_s16(v46, v47), v38), 3uLL), v49 = vminq_s16(vmaxq_s16(vsraq_n_s16(v42, vminq_s16(vaddq_s16(v46, v45), v38), 3uLL), v39), v38), v50 = vdupq_n_s16(128 << (a37 - 8)), *a30 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(v43, v48), v39), v38), v50), *a29 = vaddq_s16(v49, v50), v51 = vbicq_s8(vrshrq_n_s16(v48, 1uLL), a12), *a31 = vaddq_s16(vminq_s16(vmaxq_s16(vsubq_s16(v44, v51), v39), v38), v50), result = vaddq_s16(vminq_s16(vmaxq_s16(vaddq_s16(v51, v41), v39), v38), v50), *a8 = result, a1))
  {
    result = a18;
    *a7 = a18;
    *a32 = a23;
    if (a2 == -4 || (v53.i64[0] = 0x3000300030003, v53.i64[1] = 0x3000300030003, v54 = vaddq_s16(vmlaq_s16(vaddq_s16(a18, a18), a17, v53), vaddq_s16(vaddq_s16(a19, a20), a21)), *a7 = vrshrq_n_u16(v54, 3uLL), v55 = vaddq_s16(vaddq_s16(vsubq_s16(a19, vaddq_s16(a18, a17)), a22), v54), v56 = vrshrq_n_u16(v55, 3uLL), v57 = vaddq_s16(vaddq_s16(vsubq_s16(a20, vaddq_s16(a19, a17)), a23), v55), v58 = vrshrq_n_u16(v57, 3uLL), v59 = vaddq_s16(vaddq_s16(vsubq_s16(a21, vaddq_s16(a20, a17)), a24), v57), v60 = vrshrq_n_u16(v59, 3uLL), v61 = vaddq_s16(vaddq_s16(vsubq_s16(a22, vaddq_s16(a21, a18)), a24), v59), *a32 = vrshrq_n_u16(vaddq_s16(vaddq_s16(vsubq_s16(a23, vaddq_s16(a22, a19)), a24), v61), 3uLL), *a7 = vbslq_s8(a10, *a7, a18), *a8 = vbslq_s8(a10, v56, *a8), *a29 = vbslq_s8(a10, v58, *a29), *a30 = vbslq_s8(a10, v60, *a30), *a31 = vbslq_s8(a10, vrshrq_n_u16(v61, 3uLL), *a31), *a32 = vbslq_s8(a10, *a32, a23), a2))
    {
      v62.i64[0] = 0x7000700070007;
      v62.i64[1] = 0x7000700070007;
      v63 = vaddq_s16(vaddq_s16(vaddq_s16(vmlaq_s16(vaddq_s16(a14, a14), a13, v62), vaddq_s16(vaddq_s16(a15, a16), a17)), vaddq_s16(vaddq_s16(a18, a19), a20)), a21);
      *a3 = vbslq_s8(a11, vrshrq_n_u16(v63, 4uLL), a14);
      v64 = vaddq_s16(vaddq_s16(vsubq_s16(a15, vaddq_s16(a14, a13)), a22), v63);
      *a4 = vbslq_s8(a11, vrshrq_n_u16(v64, 4uLL), a15);
      v65 = vaddq_s16(vaddq_s16(vsubq_s16(a16, vaddq_s16(a15, a13)), a23), v64);
      *a5 = vbslq_s8(a11, vrshrq_n_u16(v65, 4uLL), a16);
      v66 = vaddq_s16(vaddq_s16(vsubq_s16(a17, vaddq_s16(a16, a13)), a24), v65);
      *a6 = vbslq_s8(a11, vrshrq_n_u16(v66, 4uLL), a17);
      v67 = vaddq_s16(vaddq_s16(vsubq_s16(a18, vaddq_s16(a17, a13)), a25), v66);
      *a7 = vbslq_s8(a11, vrshrq_n_u16(v67, 4uLL), *a7);
      v68 = vaddq_s16(vaddq_s16(vsubq_s16(a19, vaddq_s16(a18, a13)), a26), v67);
      *a8 = vbslq_s8(a11, vrshrq_n_u16(v68, 4uLL), *a8);
      v69 = vaddq_s16(vaddq_s16(vsubq_s16(a20, vaddq_s16(a19, a13)), a27), v68);
      *a29 = vbslq_s8(a11, vrshrq_n_u16(v69, 4uLL), *a29);
      v70 = vaddq_s16(vaddq_s16(vsubq_s16(a21, vaddq_s16(a20, a13)), a28), v69);
      *a30 = vbslq_s8(a11, vrshrq_n_u16(v70, 4uLL), *a30);
      v71 = vaddq_s16(vaddq_s16(vsubq_s16(a22, vaddq_s16(a21, a14)), a28), v70);
      *a31 = vbslq_s8(a11, vrshrq_n_u16(v71, 4uLL), *a31);
      v72 = vaddq_s16(vaddq_s16(vsubq_s16(a23, vaddq_s16(a22, a15)), a28), v71);
      *a32 = vbslq_s8(a11, vrshrq_n_u16(v72, 4uLL), *a32);
      v73 = vaddq_s16(vaddq_s16(vsubq_s16(a24, vaddq_s16(a23, a16)), a28), v72);
      *a33 = vbslq_s8(a11, vrshrq_n_u16(v73, 4uLL), a24);
      v74 = vaddq_s16(vaddq_s16(vsubq_s16(a25, vaddq_s16(a24, a17)), a28), v73);
      *a34 = vbslq_s8(a11, vrshrq_n_u16(v74, 4uLL), a25);
      v75 = vaddq_s16(vaddq_s16(vsubq_s16(a26, vaddq_s16(a25, a18)), a28), v74);
      *a35 = vbslq_s8(a11, vrshrq_n_u16(v75, 4uLL), a26);
      result = vbslq_s8(a11, vrshrq_n_u16(vaddq_s16(vaddq_s16(vsubq_s16(a27, vaddq_s16(a26, a19)), a28), v75), 4uLL), a27);
      *a36 = result;
    }
  }

  return result;
}

unsigned __int16 vpx_highbd_quantize_b_neon@<H0>(int32x4_t *_X0@<X0>, uint64_t a2@<X1>, int16x4_t **a3@<X2>, int16x8_t *a4@<X3>, int32x4_t *a5@<X4>, int16x4_t *a6@<X5>, unsigned __int16 *a7@<X6>, uint64_t a8@<X7>)
{
  v8 = *(a8 + 8);
  v9 = vmovl_s16(*a3[1033]);
  v10 = *a3[1034];
  v11 = vshll_n_s16(*a3[1031], 0xFuLL);
  v12 = vshll_n_s16(*a3[1032], 0xFuLL);
  v13 = *a6;
  v14 = vmovl_s16(*a6);
  v16 = *v8;
  v15 = v8 + 1;
  v17 = _X0[1];
  v18 = vcltzq_s32(*_X0);
  v19 = vcltzq_s32(v17);
  v20 = vabsq_s32(*_X0);
  v21 = vabsq_s32(v17);
  v22 = vcgtq_s32(v9, v20);
  v23 = vdupq_lane_s32(*v9.i8, 1);
  v24 = vaddw_s16(v20, v10);
  v25 = vdupq_lane_s32(*&vmovl_s16(v10), 1);
  v26 = vaddq_s32(v21, v25);
  v27 = vbicq_s8(vsubq_s32(veorq_s8(vqdmulhq_s32(vaddq_s32(vqdmulhq_s32(v24, v11), v24), v12), v18), v18), v22);
  v28 = vbicq_s8(vsubq_s32(veorq_s8(vqdmulhq_lane_s32(vaddq_s32(vqdmulhq_lane_s32(v26, *v11.i8, 1), v26), *v12.i8, 1), v19), v19), vcgtq_s32(v23, v21));
  *a4 = v27;
  a4[1] = v28;
  v29 = vdupq_lane_s32(*v14.i8, 1);
  *a5 = vmulq_s32(v27, v14);
  a5[1] = vmulq_s32(v28, vmovl_s16(vdup_lane_s16(v13, 1)));
  v30 = vbicq_s8(v16, vceqzq_s16(vuzp1q_s16(v27, v28)));
  __asm { PRFM            #0, [X0,#0x100] }

  _X9 = _X0 + 18;
  v37 = a5 + 3;
  v38 = a4 + 3;
  v39 = a2 + 8;
  do
  {
    v40 = *v15++;
    v41 = _X9[-16];
    v42 = _X9[-15];
    v43 = vcltzq_s32(v41);
    v44 = vcltzq_s32(v42);
    v45 = vabsq_s32(v41);
    v46 = vabsq_s32(v42);
    v47 = vcgtq_s32(v23, v45);
    v48 = vcgtq_s32(v23, v46);
    v49 = vaddq_s32(v45, v25);
    v50 = vaddq_s32(v46, v25);
    v51 = vbicq_s8(vsubq_s32(veorq_s8(vqdmulhq_lane_s32(vaddq_s32(vqdmulhq_lane_s32(v49, *v11.i8, 1), v49), *v12.i8, 1), v43), v43), v47);
    v52 = vbicq_s8(vsubq_s32(veorq_s8(vqdmulhq_lane_s32(vaddq_s32(vqdmulhq_lane_s32(v50, *v11.i8, 1), v50), *v12.i8, 1), v44), v44), v48);
    v38[-1] = v51;
    *v38 = v52;
    v37[-1] = vmulq_s32(v51, v29);
    *v37 = vmulq_s32(v52, v29);
    v30 = vmaxq_u16(v30, vbicq_s8(v40, vceqzq_s16(vuzp1q_s16(v51, v52))));
    __asm { PRFM            #0, [X9] }

    _X9 += 2;
    v37 += 2;
    v38 += 2;
    v39 -= 8;
  }

  while (v39 > 16);
  result = vmaxvq_u16(v30);
  *a7 = result;
  return result;
}

unsigned __int16 vpx_highbd_quantize_b_32x32_neon@<H0>(int32x4_t *_X0@<X0>, int16x4_t **a2@<X1>, int16x8_t *a3@<X2>, int32x4_t *a4@<X3>, int16x4_t *a5@<X4>, unsigned __int16 *a6@<X5>, uint64_t a7@<X6>)
{
  v7 = 0;
  v8 = *(a7 + 8);
  v9 = vrshrq_n_s32(vmovl_s16(*a2[1033]), 1uLL);
  v10 = vmovl_s16(*a2[1034]);
  v11 = vshll_n_s16(*a2[1031], 0xFuLL);
  v12 = vshll_n_s16(*a2[1032], 0x10uLL);
  v13 = *a5;
  v15 = *v8;
  v14 = v8 + 1;
  v16 = vmovl_s16(*a5);
  v17 = _X0[1];
  v18 = vcltzq_s32(*_X0);
  v19 = vcltzq_s32(v17);
  v20 = vabsq_s32(*_X0);
  v21 = vabsq_s32(v17);
  v22 = vcgtq_s32(v9, v20);
  v23 = vdupq_lane_s32(*v9.i8, 1);
  v24 = vrsraq_n_s32(v20, v10, 1uLL);
  v25 = vdupq_lane_s32(*&vrshrq_n_s32(v10, 1uLL), 1);
  v26 = vaddq_s32(v21, v25);
  v27 = vbicq_s8(vsubq_s32(veorq_s8(vqdmulhq_s32(vaddq_s32(vqdmulhq_s32(v24, v11), v24), v12), v18), v18), v22);
  v28 = vbicq_s8(vsubq_s32(veorq_s8(vqdmulhq_lane_s32(vaddq_s32(vqdmulhq_lane_s32(v26, *v11.i8, 1), v26), *v12.i8, 1), v19), v19), vcgtq_s32(v23, v21));
  *a3 = v27;
  a3[1] = v28;
  v29 = vmulq_s32(v27, v16);
  v30 = vdupq_lane_s32(*v16.i8, 1);
  v31 = vmulq_s32(v28, vmovl_s16(vdup_lane_s16(v13, 1)));
  *a4 = vshrq_n_s32(vsraq_n_u32(v29, v29, 0x1FuLL), 1uLL);
  a4[1] = vshrq_n_s32(vsraq_n_u32(v31, v31, 0x1FuLL), 1uLL);
  v32 = vbicq_s8(v15, vceqzq_s16(vuzp1q_s16(v27, v28)));
  __asm { PRFM            #0, [X0,#0x100] }

  do
  {
    v38 = &a4[v7 + 2];
    v39 = &a3[v7 + 2];
    _X14 = &_X0[v7];
    v41 = *v14++;
    v42 = _X0[v7 + 2];
    v43 = _X0[v7 + 3];
    v44 = vcltzq_s32(v42);
    v45 = vcltzq_s32(v43);
    v46 = vabsq_s32(v42);
    v47 = vabsq_s32(v43);
    v48 = vcgtq_s32(v23, v46);
    v49 = vcgtq_s32(v23, v47);
    v50 = vaddq_s32(v46, v25);
    v51 = vaddq_s32(v47, v25);
    v52 = vbicq_s8(vsubq_s32(veorq_s8(vqdmulhq_lane_s32(vaddq_s32(vqdmulhq_lane_s32(v50, *v11.i8, 1), v50), *v12.i8, 1), v44), v44), v48);
    v53 = vbicq_s8(vsubq_s32(veorq_s8(vqdmulhq_lane_s32(vaddq_s32(vqdmulhq_lane_s32(v51, *v11.i8, 1), v51), *v12.i8, 1), v45), v45), v49);
    *v39 = v52;
    v39[1] = v53;
    v54 = vmulq_s32(v52, v30);
    v55 = vmulq_s32(v53, v30);
    *v38 = vshrq_n_s32(vsraq_n_u32(v54, v54, 0x1FuLL), 1uLL);
    v38[1] = vshrq_n_s32(vsraq_n_u32(v55, v55, 0x1FuLL), 1uLL);
    v32 = vmaxq_u16(v32, vbicq_s8(v41, vceqzq_s16(vuzp1q_s16(v52, v53))));
    __asm { PRFM            #0, [X14,#0x120] }

    v7 += 2;
  }

  while ((v7 * 16) != 4064);
  result = vmaxvq_u16(v32);
  *a6 = result;
  return result;
}

int32x4_t vpx_highbd_sad4x4x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = (2 * *a3);
  v6 = (2 * a3[1]);
  v7 = (2 * a3[2]);
  v8 = (2 * a3[3]);
  v9 = *(2 * a1);
  v10 = *(2 * a1 + 2 * a2);
  v11 = 2 * a4;
  v12 = vabal_u16(vabdl_u16(v10, *(v5 + v11)), v9, *v5);
  v13 = vabal_u16(vabdl_u16(v10, *(v6 + v11)), v9, *v6);
  v14 = vabal_u16(vabdl_u16(v10, *(v7 + v11)), v9, *v7);
  v15 = vabdl_u16(v10, *(v8 + v11));
  v16 = *(2 * a1 + 4 * a2);
  v17 = 4 * a4;
  v18 = vabal_u16(vabal_u16(v15, v9, *v8), v16, *(v8 + v17));
  v19 = *(2 * a1 + 6 * a2);
  v20 = 6 * a4;
  result = vpaddq_s32(vpaddq_s32(vabal_u16(vabal_u16(v12, v16, *(v5 + v17)), v19, *(v5 + v20)), vabal_u16(vabal_u16(v13, v16, *(v6 + v17)), v19, *(v6 + v20))), vpaddq_s32(vabal_u16(vabal_u16(v14, v16, *(v7 + v17)), v19, *(v7 + v20)), vabal_u16(v18, v19, *(v8 + v20))));
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad4x8x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = (2 * *a3);
  v6 = (2 * a3[1]);
  v7 = (2 * a3[2]);
  v8 = (2 * a3[3]);
  v9 = *(2 * a1);
  v10 = *(2 * a1 + 2 * a2);
  v11 = 2 * a4;
  v12 = vabal_u16(vabdl_u16(v10, *(v5 + v11)), v9, *v5);
  v13 = vabal_u16(vabdl_u16(v10, *(v6 + v11)), v9, *v6);
  v14 = vabal_u16(vabdl_u16(v10, *(v7 + v11)), v9, *v7);
  v15 = vabdl_u16(v10, *(v8 + v11));
  v16 = *(2 * a1 + 4 * a2);
  v17 = 4 * a4;
  v18 = *(2 * a1 + 6 * a2);
  v19 = vabal_u16(vabal_u16(v15, v9, *v8), v16, *(v8 + v17));
  v20 = vabal_u16(vabal_u16(v12, v16, *(v5 + v17)), v18, *(v5 + 6 * a4));
  v21 = vabal_u16(vabal_u16(v13, v16, *(v6 + v17)), v18, *(v6 + 6 * a4));
  v22 = vabal_u16(vabal_u16(v14, v16, *(v7 + v17)), v18, *(v7 + 6 * a4));
  v23 = *(2 * a1 + 8 * a2);
  v24 = *(2 * a1 + 10 * a2);
  v25 = 10 * a4;
  v26 = vabal_u16(vabal_u16(v19, v18, *(v8 + 6 * a4)), v23, v8[a4]);
  v27 = vabal_u16(vabal_u16(v20, v23, v5[a4]), v24, *(v5 + v25));
  v28 = vabal_u16(vabal_u16(v21, v23, v6[a4]), v24, *(v6 + v25));
  v29 = vabal_u16(vabal_u16(v22, v23, v7[a4]), v24, *(v7 + v25));
  v30 = *(2 * a1 + 12 * a2);
  v31 = vabal_u16(vabal_u16(v26, v24, *(v8 + v25)), v30, *(v8 + 12 * a4));
  v32 = *(2 * a1 + 14 * a2);
  v33 = 14 * a4;
  result = vpaddq_s32(vpaddq_s32(vabal_u16(vabal_u16(v27, v30, *(v5 + 12 * a4)), v32, *(v5 + v33)), vabal_u16(vabal_u16(v28, v30, *(v6 + 12 * a4)), v32, *(v6 + v33))), vpaddq_s32(vabal_u16(vabal_u16(v29, v30, *(v7 + 12 * a4)), v32, *(v7 + v33)), vabal_u16(v31, v32, *(v8 + v33))));
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad8x8x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = (2 * *a3);
  v6 = (2 * a3[1]);
  v7 = (2 * a3[2]);
  v8 = (2 * a3[3]);
  v9 = *(2 * a1);
  v10 = *(2 * a1 + 2 * a2);
  v11 = 2 * a4;
  v12 = vabaq_u16(vabdq_u16(v10, *(v5 + v11)), v9, *v5);
  v13 = vabaq_u16(vabdq_u16(v10, *(v6 + v11)), v9, *v6);
  v14 = vabaq_u16(vabdq_u16(v10, *(v7 + v11)), v9, *v7);
  v15 = vabaq_u16(vabdq_u16(v10, *(v8 + v11)), v9, *v8);
  v16 = *(2 * a1 + 4 * a2);
  v17 = 4 * a4;
  v18 = vabaq_u16(v12, v16, *(v5 + v17));
  v19 = vabaq_u16(v13, v16, *(v6 + v17));
  v20 = vabaq_u16(v14, v16, *(v7 + v17));
  v21 = vabaq_u16(v15, v16, *(v8 + v17));
  v22 = *(2 * a1 + 6 * a2);
  v23 = *(2 * a1 + 8 * a2);
  v24 = 8 * a4;
  v25 = vabaq_u16(vabaq_u16(v19, v22, *(v6 + 6 * a4)), v23, *(v6 + v24));
  v26 = *(2 * a1 + 10 * a2);
  v27 = 10 * a4;
  v28 = vabaq_u16(vabaq_u16(vabaq_u16(v18, v22, *(v5 + 6 * a4)), v23, *(v5 + v24)), v26, *(v5 + v27));
  v29 = vabaq_u16(vabaq_u16(vabaq_u16(v20, v22, *(v7 + 6 * a4)), v23, *(v7 + v24)), v26, *(v7 + v27));
  v30 = vabaq_u16(vabaq_u16(vabaq_u16(v21, v22, *(v8 + 6 * a4)), v23, *(v8 + v24)), v26, *(v8 + v27));
  v31 = *(2 * a1 + 12 * a2);
  v32 = vabaq_u16(v28, v31, *(v5 + 12 * a4));
  v33 = vabaq_u16(vabaq_u16(v25, v26, *(v6 + v27)), v31, *(v6 + 12 * a4));
  v34 = vabaq_u16(v29, v31, *(v7 + 12 * a4));
  v35 = vabaq_u16(v30, v31, *(v8 + 12 * a4));
  v36 = *(2 * a1 + 14 * a2);
  result = vpaddq_s32(vpaddq_s32(vpaddlq_u16(vabaq_u16(v32, v36, *(v5 + 14 * a4))), vpaddlq_u16(vabaq_u16(v33, v36, *(v6 + 14 * a4)))), vpaddq_s32(vpaddlq_u16(vabaq_u16(v34, v36, *(v7 + 14 * a4))), vpaddlq_u16(vabaq_u16(v35, v36, *(v8 + 14 * a4)))));
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad16x8x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 2 * a1;
  v6 = (2 * *a3);
  v7 = (2 * a3[1]);
  v8 = (2 * a3[2]);
  v9 = (2 * a3[3]);
  v10 = *(2 * a1);
  v11 = *(2 * a1 + 0x10);
  v69 = vabdq_u16(v11, v6[1]);
  v74 = vabdq_u16(v11, v7[1]);
  v78 = vabdq_u16(v11, v8[1]);
  v79 = vabdq_u16(v11, v9[1]);
  v12 = 2 * a1 + 2 * a2;
  v13 = 2 * a4;
  v14 = *(v12 + 0x10);
  v63 = vabdq_u16(*v12, *(v6 + v13));
  v70 = vabdq_u16(*v12, *(v7 + v13));
  v75 = vabdq_u16(*v12, *(v8 + v13));
  v61 = vabdq_u16(v14, *(&v6[1] + v13));
  v68 = vabdq_u16(v14, *(&v7[1] + v13));
  v73 = vabdq_u16(v14, *(&v8[1] + v13));
  v76 = vabdq_u16(v14, *(&v9[1] + v13));
  v77 = vabdq_u16(*v12, *(v9 + v13));
  v15 = 2 * a1 + 4 * a2;
  v16 = 4 * a4;
  v17 = *(v15 + 0x10);
  v55 = vabdq_u16(*v15, *(v6 + v16));
  v62 = vabdq_u16(*v15, *(v7 + v16));
  v72 = vabdq_u16(*v15, *(v9 + v16));
  v53 = vabdq_u16(v17, *(&v6[1] + v16));
  v60 = vabdq_u16(v17, *(&v7[1] + v16));
  v66 = vabdq_u16(v17, *(&v8[1] + v16));
  v67 = vabdq_u16(*v15, *(v8 + v16));
  v71 = vabdq_u16(v17, *(&v9[1] + v16));
  v18 = 2 * a1 + 6 * a2;
  v19 = 6 * a4;
  v20 = *(v18 + 16);
  v21 = vabdq_u16(*v18, *(v6 + v19));
  v54 = vabdq_u16(*v18, *(v7 + v19));
  v65 = vabdq_u16(*v18, *(v9 + v19));
  v22 = vabdq_u16(v20, *(&v6[1] + v19));
  v51 = vabdq_u16(v20, *(&v7[1] + v19));
  v58 = vabdq_u16(v20, *(&v8[1] + v19));
  v59 = vabdq_u16(*v18, *(v8 + v19));
  v64 = vabdq_u16(v20, *(&v9[1] + v19));
  v23 = (v5 + 8 * a2);
  v24 = 8 * a4;
  v25 = v23[1];
  v26 = vabdq_u16(*v23, *(v6 + v24));
  v27 = vabdq_u16(*v23, *(v7 + v24));
  v52 = vabdq_u16(*v23, *(v8 + v24));
  v28 = vabdq_u16(v25, *(&v6[1] + v24));
  v29 = vabdq_u16(v25, *(&v7[1] + v24));
  v50 = vabdq_u16(v25, *(&v8[1] + v24));
  v56 = vabdq_u16(v25, *(&v9[1] + v24));
  v57 = vabdq_u16(*v23, *(v9 + v24));
  v30 = (v5 + 10 * a2);
  v31 = 10 * a4;
  v32 = v30[1];
  v33 = *(&v6[1] + v31);
  v34 = vabdq_u16(*v30, *(v6 + v31));
  v35 = *(&v7[1] + v31);
  v36 = vabdq_u16(*v30, *(v7 + v31));
  v37 = *(&v8[1] + v31);
  v38 = vabdq_u16(*v30, *(v8 + v31));
  v39 = (v9 + v31);
  v40 = vabdq_u16(*v30, *v39);
  v41 = vabdq_u16(v32, v39[1]);
  v42 = (v5 + 12 * a2);
  v43 = 12 * a4;
  v44 = v42[1];
  v45 = (v5 + 14 * a2);
  v46 = 14 * a4;
  v47 = v45[1];
  v48 = vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(v74), vabdq_u16(v10, *v7)), v70), v68), v62), v60), v54), v51), v27), v29), v36), vabdq_u16(v32, v35)), vabdq_u16(*v42, *(v7 + v43))), vabdq_u16(v44, *(&v7[1] + v43))), vabdq_u16(*v45, *(v7 + v46))), vabdq_u16(v47, *(&v7[1] + v46)));
  result = vpaddq_s32(vpaddq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(v69), vabdq_u16(v10, *v6)), v63), v61), v55), v53), v21), v22), v26), v28), v34), vabdq_u16(v32, v33)), vabdq_u16(*v42, *(v6 + v43))), vabdq_u16(v44, *(&v6[1] + v43))), vabdq_u16(*v45, *(v6 + v46))), vabdq_u16(v47, *(&v6[1] + v46))), v48), vpaddq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(v78), vabdq_u16(v10, *v8)), v75), v73), v67), v66), v59), v58), v52), v50), v38), vabdq_u16(v32, v37)), vabdq_u16(*v42, *(v8 + v43))), vabdq_u16(v44, *(&v8[1] + v43))), vabdq_u16(*v45, *(v8 + v46))), vabdq_u16(v47, *(&v8[1] + v46))), vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(v79), vabdq_u16(v10, *v9)), v77), v76), v72), v71), v65), v64), v57), v56), v40), v41), vabdq_u16(*v42, *(v9 + v43))), vabdq_u16(v44, *(&v9[1] + v43))), vabdq_u16(*v45, *(v9 + v46))), vabdq_u16(v47, *(&v9[1] + v46)))));
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad32x16x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[3];
  v7 = 2 * a1 + 32;
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * *a3 + 48;
  v11 = 2 * a3[1] + 48;
  v12 = 2 * a3[2] + 32;
  v13 = 16;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  v19 = 0uLL;
  do
  {
    v20 = *(v7 - 32);
    v21 = *(v7 - 16);
    v22 = vpadalq_u16(v15, vabdq_u16(v21, *(v10 + v5 - 32)));
    v23 = vpadalq_u16(v14, vabdq_u16(v21, *(v11 + v5 - 32)));
    v24 = vpadalq_u16(v9, vabdq_u16(v21, *(v12 + v5 - 16)));
    v25 = vpadalq_u16(v8, vabdq_u16(v21, *(v6 + v5 + 16)));
    v26 = *(v7 + 16);
    v16 = vpadalq_u16(vpadalq_u16(v16, vabdq_u16(v20, *(2 * *a3 + v5))), vabdq_u16(*v7, *(2 * *a3 + 48 + v5 - 16)));
    v17 = vpadalq_u16(vpadalq_u16(v17, vabdq_u16(v20, *(2 * a3[1] + v5))), vabdq_u16(*v7, *(2 * a3[1] + 48 + v5 - 16)));
    v18 = vpadalq_u16(vpadalq_u16(v18, vabdq_u16(v20, *(2 * a3[2] + v5))), vabdq_u16(*v7, *(2 * a3[2] + 32 + v5)));
    v19 = vpadalq_u16(vpadalq_u16(v19, vabdq_u16(v20, *(v6 + v5))), vabdq_u16(*v7, *(v6 + v5 + 32)));
    v15 = vpadalq_u16(v22, vabdq_u16(v26, *(v10 + v5)));
    v14 = vpadalq_u16(v23, vabdq_u16(v26, *(v11 + v5)));
    v9 = vpadalq_u16(v24, vabdq_u16(v26, *(v12 + v5 + 16)));
    v8 = vpadalq_u16(v25, vabdq_u16(v26, *(v6 + v5 + 48)));
    v7 += 2 * a2;
    v5 += 2 * a4;
    --v13;
  }

  while (v13);
  result = vpaddq_s32(vpaddq_s32(vaddq_s32(v15, v16), vaddq_s32(v14, v17)), vpaddq_s32(vaddq_s32(v9, v18), vaddq_s32(v8, v19)));
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad32x32x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[3];
  v7 = 2 * a1 + 32;
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * *a3 + 48;
  v11 = 2 * a3[1] + 48;
  v12 = 2 * a3[2] + 32;
  v13 = 32;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  v19 = 0uLL;
  do
  {
    v20 = *(v7 - 32);
    v21 = *(v7 - 16);
    v22 = vpadalq_u16(v15, vabdq_u16(v21, *(v10 + v5 - 32)));
    v23 = vpadalq_u16(v14, vabdq_u16(v21, *(v11 + v5 - 32)));
    v24 = vpadalq_u16(v9, vabdq_u16(v21, *(v12 + v5 - 16)));
    v25 = vpadalq_u16(v8, vabdq_u16(v21, *(v6 + v5 + 16)));
    v26 = *(v7 + 16);
    v16 = vpadalq_u16(vpadalq_u16(v16, vabdq_u16(v20, *(2 * *a3 + v5))), vabdq_u16(*v7, *(2 * *a3 + 48 + v5 - 16)));
    v17 = vpadalq_u16(vpadalq_u16(v17, vabdq_u16(v20, *(2 * a3[1] + v5))), vabdq_u16(*v7, *(2 * a3[1] + 48 + v5 - 16)));
    v18 = vpadalq_u16(vpadalq_u16(v18, vabdq_u16(v20, *(2 * a3[2] + v5))), vabdq_u16(*v7, *(2 * a3[2] + 32 + v5)));
    v19 = vpadalq_u16(vpadalq_u16(v19, vabdq_u16(v20, *(v6 + v5))), vabdq_u16(*v7, *(v6 + v5 + 32)));
    v15 = vpadalq_u16(v22, vabdq_u16(v26, *(v10 + v5)));
    v14 = vpadalq_u16(v23, vabdq_u16(v26, *(v11 + v5)));
    v9 = vpadalq_u16(v24, vabdq_u16(v26, *(v12 + v5 + 16)));
    v8 = vpadalq_u16(v25, vabdq_u16(v26, *(v6 + v5 + 48)));
    v7 += 2 * a2;
    v5 += 2 * a4;
    --v13;
  }

  while (v13);
  result = vpaddq_s32(vpaddq_s32(vaddq_s32(v15, v16), vaddq_s32(v14, v17)), vpaddq_s32(vaddq_s32(v9, v18), vaddq_s32(v8, v19)));
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad32x64x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[3];
  v7 = 2 * a1 + 32;
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * *a3 + 48;
  v11 = 2 * a3[1] + 48;
  v12 = 2 * a3[2] + 32;
  v13 = 64;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  v19 = 0uLL;
  do
  {
    v20 = *(v7 - 32);
    v21 = *(v7 - 16);
    v22 = vpadalq_u16(v15, vabdq_u16(v21, *(v10 + v5 - 32)));
    v23 = vpadalq_u16(v14, vabdq_u16(v21, *(v11 + v5 - 32)));
    v24 = vpadalq_u16(v9, vabdq_u16(v21, *(v12 + v5 - 16)));
    v25 = vpadalq_u16(v8, vabdq_u16(v21, *(v6 + v5 + 16)));
    v26 = *(v7 + 16);
    v16 = vpadalq_u16(vpadalq_u16(v16, vabdq_u16(v20, *(2 * *a3 + v5))), vabdq_u16(*v7, *(2 * *a3 + 48 + v5 - 16)));
    v17 = vpadalq_u16(vpadalq_u16(v17, vabdq_u16(v20, *(2 * a3[1] + v5))), vabdq_u16(*v7, *(2 * a3[1] + 48 + v5 - 16)));
    v18 = vpadalq_u16(vpadalq_u16(v18, vabdq_u16(v20, *(2 * a3[2] + v5))), vabdq_u16(*v7, *(2 * a3[2] + 32 + v5)));
    v19 = vpadalq_u16(vpadalq_u16(v19, vabdq_u16(v20, *(v6 + v5))), vabdq_u16(*v7, *(v6 + v5 + 32)));
    v15 = vpadalq_u16(v22, vabdq_u16(v26, *(v10 + v5)));
    v14 = vpadalq_u16(v23, vabdq_u16(v26, *(v11 + v5)));
    v9 = vpadalq_u16(v24, vabdq_u16(v26, *(v12 + v5 + 16)));
    v8 = vpadalq_u16(v25, vabdq_u16(v26, *(v6 + v5 + 48)));
    v7 += 2 * a2;
    v5 += 2 * a4;
    --v13;
  }

  while (v13);
  result = vpaddq_s32(vpaddq_s32(vaddq_s32(v15, v16), vaddq_s32(v14, v17)), vpaddq_s32(vaddq_s32(v9, v18), vaddq_s32(v8, v19)));
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad64x32x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[3];
  v7 = 2 * a1 + 64;
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * *a3 + 112;
  v11 = 2 * a3[1] + 112;
  v12 = 2 * a3[2] + 112;
  v13 = 32;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  v19 = 0uLL;
  do
  {
    v20 = *(v7 - 64);
    v21 = *(v7 - 48);
    v22 = vpadalq_u16(v16, vabdq_u16(v20, *(2 * *a3 + v5)));
    v23 = vpadalq_u16(v17, vabdq_u16(v20, *(2 * a3[1] + v5)));
    v24 = vpadalq_u16(v18, vabdq_u16(v20, *(2 * a3[2] + v5)));
    v25 = vpadalq_u16(v19, vabdq_u16(v20, *(v6 + v5)));
    v26 = vpadalq_u16(v15, vabdq_u16(v21, *(v10 + v5 - 96)));
    v27 = vpadalq_u16(v14, vabdq_u16(v21, *(v11 + v5 - 96)));
    v28 = vpadalq_u16(v9, vabdq_u16(v21, *(v12 + v5 - 96)));
    v29 = vpadalq_u16(v8, vabdq_u16(v21, *(v6 + v5 + 16)));
    v30 = *(v7 - 32);
    v31 = *(v7 - 16);
    v32 = vpadalq_u16(v26, vabdq_u16(v31, *(v10 + v5 - 64)));
    v33 = vpadalq_u16(v27, vabdq_u16(v31, *(v11 + v5 - 64)));
    v34 = vpadalq_u16(v28, vabdq_u16(v31, *(v12 + v5 - 64)));
    v35 = vpadalq_u16(v29, vabdq_u16(v31, *(v6 + v5 + 48)));
    v36 = *(v7 + 16);
    v37 = vpadalq_u16(vpadalq_u16(v22, vabdq_u16(v30, *(v10 + v5 - 80))), vabdq_u16(*v7, *(v10 + v5 - 48)));
    v38 = vpadalq_u16(vpadalq_u16(v23, vabdq_u16(v30, *(v11 + v5 - 80))), vabdq_u16(*v7, *(v11 + v5 - 48)));
    v39 = vpadalq_u16(vpadalq_u16(v24, vabdq_u16(v30, *(v12 + v5 - 80))), vabdq_u16(*v7, *(v12 + v5 - 48)));
    v40 = vpadalq_u16(vpadalq_u16(v25, vabdq_u16(v30, *(v6 + v5 + 32))), vabdq_u16(*v7, *(v6 + v5 + 64)));
    v41 = vpadalq_u16(v32, vabdq_u16(v36, *(v10 + v5 - 32)));
    v42 = vpadalq_u16(v33, vabdq_u16(v36, *(v11 + v5 - 32)));
    v43 = vpadalq_u16(v34, vabdq_u16(v36, *(v12 + v5 - 32)));
    v44 = vpadalq_u16(v35, vabdq_u16(v36, *(v6 + v5 + 80)));
    v45 = *(v7 + 32);
    v46 = *(v7 + 48);
    v16 = vpadalq_u16(v37, vabdq_u16(v45, *(v10 + v5 - 16)));
    v17 = vpadalq_u16(v38, vabdq_u16(v45, *(v11 + v5 - 16)));
    v18 = vpadalq_u16(v39, vabdq_u16(v45, *(v12 + v5 - 16)));
    v47 = *(v6 + v5 + 112);
    v19 = vpadalq_u16(v40, vabdq_u16(v45, *(v6 + v5 + 96)));
    v15 = vpadalq_u16(v41, vabdq_u16(v46, *(v10 + v5)));
    v14 = vpadalq_u16(v42, vabdq_u16(v46, *(v11 + v5)));
    v9 = vpadalq_u16(v43, vabdq_u16(v46, *(v12 + v5)));
    v7 += 2 * a2;
    v5 += 2 * a4;
    v8 = vpadalq_u16(v44, vabdq_u16(v46, v47));
    --v13;
  }

  while (v13);
  result = vpaddq_s32(vpaddq_s32(vaddq_s32(v15, v16), vaddq_s32(v14, v17)), vpaddq_s32(vaddq_s32(v9, v18), vaddq_s32(v8, v19)));
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad64x64x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[3];
  v7 = 2 * a1 + 64;
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * *a3 + 112;
  v11 = 2 * a3[1] + 112;
  v12 = 2 * a3[2] + 112;
  v13 = 64;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  v19 = 0uLL;
  do
  {
    v20 = *(v7 - 64);
    v21 = *(v7 - 48);
    v22 = vpadalq_u16(v16, vabdq_u16(v20, *(2 * *a3 + v5)));
    v23 = vpadalq_u16(v17, vabdq_u16(v20, *(2 * a3[1] + v5)));
    v24 = vpadalq_u16(v18, vabdq_u16(v20, *(2 * a3[2] + v5)));
    v25 = vpadalq_u16(v19, vabdq_u16(v20, *(v6 + v5)));
    v26 = vpadalq_u16(v15, vabdq_u16(v21, *(v10 + v5 - 96)));
    v27 = vpadalq_u16(v14, vabdq_u16(v21, *(v11 + v5 - 96)));
    v28 = vpadalq_u16(v9, vabdq_u16(v21, *(v12 + v5 - 96)));
    v29 = vpadalq_u16(v8, vabdq_u16(v21, *(v6 + v5 + 16)));
    v30 = *(v7 - 32);
    v31 = *(v7 - 16);
    v32 = vpadalq_u16(v26, vabdq_u16(v31, *(v10 + v5 - 64)));
    v33 = vpadalq_u16(v27, vabdq_u16(v31, *(v11 + v5 - 64)));
    v34 = vpadalq_u16(v28, vabdq_u16(v31, *(v12 + v5 - 64)));
    v35 = vpadalq_u16(v29, vabdq_u16(v31, *(v6 + v5 + 48)));
    v36 = *(v7 + 16);
    v37 = vpadalq_u16(vpadalq_u16(v22, vabdq_u16(v30, *(v10 + v5 - 80))), vabdq_u16(*v7, *(v10 + v5 - 48)));
    v38 = vpadalq_u16(vpadalq_u16(v23, vabdq_u16(v30, *(v11 + v5 - 80))), vabdq_u16(*v7, *(v11 + v5 - 48)));
    v39 = vpadalq_u16(vpadalq_u16(v24, vabdq_u16(v30, *(v12 + v5 - 80))), vabdq_u16(*v7, *(v12 + v5 - 48)));
    v40 = vpadalq_u16(vpadalq_u16(v25, vabdq_u16(v30, *(v6 + v5 + 32))), vabdq_u16(*v7, *(v6 + v5 + 64)));
    v41 = vpadalq_u16(v32, vabdq_u16(v36, *(v10 + v5 - 32)));
    v42 = vpadalq_u16(v33, vabdq_u16(v36, *(v11 + v5 - 32)));
    v43 = vpadalq_u16(v34, vabdq_u16(v36, *(v12 + v5 - 32)));
    v44 = vpadalq_u16(v35, vabdq_u16(v36, *(v6 + v5 + 80)));
    v45 = *(v7 + 32);
    v46 = *(v7 + 48);
    v16 = vpadalq_u16(v37, vabdq_u16(v45, *(v10 + v5 - 16)));
    v17 = vpadalq_u16(v38, vabdq_u16(v45, *(v11 + v5 - 16)));
    v18 = vpadalq_u16(v39, vabdq_u16(v45, *(v12 + v5 - 16)));
    v47 = *(v6 + v5 + 112);
    v19 = vpadalq_u16(v40, vabdq_u16(v45, *(v6 + v5 + 96)));
    v15 = vpadalq_u16(v41, vabdq_u16(v46, *(v10 + v5)));
    v14 = vpadalq_u16(v42, vabdq_u16(v46, *(v11 + v5)));
    v9 = vpadalq_u16(v43, vabdq_u16(v46, *(v12 + v5)));
    v7 += 2 * a2;
    v5 += 2 * a4;
    v8 = vpadalq_u16(v44, vabdq_u16(v46, v47));
    --v13;
  }

  while (v13);
  result = vpaddq_s32(vpaddq_s32(vaddq_s32(v15, v16), vaddq_s32(v14, v17)), vpaddq_s32(vaddq_s32(v9, v18), vaddq_s32(v8, v19)));
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_4x8x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = (2 * *a3);
  v6 = (2 * a3[1]);
  v7 = (2 * a3[2]);
  v8 = (2 * a3[3]);
  v9 = a4;
  v10 = *(2 * a1);
  v11 = *(2 * a1 + 4 * a2);
  v12 = vabal_u16(vabdl_u16(v11, *(v5 + 1 * v9)), v10, *v5);
  v13 = vabal_u16(vabdl_u16(v11, *(v6 + 1 * v9)), v10, *v6);
  v14 = vabal_u16(vabdl_u16(v11, *(v7 + 1 * v9)), v10, *v7);
  v15 = vabdl_u16(v11, *(v8 + 1 * v9));
  v16 = *(2 * a1 + 8 * a2);
  v17 = *(2 * a1 + 12 * a2);
  v18 = vpaddq_s32(vpaddq_s32(vabal_u16(vabal_u16(v12, v16, v5[v9]), v17, *(v5 + 3 * v9)), vabal_u16(vabal_u16(v13, v16, v6[v9]), v17, *(v6 + 3 * v9))), vpaddq_s32(vabal_u16(vabal_u16(v14, v16, v7[v9]), v17, *(v7 + 3 * v9)), vabal_u16(vabal_u16(vabal_u16(v15, v10, *v8), v16, v8[v9]), v17, *(v8 + 3 * v9))));
  result = vaddq_s32(v18, v18);
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_8x8x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = (2 * *a3);
  v6 = (2 * a3[1]);
  v7 = (2 * a3[2]);
  v8 = (2 * a3[3]);
  v9 = 2 * a4;
  v10 = *(2 * a1);
  v11 = *(2 * a1 + 4 * a2);
  v12 = vabaq_u16(vabdq_u16(v11, *(v5 + 2 * v9)), v10, *v5);
  v13 = vabaq_u16(vabdq_u16(v11, *(v6 + 2 * v9)), v10, *v6);
  v14 = vabaq_u16(vabdq_u16(v11, *(v7 + 2 * v9)), v10, *v7);
  v15 = vabaq_u16(vabdq_u16(v11, *(v8 + 2 * v9)), v10, *v8);
  v16 = *(2 * a1 + 8 * a2);
  v17 = *(2 * a1 + 12 * a2);
  v18 = vpaddq_s32(vpaddq_s32(vpaddlq_u16(vabaq_u16(vabaq_u16(v12, v16, *(v5 + 4 * v9)), v17, *(v5 + 6 * v9))), vpaddlq_u16(vabaq_u16(vabaq_u16(v13, v16, *(v6 + 4 * v9)), v17, *(v6 + 6 * v9)))), vpaddq_s32(vpaddlq_u16(vabaq_u16(vabaq_u16(v14, v16, *(v7 + 4 * v9)), v17, *(v7 + 6 * v9))), vpaddlq_u16(vabaq_u16(vabaq_u16(v15, v16, *(v8 + 4 * v9)), v17, *(v8 + 6 * v9)))));
  result = vaddq_s32(v18, v18);
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_8x16x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 2 * a2;
  v6 = (2 * *a3);
  v7 = (2 * a3[1]);
  v8 = (2 * a3[2]);
  v9 = (2 * a3[3]);
  v10 = *(2 * a1);
  v11 = *(2 * a1 + 4 * a2);
  v12 = 4 * a4;
  v13 = vabaq_u16(vabdq_u16(v11, *(v6 + v12)), v10, *v6);
  v14 = vabaq_u16(vabdq_u16(v11, *(v7 + v12)), v10, *v7);
  v15 = vabaq_u16(vabdq_u16(v11, *(v8 + v12)), v10, *v8);
  v16 = vabaq_u16(vabdq_u16(v11, *(v9 + v12)), v10, *v9);
  v17 = *(2 * a1 + 8 * a2);
  v18 = 8 * a4;
  v19 = vabaq_u16(v13, v17, *(v6 + v18));
  v20 = vabaq_u16(v14, v17, *(v7 + v18));
  v21 = vabaq_u16(v15, v17, *(v8 + v18));
  v22 = vabaq_u16(v16, v17, *(v9 + v18));
  v23 = *(2 * a1 + 12 * a2);
  v24 = vabaq_u16(v19, v23, *(v6 + 12 * a4));
  v25 = vabaq_u16(v20, v23, *(v7 + 12 * a4));
  v26 = vabaq_u16(v21, v23, *(v8 + 12 * a4));
  v27 = vabaq_u16(v22, v23, *(v9 + 12 * a4));
  v28 = *(2 * a1 + 16 * a2);
  v29 = a4;
  v30 = vabaq_u16(v24, v28, v6[v29]);
  v31 = vabaq_u16(v25, v28, v7[v29]);
  v32 = vabaq_u16(v26, v28, v8[v29]);
  v33 = vabaq_u16(v27, v28, v9[v29]);
  v34 = *(2 * a1 + 10 * v5);
  v35 = 20 * a4;
  v36 = vabaq_u16(v30, v34, *(v6 + v35));
  v37 = vabaq_u16(v31, v34, *(v7 + v35));
  v38 = vabaq_u16(v32, v34, *(v8 + v35));
  v39 = vabaq_u16(v33, v34, *(v9 + v35));
  v40 = *(2 * a1 + 12 * v5);
  v41 = *(2 * a1 + 14 * v5);
  v42 = 28 * a4;
  v43 = vpaddq_s32(vpaddq_s32(vpaddlq_u16(vabaq_u16(vabaq_u16(v36, v40, *(v6 + 24 * a4)), v41, *(v6 + v42))), vpaddlq_u16(vabaq_u16(vabaq_u16(v37, v40, *(v7 + 24 * a4)), v41, *(v7 + v42)))), vpaddq_s32(vpaddlq_u16(vabaq_u16(vabaq_u16(v38, v40, *(v8 + 24 * a4)), v41, *(v8 + v42))), vpaddlq_u16(vabaq_u16(vabaq_u16(v39, v40, *(v9 + 24 * a4)), v41, *(v9 + v42)))));
  result = vaddq_s32(v43, v43);
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_16x8x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = (2 * *a3);
  v6 = (2 * a3[1]);
  v7 = (2 * a3[2]);
  v8 = (2 * a3[3]);
  v9 = 2 * a4;
  v10 = *(2 * a1);
  v11 = *(2 * a1 + 0x10);
  v12 = vabdq_u16(v10, *v5);
  v13 = vabdq_u16(v10, *v6);
  v14 = vabdq_u16(v10, *v7);
  v15 = 2 * a1 + 4 * a2;
  v16 = *(v15 + 0x10);
  v17 = vabdq_u16(*v15, *(v5 + 2 * v9));
  v18 = vabdq_u16(*v15, *(v6 + 2 * v9));
  v19 = vabdq_u16(*v15, *(v7 + 2 * v9));
  v20 = vabdq_u16(*v15, *(v8 + 2 * v9));
  v21 = 2 * a1 + 8 * a2;
  v22 = *(v21 + 0x10);
  v23 = vabdq_u16(*v21, *(v5 + 4 * v9));
  v24 = vabdq_u16(*v21, *(v6 + 4 * v9));
  v25 = vabdq_u16(*v21, *(v7 + 4 * v9));
  v26 = (v8 + 4 * v9);
  v27 = vabdq_u16(v22, *(v5 + 4 * v9 + 16));
  v28 = vabdq_u16(v22, *(v6 + 4 * v9 + 16));
  v29 = 2 * a1 + 12 * a2;
  v30 = (v5 + 6 * v9);
  v31 = *(v29 + 0x10);
  v32 = (v6 + 6 * v9);
  v33 = (v7 + 6 * v9);
  v34 = vpaddq_s32(vpaddq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(v11, *(2 * *a3 + 0x10))), v12), v17), vabdq_u16(v16, *(2 * *a3 + 2 * v9 + 0x10))), v23), v27), vabdq_u16(*v29, *v30)), vabdq_u16(v31, v30[1])), vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(v11, *(2 * a3[1] + 0x10))), v13), v18), vabdq_u16(v16, *(2 * a3[1] + 2 * v9 + 0x10))), v24), v28), vabdq_u16(*v29, *v32)), vabdq_u16(v31, v32[1]))), vpaddq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(v11, *(2 * a3[2] + 0x10))), v14), v19), vabdq_u16(v16, *(2 * a3[2] + 2 * v9 + 0x10))), v25), vabdq_u16(v22, *(2 * a3[2] + 4 * v9 + 0x10))), vabdq_u16(*v29, *v33)), vabdq_u16(v31, v33[1])), vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(v11, v8[1])), vabdq_u16(v10, *v8)), v20), vabdq_u16(v16, *(v8 + 2 * v9 + 16))), vabdq_u16(*v21, *v26)), vabdq_u16(v22, v26[1])), vabdq_u16(*v29, *(v8 + 6 * v9))), vabdq_u16(v31, *(v8 + 6 * v9 + 16)))));
  result = vaddq_s32(v34, v34);
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_16x16x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 2 * a2;
  v6 = 2 * a4;
  v7 = 2 * a1;
  v8 = (2 * *a3);
  v9 = (2 * a3[1]);
  v10 = (2 * a3[2]);
  v11 = (2 * a3[3]);
  v12 = *(2 * a1);
  v13 = *(2 * a1 + 0x10);
  v80 = vabdq_u16(v12, *v8);
  v86 = vabdq_u16(v12, *v9);
  v92 = vabdq_u16(v12, *v11);
  v78 = vabdq_u16(v13, v8[1]);
  v84 = vabdq_u16(v13, v9[1]);
  v89 = vabdq_u16(v13, v10[1]);
  v90 = vabdq_u16(v12, *v10);
  v91 = vabdq_u16(v13, v11[1]);
  v14 = 2 * a1 + 4 * a2;
  v15 = 4 * a4;
  v16 = *(v14 + 0x10);
  v72 = vabdq_u16(*v14, *(v8 + v15));
  v79 = vabdq_u16(*v14, *(v9 + v15));
  v85 = vabdq_u16(*v14, *(v10 + v15));
  v70 = vabdq_u16(v16, *(&v8[1] + v15));
  v76 = vabdq_u16(v16, *(&v9[1] + v15));
  v83 = vabdq_u16(v16, *(&v10[1] + v15));
  v87 = vabdq_u16(v16, *(&v11[1] + v15));
  v88 = vabdq_u16(*v14, *(v11 + v15));
  v17 = 2 * a1 + 8 * a2;
  v18 = 8 * a4;
  v19 = *(v17 + 16);
  v65 = vabdq_u16(*v17, *(v8 + v18));
  v71 = vabdq_u16(*v17, *(v9 + v18));
  v77 = vabdq_u16(*v17, *(v10 + v18));
  v63 = vabdq_u16(v19, *(&v8[1] + v18));
  v69 = vabdq_u16(v19, *(&v9[1] + v18));
  v75 = vabdq_u16(v19, *(&v10[1] + v18));
  v81 = vabdq_u16(v19, *(&v11[1] + v18));
  v82 = vabdq_u16(*v17, *(v11 + v18));
  v20 = (v7 + 12 * a2);
  v21 = 6 * a4;
  v22 = (v8 + 12 * a4);
  v23 = *v20;
  v24 = v20[1];
  v25 = vabdq_u16(*v20, *v22);
  v26 = (v9 + 2 * v21);
  v27 = v26[1];
  v64 = vabdq_u16(v23, *v26);
  v28 = (v10 + 2 * v21);
  v29 = v28[1];
  v30 = vabdq_u16(v23, *v28);
  v31 = (v11 + 2 * v21);
  v74 = vabdq_u16(v23, *v31);
  v32 = vabdq_u16(v24, v22[1]);
  v61 = vabdq_u16(v24, v27);
  v68 = vabdq_u16(v24, v29);
  v73 = vabdq_u16(v24, v31[1]);
  v33 = (v7 + 8 * v5);
  v34 = 8 * v6;
  v35 = v33[1];
  v36 = vabdq_u16(*v33, *(v8 + v34));
  v37 = vabdq_u16(*v33, *(v9 + v34));
  v62 = vabdq_u16(*v33, *(v10 + v34));
  v38 = vabdq_u16(v35, *(&v8[1] + v34));
  v39 = vabdq_u16(v35, *(&v9[1] + v34));
  v60 = vabdq_u16(v35, *(&v10[1] + v34));
  v66 = vabdq_u16(v35, *(&v11[1] + v34));
  v67 = vabdq_u16(*v33, *(v11 + v34));
  v40 = (v7 + 10 * v5);
  v41 = 10 * v6;
  v42 = v40[1];
  v43 = *(&v8[1] + v41);
  v44 = vabdq_u16(*v40, *(v8 + v41));
  v45 = *(&v9[1] + v41);
  v46 = vabdq_u16(*v40, *(v9 + v41));
  v47 = *(&v10[1] + v41);
  v48 = vabdq_u16(*v40, *(v10 + v41));
  v49 = (v11 + v41);
  v50 = vabdq_u16(*v40, *v49);
  v51 = vabdq_u16(v42, v49[1]);
  v52 = (v7 + 12 * v5);
  v21 *= 4;
  v53 = v52[1];
  v54 = (v7 + 14 * v5);
  v55 = 14 * v6;
  v56 = v54[1];
  v57 = vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(v84), v86), v79), v76), v71), v69), v64), v61), v37), v39), v46), vabdq_u16(v42, v45)), vabdq_u16(*v52, *(v9 + v21))), vabdq_u16(v53, *(&v9[1] + v21))), vabdq_u16(*v54, *(v9 + v55))), vabdq_u16(v56, *(&v9[1] + v55)));
  v58 = vpaddq_s32(vpaddq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(v78), v80), v72), v70), v65), v63), v25), v32), v36), v38), v44), vabdq_u16(v42, v43)), vabdq_u16(*v52, *(v8 + v21))), vabdq_u16(v53, *(&v8[1] + v21))), vabdq_u16(*v54, *(v8 + v55))), vabdq_u16(v56, *(&v8[1] + v55))), v57), vpaddq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(v89), v90), v85), v83), v77), v75), v30), v68), v62), v60), v48), vabdq_u16(v42, v47)), vabdq_u16(*v52, *(v10 + v21))), vabdq_u16(v53, *(&v10[1] + v21))), vabdq_u16(*v54, *(v10 + v55))), vabdq_u16(v56, *(&v10[1] + v55))), vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(v91), v92), v88), v87), v82), v81), v74), v73), v67), v66), v50), v51), vabdq_u16(*v52, *(v11 + v21))), vabdq_u16(v53, *(&v11[1] + v21))), vabdq_u16(*v54, *(v11 + v55))), vabdq_u16(v56, *(&v11[1] + v55)))));
  result = vaddq_s32(v58, v58);
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_16x32x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[1];
  v7 = 2 * a3[2];
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * a3[3];
  v11 = (2 * a1 + 16);
  v12 = 16;
  v13 = 0uLL;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  do
  {
    v19 = v11[-1];
    v15 = vpadalq_u16(v15, vabdq_u16(v19, *(2 * *a3 + v5)));
    v16 = vpadalq_u16(v16, vabdq_u16(v19, *(v6 + v5)));
    v17 = vpadalq_u16(v17, vabdq_u16(v19, *(v7 + v5)));
    v18 = vpadalq_u16(v18, vabdq_u16(v19, *(v10 + v5)));
    v14 = vpadalq_u16(v14, vabdq_u16(*v11, *(2 * *a3 + 16 + v5)));
    v13 = vpadalq_u16(v13, vabdq_u16(*v11, *(v6 + v5 + 16)));
    v9 = vpadalq_u16(v9, vabdq_u16(*v11, *(v7 + v5 + 16)));
    v8 = vpadalq_u16(v8, vabdq_u16(*v11, *(v10 + v5 + 16)));
    v11 = (v11 + 4 * a2);
    v5 += 4 * a4;
    --v12;
  }

  while (v12);
  v20 = vpaddq_s32(vpaddq_s32(vaddq_s32(v14, v15), vaddq_s32(v13, v16)), vpaddq_s32(vaddq_s32(v9, v17), vaddq_s32(v8, v18)));
  result = vaddq_s32(v20, v20);
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_32x16x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[3];
  v7 = 2 * a1 + 32;
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * *a3 + 48;
  v11 = 2 * a3[1] + 48;
  v12 = 2 * a3[2] + 32;
  v13 = 8;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  v19 = 0uLL;
  do
  {
    v20 = *(v7 - 32);
    v21 = *(v7 - 16);
    v22 = vpadalq_u16(v15, vabdq_u16(v21, *(v10 + v5 - 32)));
    v23 = vpadalq_u16(v14, vabdq_u16(v21, *(v11 + v5 - 32)));
    v24 = vpadalq_u16(v9, vabdq_u16(v21, *(v12 + v5 - 16)));
    v25 = vpadalq_u16(v8, vabdq_u16(v21, *(v6 + v5 + 16)));
    v26 = *(v7 + 16);
    v16 = vpadalq_u16(vpadalq_u16(v16, vabdq_u16(v20, *(2 * *a3 + v5))), vabdq_u16(*v7, *(2 * *a3 + 48 + v5 - 16)));
    v17 = vpadalq_u16(vpadalq_u16(v17, vabdq_u16(v20, *(2 * a3[1] + v5))), vabdq_u16(*v7, *(2 * a3[1] + 48 + v5 - 16)));
    v18 = vpadalq_u16(vpadalq_u16(v18, vabdq_u16(v20, *(2 * a3[2] + v5))), vabdq_u16(*v7, *(2 * a3[2] + 32 + v5)));
    v19 = vpadalq_u16(vpadalq_u16(v19, vabdq_u16(v20, *(v6 + v5))), vabdq_u16(*v7, *(v6 + v5 + 32)));
    v15 = vpadalq_u16(v22, vabdq_u16(v26, *(v10 + v5)));
    v14 = vpadalq_u16(v23, vabdq_u16(v26, *(v11 + v5)));
    v9 = vpadalq_u16(v24, vabdq_u16(v26, *(v12 + v5 + 16)));
    v8 = vpadalq_u16(v25, vabdq_u16(v26, *(v6 + v5 + 48)));
    v7 += 4 * a2;
    v5 += 4 * a4;
    --v13;
  }

  while (v13);
  v27 = vpaddq_s32(vpaddq_s32(vaddq_s32(v15, v16), vaddq_s32(v14, v17)), vpaddq_s32(vaddq_s32(v9, v18), vaddq_s32(v8, v19)));
  result = vaddq_s32(v27, v27);
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_32x32x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[3];
  v7 = 2 * a1 + 32;
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * *a3 + 48;
  v11 = 2 * a3[1] + 48;
  v12 = 2 * a3[2] + 32;
  v13 = 16;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  v19 = 0uLL;
  do
  {
    v20 = *(v7 - 32);
    v21 = *(v7 - 16);
    v22 = vpadalq_u16(v15, vabdq_u16(v21, *(v10 + v5 - 32)));
    v23 = vpadalq_u16(v14, vabdq_u16(v21, *(v11 + v5 - 32)));
    v24 = vpadalq_u16(v9, vabdq_u16(v21, *(v12 + v5 - 16)));
    v25 = vpadalq_u16(v8, vabdq_u16(v21, *(v6 + v5 + 16)));
    v26 = *(v7 + 16);
    v16 = vpadalq_u16(vpadalq_u16(v16, vabdq_u16(v20, *(2 * *a3 + v5))), vabdq_u16(*v7, *(2 * *a3 + 48 + v5 - 16)));
    v17 = vpadalq_u16(vpadalq_u16(v17, vabdq_u16(v20, *(2 * a3[1] + v5))), vabdq_u16(*v7, *(2 * a3[1] + 48 + v5 - 16)));
    v18 = vpadalq_u16(vpadalq_u16(v18, vabdq_u16(v20, *(2 * a3[2] + v5))), vabdq_u16(*v7, *(2 * a3[2] + 32 + v5)));
    v19 = vpadalq_u16(vpadalq_u16(v19, vabdq_u16(v20, *(v6 + v5))), vabdq_u16(*v7, *(v6 + v5 + 32)));
    v15 = vpadalq_u16(v22, vabdq_u16(v26, *(v10 + v5)));
    v14 = vpadalq_u16(v23, vabdq_u16(v26, *(v11 + v5)));
    v9 = vpadalq_u16(v24, vabdq_u16(v26, *(v12 + v5 + 16)));
    v8 = vpadalq_u16(v25, vabdq_u16(v26, *(v6 + v5 + 48)));
    v7 += 4 * a2;
    v5 += 4 * a4;
    --v13;
  }

  while (v13);
  v27 = vpaddq_s32(vpaddq_s32(vaddq_s32(v15, v16), vaddq_s32(v14, v17)), vpaddq_s32(vaddq_s32(v9, v18), vaddq_s32(v8, v19)));
  result = vaddq_s32(v27, v27);
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_32x64x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[3];
  v7 = 2 * a1 + 32;
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * *a3 + 48;
  v11 = 2 * a3[1] + 48;
  v12 = 2 * a3[2] + 32;
  v13 = 32;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  v19 = 0uLL;
  do
  {
    v20 = *(v7 - 32);
    v21 = *(v7 - 16);
    v22 = vpadalq_u16(v15, vabdq_u16(v21, *(v10 + v5 - 32)));
    v23 = vpadalq_u16(v14, vabdq_u16(v21, *(v11 + v5 - 32)));
    v24 = vpadalq_u16(v9, vabdq_u16(v21, *(v12 + v5 - 16)));
    v25 = vpadalq_u16(v8, vabdq_u16(v21, *(v6 + v5 + 16)));
    v26 = *(v7 + 16);
    v16 = vpadalq_u16(vpadalq_u16(v16, vabdq_u16(v20, *(2 * *a3 + v5))), vabdq_u16(*v7, *(2 * *a3 + 48 + v5 - 16)));
    v17 = vpadalq_u16(vpadalq_u16(v17, vabdq_u16(v20, *(2 * a3[1] + v5))), vabdq_u16(*v7, *(2 * a3[1] + 48 + v5 - 16)));
    v18 = vpadalq_u16(vpadalq_u16(v18, vabdq_u16(v20, *(2 * a3[2] + v5))), vabdq_u16(*v7, *(2 * a3[2] + 32 + v5)));
    v19 = vpadalq_u16(vpadalq_u16(v19, vabdq_u16(v20, *(v6 + v5))), vabdq_u16(*v7, *(v6 + v5 + 32)));
    v15 = vpadalq_u16(v22, vabdq_u16(v26, *(v10 + v5)));
    v14 = vpadalq_u16(v23, vabdq_u16(v26, *(v11 + v5)));
    v9 = vpadalq_u16(v24, vabdq_u16(v26, *(v12 + v5 + 16)));
    v8 = vpadalq_u16(v25, vabdq_u16(v26, *(v6 + v5 + 48)));
    v7 += 4 * a2;
    v5 += 4 * a4;
    --v13;
  }

  while (v13);
  v27 = vpaddq_s32(vpaddq_s32(vaddq_s32(v15, v16), vaddq_s32(v14, v17)), vpaddq_s32(vaddq_s32(v9, v18), vaddq_s32(v8, v19)));
  result = vaddq_s32(v27, v27);
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_64x32x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[3];
  v7 = 2 * a1 + 64;
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * *a3 + 112;
  v11 = 2 * a3[1] + 112;
  v12 = 2 * a3[2] + 112;
  v13 = 16;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  v19 = 0uLL;
  do
  {
    v20 = *(v7 - 64);
    v21 = *(v7 - 48);
    v22 = vpadalq_u16(v16, vabdq_u16(v20, *(2 * *a3 + v5)));
    v23 = vpadalq_u16(v17, vabdq_u16(v20, *(2 * a3[1] + v5)));
    v24 = vpadalq_u16(v18, vabdq_u16(v20, *(2 * a3[2] + v5)));
    v25 = vpadalq_u16(v19, vabdq_u16(v20, *(v6 + v5)));
    v26 = vpadalq_u16(v15, vabdq_u16(v21, *(v10 + v5 - 96)));
    v27 = vpadalq_u16(v14, vabdq_u16(v21, *(v11 + v5 - 96)));
    v28 = vpadalq_u16(v9, vabdq_u16(v21, *(v12 + v5 - 96)));
    v29 = vpadalq_u16(v8, vabdq_u16(v21, *(v6 + v5 + 16)));
    v30 = *(v7 - 32);
    v31 = *(v7 - 16);
    v32 = vpadalq_u16(v26, vabdq_u16(v31, *(v10 + v5 - 64)));
    v33 = vpadalq_u16(v27, vabdq_u16(v31, *(v11 + v5 - 64)));
    v34 = vpadalq_u16(v28, vabdq_u16(v31, *(v12 + v5 - 64)));
    v35 = vpadalq_u16(v29, vabdq_u16(v31, *(v6 + v5 + 48)));
    v36 = *(v7 + 16);
    v37 = vpadalq_u16(vpadalq_u16(v22, vabdq_u16(v30, *(v10 + v5 - 80))), vabdq_u16(*v7, *(v10 + v5 - 48)));
    v38 = vpadalq_u16(vpadalq_u16(v23, vabdq_u16(v30, *(v11 + v5 - 80))), vabdq_u16(*v7, *(v11 + v5 - 48)));
    v39 = vpadalq_u16(vpadalq_u16(v24, vabdq_u16(v30, *(v12 + v5 - 80))), vabdq_u16(*v7, *(v12 + v5 - 48)));
    v40 = vpadalq_u16(vpadalq_u16(v25, vabdq_u16(v30, *(v6 + v5 + 32))), vabdq_u16(*v7, *(v6 + v5 + 64)));
    v41 = vpadalq_u16(v32, vabdq_u16(v36, *(v10 + v5 - 32)));
    v42 = vpadalq_u16(v33, vabdq_u16(v36, *(v11 + v5 - 32)));
    v43 = vpadalq_u16(v34, vabdq_u16(v36, *(v12 + v5 - 32)));
    v44 = vpadalq_u16(v35, vabdq_u16(v36, *(v6 + v5 + 80)));
    v45 = *(v7 + 32);
    v46 = *(v7 + 48);
    v16 = vpadalq_u16(v37, vabdq_u16(v45, *(v10 + v5 - 16)));
    v17 = vpadalq_u16(v38, vabdq_u16(v45, *(v11 + v5 - 16)));
    v18 = vpadalq_u16(v39, vabdq_u16(v45, *(v12 + v5 - 16)));
    v47 = *(v6 + v5 + 112);
    v19 = vpadalq_u16(v40, vabdq_u16(v45, *(v6 + v5 + 96)));
    v15 = vpadalq_u16(v41, vabdq_u16(v46, *(v10 + v5)));
    v14 = vpadalq_u16(v42, vabdq_u16(v46, *(v11 + v5)));
    v9 = vpadalq_u16(v43, vabdq_u16(v46, *(v12 + v5)));
    v7 += 4 * a2;
    v5 += 4 * a4;
    v8 = vpadalq_u16(v44, vabdq_u16(v46, v47));
    --v13;
  }

  while (v13);
  v48 = vpaddq_s32(vpaddq_s32(vaddq_s32(v15, v16), vaddq_s32(v14, v17)), vpaddq_s32(vaddq_s32(v9, v18), vaddq_s32(v8, v19)));
  result = vaddq_s32(v48, v48);
  *a5 = result;
  return result;
}

int32x4_t vpx_highbd_sad_skip_64x64x4d_neon(uint64_t a1, int a2, void *a3, int a4, int32x4_t *a5)
{
  v5 = 0;
  v6 = 2 * a3[3];
  v7 = 2 * a1 + 64;
  v8 = 0uLL;
  v9 = 0uLL;
  v10 = 2 * *a3 + 112;
  v11 = 2 * a3[1] + 112;
  v12 = 2 * a3[2] + 112;
  v13 = 32;
  v14 = 0uLL;
  v15 = 0uLL;
  v16 = 0uLL;
  v17 = 0uLL;
  v18 = 0uLL;
  v19 = 0uLL;
  do
  {
    v20 = *(v7 - 64);
    v21 = *(v7 - 48);
    v22 = vpadalq_u16(v16, vabdq_u16(v20, *(2 * *a3 + v5)));
    v23 = vpadalq_u16(v17, vabdq_u16(v20, *(2 * a3[1] + v5)));
    v24 = vpadalq_u16(v18, vabdq_u16(v20, *(2 * a3[2] + v5)));
    v25 = vpadalq_u16(v19, vabdq_u16(v20, *(v6 + v5)));
    v26 = vpadalq_u16(v15, vabdq_u16(v21, *(v10 + v5 - 96)));
    v27 = vpadalq_u16(v14, vabdq_u16(v21, *(v11 + v5 - 96)));
    v28 = vpadalq_u16(v9, vabdq_u16(v21, *(v12 + v5 - 96)));
    v29 = vpadalq_u16(v8, vabdq_u16(v21, *(v6 + v5 + 16)));
    v30 = *(v7 - 32);
    v31 = *(v7 - 16);
    v32 = vpadalq_u16(v26, vabdq_u16(v31, *(v10 + v5 - 64)));
    v33 = vpadalq_u16(v27, vabdq_u16(v31, *(v11 + v5 - 64)));
    v34 = vpadalq_u16(v28, vabdq_u16(v31, *(v12 + v5 - 64)));
    v35 = vpadalq_u16(v29, vabdq_u16(v31, *(v6 + v5 + 48)));
    v36 = *(v7 + 16);
    v37 = vpadalq_u16(vpadalq_u16(v22, vabdq_u16(v30, *(v10 + v5 - 80))), vabdq_u16(*v7, *(v10 + v5 - 48)));
    v38 = vpadalq_u16(vpadalq_u16(v23, vabdq_u16(v30, *(v11 + v5 - 80))), vabdq_u16(*v7, *(v11 + v5 - 48)));
    v39 = vpadalq_u16(vpadalq_u16(v24, vabdq_u16(v30, *(v12 + v5 - 80))), vabdq_u16(*v7, *(v12 + v5 - 48)));
    v40 = vpadalq_u16(vpadalq_u16(v25, vabdq_u16(v30, *(v6 + v5 + 32))), vabdq_u16(*v7, *(v6 + v5 + 64)));
    v41 = vpadalq_u16(v32, vabdq_u16(v36, *(v10 + v5 - 32)));
    v42 = vpadalq_u16(v33, vabdq_u16(v36, *(v11 + v5 - 32)));
    v43 = vpadalq_u16(v34, vabdq_u16(v36, *(v12 + v5 - 32)));
    v44 = vpadalq_u16(v35, vabdq_u16(v36, *(v6 + v5 + 80)));
    v45 = *(v7 + 32);
    v46 = *(v7 + 48);
    v16 = vpadalq_u16(v37, vabdq_u16(v45, *(v10 + v5 - 16)));
    v17 = vpadalq_u16(v38, vabdq_u16(v45, *(v11 + v5 - 16)));
    v18 = vpadalq_u16(v39, vabdq_u16(v45, *(v12 + v5 - 16)));
    v47 = *(v6 + v5 + 112);
    v19 = vpadalq_u16(v40, vabdq_u16(v45, *(v6 + v5 + 96)));
    v15 = vpadalq_u16(v41, vabdq_u16(v46, *(v10 + v5)));
    v14 = vpadalq_u16(v42, vabdq_u16(v46, *(v11 + v5)));
    v9 = vpadalq_u16(v43, vabdq_u16(v46, *(v12 + v5)));
    v7 += 4 * a2;
    v5 += 4 * a4;
    v8 = vpadalq_u16(v44, vabdq_u16(v46, v47));
    --v13;
  }

  while (v13);
  v48 = vpaddq_s32(vpaddq_s32(vaddq_s32(v15, v16), vaddq_s32(v14, v17)), vpaddq_s32(vaddq_s32(v9, v18), vaddq_s32(v8, v19)));
  result = vaddq_s32(v48, v48);
  *a5 = result;
  return result;
}

uint64_t vpx_highbd_sad8x16_neon(uint64_t a1, int a2, uint64_t a3, int a4)
{
  v4 = 2 * a2;
  v5 = (2 * a1 + v4);
  v6 = 2 * a4;
  v7 = (2 * a3 + v6);
  v8 = vabdq_u16(*v5, *v7);
  v9 = (v5 + v4);
  v10 = (v7 + v6);
  v11 = *v9;
  v12 = *v10;
  v13 = (v9 + v4);
  v14 = (v10 + v6);
  v15 = *v13;
  v16 = *v14;
  v17 = (v13 + v4);
  v18 = (v14 + v6);
  v19 = *v17;
  v20 = *v18;
  v21 = (v17 + v4);
  v22 = (v18 + v6);
  v23 = *v21;
  v24 = *v22;
  v25 = (v21 + v4);
  v26 = (v22 + v6);
  v27 = *v25;
  v28 = *v26;
  v29 = (v25 + v4);
  v30 = (v26 + v6);
  v31 = *v29;
  v32 = *v30;
  v33 = (v29 + v4);
  v34 = (v30 + v6);
  v35 = *v33;
  v36 = *v34;
  v37 = (v33 + v4);
  v38 = (v34 + v6);
  v39 = *v37;
  v40 = *v38;
  v41 = (v37 + v4);
  v42 = (v38 + v6);
  v43 = *v41;
  v44 = *v42;
  v45 = (v41 + v4);
  v46 = (v42 + v6);
  v47 = *v45;
  v48 = *v46;
  v49 = (v45 + v4);
  v50 = (v46 + v6);
  return vaddlvq_u16(vaddq_s16(vaddq_s16(vabaq_u16(vabaq_u16(vabaq_u16(vabaq_u16(vabdq_u16(*(2 * a1), *(2 * a3)), v19, v20), v31, v32), v43, v44), *(v49 + v4), *(v50 + v6)), vabaq_u16(vabaq_u16(vabaq_u16(vabaq_u16(vabdq_u16(v11, v12), v23, v24), v35, v36), v47, v48), *(v49 + v4 + v4), *(v50 + v6 + v6))), vabaq_u16(vabaq_u16(vabaq_u16(vabaq_u16(vabaq_u16(v8, v15, v16), v27, v28), v39, v40), *v49, *v50), *(v49 + v4 + v4 + v4), *(v50 + v6 + v6 + v6))));
}

uint64_t vpx_highbd_sad16x8_neon(uint64_t a1, int a2, uint64_t a3, int a4)
{
  v4 = 2 * a2;
  v5 = (2 * a1 + v4);
  v6 = 2 * a4;
  v7 = (2 * a3 + v6);
  v8 = vabdq_u16(*v5, *v7);
  v9 = vabdq_u16(v5[1], v7[1]);
  v10 = (v5 + v4);
  v11 = (v7 + v6);
  v12 = vabdq_u16(*v10, *v11);
  v13 = vabdq_u16(v10[1], v11[1]);
  v14 = (v10 + v4);
  v15 = (v11 + v6);
  v16 = vabdq_u16(*v14, *v15);
  v17 = vabdq_u16(v14[1], v15[1]);
  v18 = (v14 + v4);
  v19 = (v15 + v6);
  v20 = vabdq_u16(*v18, *v19);
  v21 = vabdq_u16(v18[1], v19[1]);
  v22 = (v18 + v4);
  v23 = (v19 + v6);
  v24 = vabdq_u16(*v22, *v23);
  v25 = vabdq_u16(v22[1], v23[1]);
  v26 = (v22 + v4);
  v27 = (v23 + v6);
  return vaddvq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(*(2 * a1 + 0x10), *(2 * a3 + 0x10))), vabdq_u16(*(2 * a1), *(2 * a3))), v8), v9), v12), v13), v16), v17), v20), v21), v24), v25), vabdq_u16(*v26, *v27)), vabdq_u16(v26[1], v27[1])), vabdq_u16(*(v26 + v4), *(v27 + v6))), vabdq_u16(*(&v26[1] + v4), *(&v27[1] + v6))));
}

uint64_t vpx_highbd_sad16x16_neon(uint64_t a1, int a2, uint64_t a3, int a4)
{
  v4 = 2 * a2;
  v5 = (2 * a1 + v4);
  v6 = 2 * a4;
  v7 = (2 * a3 + v6);
  v8 = vabdq_u16(*v5, *v7);
  v9 = vabdq_u16(v5[1], v7[1]);
  v10 = (v5 + v4);
  v11 = (v7 + v6);
  v12 = vabdq_u16(*v10, *v11);
  v13 = vabdq_u16(v10[1], v11[1]);
  v14 = (v10 + v4);
  v15 = (v11 + v6);
  v16 = vabdq_u16(*v14, *v15);
  v17 = vabdq_u16(v14[1], v15[1]);
  v18 = (v14 + v4);
  v19 = (v15 + v6);
  v20 = vabdq_u16(*v18, *v19);
  v21 = vabdq_u16(v18[1], v19[1]);
  v22 = (v18 + v4);
  v23 = (v19 + v6);
  v24 = vabdq_u16(*v22, *v23);
  v25 = vabdq_u16(v22[1], v23[1]);
  v26 = (v22 + v4);
  v27 = (v23 + v6);
  v28 = vabdq_u16(*v26, *v27);
  v29 = vabdq_u16(v26[1], v27[1]);
  v30 = (v26 + v4);
  v31 = (v27 + v6);
  v32 = vabdq_u16(*v30, *v31);
  v33 = vabdq_u16(v30[1], v31[1]);
  v34 = (v30 + v4);
  v35 = (v31 + v6);
  v36 = vabdq_u16(*v34, *v35);
  v37 = vabdq_u16(v34[1], v35[1]);
  v38 = (v34 + v4);
  v39 = (v35 + v6);
  v40 = vabdq_u16(*v38, *v39);
  v41 = vabdq_u16(v38[1], v39[1]);
  v42 = (v38 + v4);
  v43 = (v39 + v6);
  v44 = vabdq_u16(*v42, *v43);
  v45 = vabdq_u16(v42[1], v43[1]);
  v46 = (v42 + v4);
  v47 = (v43 + v6);
  v48 = vabdq_u16(*v46, *v47);
  v49 = vabdq_u16(v46[1], v47[1]);
  v50 = (v46 + v4);
  v51 = (v47 + v6);
  v52 = vabdq_u16(*v50, *v51);
  v53 = vabdq_u16(v50[1], v51[1]);
  v54 = (v50 + v4);
  v55 = (v51 + v6);
  v56 = vabdq_u16(*v54, *v55);
  v57 = vabdq_u16(v54[1], v55[1]);
  v58 = (v54 + v4);
  v59 = (v55 + v6);
  return vaddvq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(*(2 * a1 + 0x10), *(2 * a3 + 0x10))), vabdq_u16(*(2 * a1), *(2 * a3))), v8), v9), v12), v13), v16), v17), v20), v21), v24), v25), v28), v29), v32), v33), v36), v37), v40), v41), v44), v45), v48), v49), v52), v53), v56), v57), vabdq_u16(*v58, *v59)), vabdq_u16(v58[1], v59[1])), vabdq_u16(*(v58 + v4), *(v59 + v6))), vabdq_u16(*(&v58[1] + v4), *(&v59[1] + v6))));
}

uint64_t vpx_highbd_sad_skip_16x16_neon(uint64_t a1, int a2, uint64_t a3, int a4)
{
  v4 = 4 * a2;
  v5 = (2 * a1 + v4);
  v6 = 4 * a4;
  v7 = (2 * a3 + v6);
  v8 = vabdq_u16(*v5, *v7);
  v9 = vabdq_u16(v5[1], v7[1]);
  v10 = (v5 + v4);
  v11 = (v7 + v6);
  v12 = vabdq_u16(*v10, *v11);
  v13 = vabdq_u16(v10[1], v11[1]);
  v14 = (v10 + v4);
  v15 = (v11 + v6);
  v16 = vabdq_u16(*v14, *v15);
  v17 = vabdq_u16(v14[1], v15[1]);
  v18 = (v14 + v4);
  v19 = (v15 + v6);
  v20 = vabdq_u16(*v18, *v19);
  v21 = vabdq_u16(v18[1], v19[1]);
  v22 = (v18 + v4);
  v23 = (v19 + v6);
  v24 = vabdq_u16(*v22, *v23);
  v25 = vabdq_u16(v22[1], v23[1]);
  v26 = (v22 + v4);
  v27 = (v23 + v6);
  return (2 * vaddvq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(*(2 * a1 + 0x10), *(2 * a3 + 0x10))), vabdq_u16(*(2 * a1), *(2 * a3))), v8), v9), v12), v13), v16), v17), v20), v21), v24), v25), vabdq_u16(*v26, *v27)), vabdq_u16(v26[1], v27[1])), vabdq_u16(*(v26 + v4), *(v27 + v6))), vabdq_u16(*(&v26[1] + v4), *(&v27[1] + v6)))));
}

uint64_t vpx_highbd_sad_skip_16x32_neon(uint64_t a1, int a2, uint64_t a3, int a4)
{
  v4 = 4 * a2;
  v5 = (2 * a1 + v4);
  v6 = 4 * a4;
  v7 = (2 * a3 + v6);
  v8 = vabdq_u16(*v5, *v7);
  v9 = vabdq_u16(v5[1], v7[1]);
  v10 = (v5 + v4);
  v11 = (v7 + v6);
  v12 = vabdq_u16(*v10, *v11);
  v13 = vabdq_u16(v10[1], v11[1]);
  v14 = (v10 + v4);
  v15 = (v11 + v6);
  v16 = vabdq_u16(*v14, *v15);
  v17 = vabdq_u16(v14[1], v15[1]);
  v18 = (v14 + v4);
  v19 = (v15 + v6);
  v20 = vabdq_u16(*v18, *v19);
  v21 = vabdq_u16(v18[1], v19[1]);
  v22 = (v18 + v4);
  v23 = (v19 + v6);
  v24 = vabdq_u16(*v22, *v23);
  v25 = vabdq_u16(v22[1], v23[1]);
  v26 = (v22 + v4);
  v27 = (v23 + v6);
  v28 = vabdq_u16(*v26, *v27);
  v29 = vabdq_u16(v26[1], v27[1]);
  v30 = (v26 + v4);
  v31 = (v27 + v6);
  v32 = vabdq_u16(*v30, *v31);
  v33 = vabdq_u16(v30[1], v31[1]);
  v34 = (v30 + v4);
  v35 = (v31 + v6);
  v36 = vabdq_u16(*v34, *v35);
  v37 = vabdq_u16(v34[1], v35[1]);
  v38 = (v34 + v4);
  v39 = (v35 + v6);
  v40 = vabdq_u16(*v38, *v39);
  v41 = vabdq_u16(v38[1], v39[1]);
  v42 = (v38 + v4);
  v43 = (v39 + v6);
  v44 = vabdq_u16(*v42, *v43);
  v45 = vabdq_u16(v42[1], v43[1]);
  v46 = (v42 + v4);
  v47 = (v43 + v6);
  v48 = vabdq_u16(*v46, *v47);
  v49 = vabdq_u16(v46[1], v47[1]);
  v50 = (v46 + v4);
  v51 = (v47 + v6);
  v52 = vabdq_u16(*v50, *v51);
  v53 = vabdq_u16(v50[1], v51[1]);
  v54 = (v50 + v4);
  v55 = (v51 + v6);
  v56 = vabdq_u16(*v54, *v55);
  v57 = vabdq_u16(v54[1], v55[1]);
  v58 = (v54 + v4);
  v59 = (v55 + v6);
  return (2 * vaddvq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(*(2 * a1 + 0x10), *(2 * a3 + 0x10))), vabdq_u16(*(2 * a1), *(2 * a3))), v8), v9), v12), v13), v16), v17), v20), v21), v24), v25), v28), v29), v32), v33), v36), v37), v40), v41), v44), v45), v48), v49), v52), v53), v56), v57), vabdq_u16(*v58, *v59)), vabdq_u16(v58[1], v59[1])), vabdq_u16(*(v58 + v4), *(v59 + v6))), vabdq_u16(*(&v58[1] + v4), *(&v59[1] + v6)))));
}

uint64_t vpx_highbd_sad_skip_32x16_neon(uint64_t a1, int a2, uint64_t a3, int a4)
{
  v4 = 4 * a2;
  v5 = (2 * a1 + v4);
  v6 = 4 * a4;
  v7 = (2 * a3 + v6);
  v8 = vabdq_u16(*v5, *v7);
  v9 = vabdq_u16(v5[1], v7[1]);
  v10 = vabdq_u16(v5[2], v7[2]);
  v11 = vabdq_u16(v5[3], v7[3]);
  v12 = (v5 + v4);
  v13 = (v7 + v6);
  v14 = vabdq_u16(*v12, *v13);
  v15 = vabdq_u16(v12[1], v13[1]);
  v16 = vabdq_u16(v12[2], v13[2]);
  v17 = vabdq_u16(v12[3], v13[3]);
  v18 = (v12 + v4);
  v19 = (v13 + v6);
  v20 = vabdq_u16(*v18, *v19);
  v21 = vabdq_u16(v18[1], v19[1]);
  v22 = vabdq_u16(v18[2], v19[2]);
  v23 = vabdq_u16(v18[3], v19[3]);
  v24 = (v18 + v4);
  v25 = (v19 + v6);
  v26 = vabdq_u16(*v24, *v25);
  v27 = vabdq_u16(v24[1], v25[1]);
  v28 = vabdq_u16(v24[2], v25[2]);
  v29 = vabdq_u16(v24[3], v25[3]);
  v30 = (v24 + v4);
  v31 = (v25 + v6);
  v32 = vabdq_u16(*v30, *v31);
  v33 = vabdq_u16(v30[1], v31[1]);
  v34 = vabdq_u16(v30[2], v31[2]);
  v35 = vabdq_u16(v30[3], v31[3]);
  v36 = (v30 + v4);
  v37 = (v31 + v6);
  v38 = (v36 + v4);
  v39 = (v37 + v6);
  return (2 * vaddvq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(*(2 * a1 + 0x10), *(2 * a3 + 0x10))), vabdq_u16(*(2 * a1), *(2 * a3))), vabdq_u16(*(2 * a1 + 0x20), *(2 * a3 + 0x20))), vabdq_u16(*(2 * a1 + 0x30), *(2 * a3 + 0x30))), v8), v9), v10), v11), v14), v15), v16), v17), v20), v21), v22), v23), v26), v27), v28), v29), v32), v33), v34), v35), vabdq_u16(*v36, *v37)), vabdq_u16(v36[1], v37[1])), vabdq_u16(v36[2], v37[2])), vabdq_u16(v36[3], v37[3])), vabdq_u16(*v38, *v39)), vabdq_u16(v38[1], v39[1])), vabdq_u16(v38[2], v39[2])), vabdq_u16(v38[3], v39[3]))));
}

uint64_t vpx_highbd_sad8x16_avg_neon(uint64_t a1, int a2, uint64_t a3, int a4, uint64_t a5)
{
  v5 = 2 * a2;
  v6 = (2 * a1 + v5);
  v7 = 2 * a4;
  v8 = (2 * a3 + v7);
  v9 = vpadalq_u16(vpaddlq_u16(vabdq_u16(*v6, vrhaddq_u16(*v8, *(2 * a5 + 0x10)))), vabdq_u16(*(2 * a1), vrhaddq_u16(*(2 * a3), *(2 * a5))));
  v10 = (v6 + v5);
  v11 = (v8 + v7);
  v12 = vpadalq_u16(v9, vabdq_u16(*v10, vrhaddq_u16(*v11, *(2 * a5 + 0x20))));
  v13 = (v10 + v5);
  v14 = (v11 + v7);
  v15 = vpadalq_u16(v12, vabdq_u16(*v13, vrhaddq_u16(*v14, *(2 * a5 + 0x30))));
  v16 = (v13 + v5);
  v17 = (v14 + v7);
  v18 = vpadalq_u16(v15, vabdq_u16(*v16, vrhaddq_u16(*v17, *(2 * a5 + 0x40))));
  v19 = (v16 + v5);
  v20 = (v17 + v7);
  v21 = vpadalq_u16(v18, vabdq_u16(*v19, vrhaddq_u16(*v20, *(2 * a5 + 0x50))));
  v22 = (v19 + v5);
  v23 = (v20 + v7);
  v24 = vpadalq_u16(v21, vabdq_u16(*v22, vrhaddq_u16(*v23, *(2 * a5 + 0x60))));
  v25 = (v22 + v5);
  v26 = (v23 + v7);
  v27 = vpadalq_u16(v24, vabdq_u16(*v25, vrhaddq_u16(*v26, *(2 * a5 + 0x70))));
  v28 = (v25 + v5);
  v29 = (v26 + v7);
  v30 = vpadalq_u16(v27, vabdq_u16(*v28, vrhaddq_u16(*v29, *(2 * a5 + 0x80))));
  v31 = (v28 + v5);
  v32 = (v29 + v7);
  v33 = vpadalq_u16(v30, vabdq_u16(*v31, vrhaddq_u16(*v32, *(2 * a5 + 0x90))));
  v34 = (v31 + v5);
  v35 = (v32 + v7);
  v36 = vpadalq_u16(v33, vabdq_u16(*v34, vrhaddq_u16(*v35, *(2 * a5 + 0xA0))));
  v37 = (v34 + v5);
  v38 = (v35 + v7);
  v39 = vpadalq_u16(v36, vabdq_u16(*v37, vrhaddq_u16(*v38, *(2 * a5 + 0xB0))));
  v40 = (v37 + v5);
  v41 = (v38 + v7);
  return vaddvq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(v39, vabdq_u16(*v40, vrhaddq_u16(*v41, *(2 * a5 + 0xC0)))), vabdq_u16(*(v40 + v5), vrhaddq_u16(*(v41 + v7), *(2 * a5 + 0xD0)))), vabdq_u16(*(v40 + v5 + v5), vrhaddq_u16(*(v41 + v7 + v7), *(2 * a5 + 0xE0)))), vabdq_u16(*(v40 + v5 + v5 + v5), vrhaddq_u16(*(v41 + v7 + v7 + v7), *(2 * a5 + 0xF0)))));
}

uint64_t vpx_highbd_sad16x8_avg_neon(uint64_t a1, int a2, uint64_t a3, int a4, uint64_t a5)
{
  v5 = 2 * a2;
  v6 = (2 * a1 + v5);
  v7 = 2 * a4;
  v8 = (2 * a3 + v7);
  v9 = vabdq_u16(*v6, vrhaddq_u16(*v8, *(2 * a5 + 0x20)));
  v10 = vabdq_u16(v6[1], vrhaddq_u16(v8[1], *(2 * a5 + 0x30)));
  v11 = (v6 + v5);
  v12 = (v8 + v7);
  v13 = vabdq_u16(*v11, vrhaddq_u16(*v12, *(2 * a5 + 0x40)));
  v14 = vabdq_u16(v11[1], vrhaddq_u16(v12[1], *(2 * a5 + 0x50)));
  v15 = (v11 + v5);
  v16 = (v12 + v7);
  v17 = vabdq_u16(*v15, vrhaddq_u16(*v16, *(2 * a5 + 0x60)));
  v18 = vabdq_u16(v15[1], vrhaddq_u16(v16[1], *(2 * a5 + 0x70)));
  v19 = (v15 + v5);
  v20 = (v16 + v7);
  v21 = vabdq_u16(*v19, vrhaddq_u16(*v20, *(2 * a5 + 0x80)));
  v22 = vabdq_u16(v19[1], vrhaddq_u16(v20[1], *(2 * a5 + 0x90)));
  v23 = (v19 + v5);
  v24 = (v20 + v7);
  v25 = vabdq_u16(*v23, vrhaddq_u16(*v24, *(2 * a5 + 0xA0)));
  v26 = vabdq_u16(v23[1], vrhaddq_u16(v24[1], *(2 * a5 + 0xB0)));
  v27 = (v23 + v5);
  v28 = (v24 + v7);
  v29 = (v27 + v5);
  return vaddvq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(*(2 * a1 + 0x10), vrhaddq_u16(*(2 * a3 + 0x10), *(2 * a5 + 0x10)))), vabdq_u16(*(2 * a1), vrhaddq_u16(*(2 * a3), *(2 * a5)))), v9), v10), v13), v14), v17), v18), v21), v22), v25), v26), vabdq_u16(*v27, vrhaddq_u16(*v28, *(2 * a5 + 0xC0)))), vabdq_u16(v27[1], vrhaddq_u16(v28[1], *(2 * a5 + 0xD0)))), vabdq_u16(*v29, vrhaddq_u16(*(v28 + v7), *(2 * a5 + 0xE0)))), vabdq_u16(v29[1], vrhaddq_u16(*(&v28[1] + v7), *(2 * a5 + 0xF0)))));
}

uint64_t vpx_highbd_sad16x16_avg_neon(uint64_t a1, int a2, uint64_t a3, int a4, uint64_t a5)
{
  v5 = 2 * a2;
  v6 = (2 * a1 + v5);
  v7 = 2 * a4;
  v8 = (2 * a3 + v7);
  v62 = vabdq_u16(*v6, vrhaddq_u16(*v8, *(2 * a5 + 0x20)));
  v9 = vabdq_u16(v6[1], vrhaddq_u16(v8[1], *(2 * a5 + 0x30)));
  v10 = (v6 + v5);
  v11 = (v8 + v7);
  v12 = vabdq_u16(*v10, vrhaddq_u16(*v11, *(2 * a5 + 0x40)));
  v13 = vabdq_u16(v10[1], vrhaddq_u16(v11[1], *(2 * a5 + 0x50)));
  v14 = (v10 + v5);
  v15 = (v11 + v7);
  v16 = vabdq_u16(*v14, vrhaddq_u16(*v15, *(2 * a5 + 0x60)));
  v17 = vabdq_u16(v14[1], vrhaddq_u16(v15[1], *(2 * a5 + 0x70)));
  v18 = (v14 + v5);
  v19 = (v15 + v7);
  v20 = vabdq_u16(*v18, vrhaddq_u16(*v19, *(2 * a5 + 0x80)));
  v21 = vabdq_u16(v18[1], vrhaddq_u16(v19[1], *(2 * a5 + 0x90)));
  v22 = (v18 + v5);
  v23 = (v19 + v7);
  v24 = vabdq_u16(*v22, vrhaddq_u16(*v23, *(2 * a5 + 0xA0)));
  v25 = vabdq_u16(v22[1], vrhaddq_u16(v23[1], *(2 * a5 + 0xB0)));
  v26 = (v22 + v5);
  v27 = (v23 + v7);
  v28 = vabdq_u16(*v26, vrhaddq_u16(*v27, *(2 * a5 + 0xC0)));
  v29 = vabdq_u16(v26[1], vrhaddq_u16(v27[1], *(2 * a5 + 0xD0)));
  v30 = (v26 + v5);
  v31 = (v27 + v7);
  v32 = vabdq_u16(*v30, vrhaddq_u16(*v31, *(2 * a5 + 0xE0)));
  v33 = vabdq_u16(v30[1], vrhaddq_u16(v31[1], *(2 * a5 + 0xF0)));
  v34 = (v30 + v5);
  v35 = (v31 + v7);
  v36 = vabdq_u16(*v34, vrhaddq_u16(*v35, *(2 * a5 + 0x100)));
  v37 = vabdq_u16(v34[1], vrhaddq_u16(v35[1], *(2 * a5 + 0x110)));
  v38 = (v34 + v5);
  v39 = (v35 + v7);
  v40 = vabdq_u16(*v38, vrhaddq_u16(*v39, *(2 * a5 + 0x120)));
  v41 = vabdq_u16(v38[1], vrhaddq_u16(v39[1], *(2 * a5 + 0x130)));
  v42 = (v38 + v5);
  v43 = (v39 + v7);
  v44 = vabdq_u16(*v42, vrhaddq_u16(*v43, *(2 * a5 + 0x140)));
  v45 = vabdq_u16(v42[1], vrhaddq_u16(v43[1], *(2 * a5 + 0x150)));
  v46 = (v42 + v5);
  v47 = (v43 + v7);
  v48 = vabdq_u16(*v46, vrhaddq_u16(*v47, *(2 * a5 + 0x160)));
  v49 = vabdq_u16(v46[1], vrhaddq_u16(v47[1], *(2 * a5 + 0x170)));
  v50 = (v46 + v5);
  v51 = (v47 + v7);
  v52 = vabdq_u16(*v50, vrhaddq_u16(*v51, *(2 * a5 + 0x180)));
  v53 = vabdq_u16(v50[1], vrhaddq_u16(v51[1], *(2 * a5 + 0x190)));
  v54 = (v50 + v5);
  v55 = (v51 + v7);
  v56 = vabdq_u16(*v54, vrhaddq_u16(*v55, *(2 * a5 + 0x1A0)));
  v57 = vabdq_u16(v54[1], vrhaddq_u16(v55[1], *(2 * a5 + 0x1B0)));
  v58 = (v54 + v5);
  v59 = (v55 + v7);
  v60 = (v59 + v7);
  return vaddvq_s32(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpadalq_u16(vpaddlq_u16(vabdq_u16(*(2 * a1 + 0x10), vrhaddq_u16(*(2 * a3 + 0x10), *(2 * a5 + 0x10)))), vabdq_u16(*(2 * a1), vrhaddq_u16(*(2 * a3), *(2 * a5)))), v62), v9), v12), v13), v16), v17), v20), v21), v24), v25), v28), v29), v32), v33), v36), v37), v40), v41), v44), v45), v48), v49), v52), v53), v56), v57), vabdq_u16(*v58, vrhaddq_u16(*v59, *(2 * a5 + 0x1C0)))), vabdq_u16(v58[1], vrhaddq_u16(v59[1], *(2 * a5 + 0x1D0)))), vabdq_u16(*(v58 + v5), vrhaddq_u16(*v60, *(2 * a5 + 0x1E0)))), vabdq_u16(*(&v58[1] + v5), vrhaddq_u16(v60[1], *(2 * a5 + 0x1F0)))));
}

uint64_t vpx_highbd_sad64x32_avg_neon(uint64_t a1, int a2, uint64_t a3, int a4, uint64_t a5)
{
  v5 = 0;
  v6 = 2 * a5;
  v7 = (2 * a1 + 64);
  v8 = (2 * a3 + 64);
  v9 = 0uLL;
  v10 = 0uLL;
  v11 = 0uLL;
  v12 = 0uLL;
  do
  {
    v9 = vpadalq_u16(vpadalq_u16(v9, vabdq_u16(v7[-4], vrhaddq_u16(v8[-4], *(v6 + v5)))), vabdq_u16(*v7, vrhaddq_u16(*v8, *(v6 + v5 + 64))));
    v10 = vpadalq_u16(vpadalq_u16(v10, vabdq_u16(v7[-3], vrhaddq_u16(v8[-3], *(v6 + v5 + 16)))), vabdq_u16(v7[1], vrhaddq_u16(v8[1], *(v6 + v5 + 80))));
    v11 = vpadalq_u16(vpadalq_u16(v11, vabdq_u16(v7[-2], vrhaddq_u16(v8[-2], *(v6 + v5 + 32)))), vabdq_u16(v7[2], vrhaddq_u16(v8[2], *(v6 + v5 + 96))));
    v12 = vpadalq_u16(vpadalq_u16(v12, vabdq_u16(v7[-1], vrhaddq_u16(v8[-1], *(v6 + v5 + 48)))), vabdq_u16(v7[3], vrhaddq_u16(v8[3], *(v6 + v5 + 112))));
    v5 += 128;
    v7 = (v7 + 2 * a2);
    v8 = (v8 + 2 * a4);
  }

  while (v5 != 4096);
  return vaddvq_s32(vaddq_s32(vaddq_s32(v10, v9), vaddq_s32(v11, v12)));
}

uint64_t vpx_highbd_sad64x64_avg_neon(uint64_t a1, int a2, uint64_t a3, int a4, uint64_t a5)
{
  v5 = 0;
  v6 = 2 * a5;
  v7 = (2 * a1 + 64);
  v8 = (2 * a3 + 64);
  v9 = 0uLL;
  v10 = 0uLL;
  v11 = 0uLL;
  v12 = 0uLL;
  do
  {
    v9 = vpadalq_u16(vpadalq_u16(v9, vabdq_u16(v7[-4], vrhaddq_u16(v8[-4], *(v6 + v5)))), vabdq_u16(*v7, vrhaddq_u16(*v8, *(v6 + v5 + 64))));
    v10 = vpadalq_u16(vpadalq_u16(v10, vabdq_u16(v7[-3], vrhaddq_u16(v8[-3], *(v6 + v5 + 16)))), vabdq_u16(v7[1], vrhaddq_u16(v8[1], *(v6 + v5 + 80))));
    v11 = vpadalq_u16(vpadalq_u16(v11, vabdq_u16(v7[-2], vrhaddq_u16(v8[-2], *(v6 + v5 + 32)))), vabdq_u16(v7[2], vrhaddq_u16(v8[2], *(v6 + v5 + 96))));
    v12 = vpadalq_u16(vpadalq_u16(v12, vabdq_u16(v7[-1], vrhaddq_u16(v8[-1], *(v6 + v5 + 48)))), vabdq_u16(v7[3], vrhaddq_u16(v8[3], *(v6 + v5 + 112))));
    v5 += 128;
    v7 = (v7 + 2 * a2);
    v8 = (v8 + 2 * a4);
  }

  while (v5 != 0x2000);
  return vaddvq_s32(vaddq_s32(vaddq_s32(v10, v9), vaddq_s32(v11, v12)));
}

unint64_t vpx_highbd_sse_neon(uint64_t a1, int a2, uint64_t a3, int a4, int a5, int a6, double a7, double a8, double a9, double a10, int32x4_t a11)
{
  v11 = (2 * a1);
  v12 = (2 * a3);
  if (a5 <= 15)
  {
    if (a5 == 4)
    {
      v94 = vabd_u16(*v11, *v12);
      v95 = vmull_u16(v94, v94);
      v96 = a6 - 1;
      if (a6 != 1)
      {
        v97 = 2 * a2;
        v98 = (v11 + v97);
        v99 = 2 * a4;
        v100 = (v12 + v99);
        do
        {
          v101 = vabd_u16(*v98, *v100);
          v95 = vmlal_u16(v95, v101, v101);
          v98 = (v98 + v97);
          v100 = (v100 + v99);
          --v96;
        }

        while (v96);
      }

      return vaddlvq_u32(v95);
    }

    else
    {
      if (a5 != 8)
      {
        goto LABEL_27;
      }

      v51 = vabdq_u16(*v11->i8, *v12->i8);
      v52 = vmull_u16(*v51.i8, *v51.i8);
      v53 = vmull_high_u16(v51, v51);
      v54 = a6 - 1;
      if (a6 != 1)
      {
        v55 = 2 * a2;
        v56 = (v11 + v55);
        v57 = 2 * a4;
        v58 = (v12 + v57);
        do
        {
          v59 = vabdq_u16(*v56, *v58);
          v52 = vmlal_u16(v52, *v59.i8, *v59.i8);
          v53 = vmlal_high_u16(v53, v59, v59);
          v56 = (v56 + v55);
          v58 = (v58 + v57);
          --v54;
        }

        while (v54);
      }

      return vaddlvq_u32(v53) + vaddlvq_u32(v52);
    }
  }

  else
  {
    switch(a5)
    {
      case 16:
        v60 = vabdq_u16(*v11->i8, *v12->i8);
        v61 = vmull_u16(*v60.i8, *v60.i8);
        v62 = vmull_high_u16(v60, v60);
        v63 = vabdq_u16(*(2 * a1 + 0x10), *(2 * a3 + 0x10));
        v64 = vmull_u16(*v63.i8, *v63.i8);
        v65 = vmull_high_u16(v63, v63);
        v66 = a6 - 1;
        if (a6 != 1)
        {
          v67 = 2 * a2;
          v68 = (&v11[2] + v67);
          v69 = 2 * a4;
          v70 = (&v12[2] + v69);
          do
          {
            v71 = vabdq_u16(v68[-1], v70[-1]);
            v61 = vmlal_u16(v61, *v71.i8, *v71.i8);
            v62 = vmlal_high_u16(v62, v71, v71);
            v72 = vabdq_u16(*v68, *v70);
            v64 = vmlal_u16(v64, *v72.i8, *v72.i8);
            v65 = vmlal_high_u16(v65, v72, v72);
            v68 = (v68 + v67);
            v70 = (v70 + v69);
            --v66;
          }

          while (v66);
        }

        return vaddvq_s64(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpaddlq_u32(v62), v61), v64), v65));
      case 32:
        v73 = vabdq_u16(*v11->i8, *v12->i8);
        v74 = vmull_u16(*v73.i8, *v73.i8);
        v75 = vmull_high_u16(v73, v73);
        v76 = vabdq_u16(*(2 * a1 + 0x10), *(2 * a3 + 0x10));
        v77 = vmull_u16(*v76.i8, *v76.i8);
        v78 = vmull_high_u16(v76, v76);
        v79 = vabdq_u16(*(2 * a1 + 0x20), *(2 * a3 + 0x20));
        v80 = vmull_u16(*v79.i8, *v79.i8);
        v81 = vmull_high_u16(v79, v79);
        v82 = vabdq_u16(*(2 * a1 + 0x30), *(2 * a3 + 0x30));
        v83 = vmull_u16(*v82.i8, *v82.i8);
        v84 = vmull_high_u16(v82, v82);
        v85 = a6 - 1;
        if (a6 != 1)
        {
          v86 = 2 * a4;
          v87 = (&v12[4] + v86);
          v88 = 2 * a2;
          v89 = (&v11[4] + v88);
          do
          {
            v90 = vabdq_u16(v89[-2], v87[-2]);
            v74 = vmlal_u16(v74, *v90.i8, *v90.i8);
            v75 = vmlal_high_u16(v75, v90, v90);
            v91 = vabdq_u16(v89[-1], v87[-1]);
            v77 = vmlal_u16(v77, *v91.i8, *v91.i8);
            v78 = vmlal_high_u16(v78, v91, v91);
            v92 = vabdq_u16(*v89, *v87);
            v80 = vmlal_u16(v80, *v92.i8, *v92.i8);
            v81 = vmlal_high_u16(v81, v92, v92);
            v93 = vabdq_u16(v89[1], v87[1]);
            v83 = vmlal_u16(v83, *v93.i8, *v93.i8);
            v84 = vmlal_high_u16(v84, v93, v93);
            v87 = (v87 + v86);
            v89 = (v89 + v88);
            --v85;
          }

          while (v85);
        }

        return vaddvq_s64(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpaddlq_u32(v75), v74), v77), v78), v80), v81), v83), v84));
      case 64:
        v13 = vabdq_u16(*v11->i8, *v12->i8);
        v14 = vabdq_u16(*(2 * a1 + 0x10), *(2 * a3 + 0x10));
        v15 = vabdq_u16(*(2 * a1 + 0x20), *(2 * a3 + 0x20));
        v16 = vabdq_u16(*(2 * a1 + 0x30), *(2 * a3 + 0x30));
        v17 = vabdq_u16(*(2 * a1 + 0x40), *(2 * a3 + 0x40));
        v18 = vmlal_u16(vmull_u16(*v17.i8, *v17.i8), *v13.i8, *v13.i8);
        v19 = vmlal_high_u16(vmull_high_u16(v17, v17), v13, v13);
        v20 = vabdq_u16(*(2 * a1 + 0x50), *(2 * a3 + 0x50));
        v21 = vmlal_u16(vmull_u16(*v20.i8, *v20.i8), *v14.i8, *v14.i8);
        v22 = vmlal_high_u16(vmull_high_u16(v20, v20), v14, v14);
        v23 = vabdq_u16(*(2 * a1 + 0x60), *(2 * a3 + 0x60));
        v24 = vmlal_u16(vmull_u16(*v23.i8, *v23.i8), *v15.i8, *v15.i8);
        v25 = vmlal_high_u16(vmull_high_u16(v23, v23), v15, v15);
        v26 = vabdq_u16(*(2 * a1 + 0x70), *(2 * a3 + 0x70));
        v27 = vmlal_u16(vmull_u16(*v26.i8, *v26.i8), *v16.i8, *v16.i8);
        v28 = vmlal_high_u16(vmull_high_u16(v26, v26), v16, v16);
        v29 = a6 - 1;
        if (a6 != 1)
        {
          v30 = 2 * a4;
          v31 = (&v12[8] + v30);
          v32 = 2 * a2;
          v33 = (&v11[8] + v32);
          do
          {
            v34 = vabdq_u16(v33[-4], v31[-4]);
            v35 = vmlal_u16(v18, *v34.i8, *v34.i8);
            v36 = vmlal_high_u16(v19, v34, v34);
            v37 = vabdq_u16(v33[-3], v31[-3]);
            v38 = vmlal_u16(v21, *v37.i8, *v37.i8);
            v39 = vmlal_high_u16(v22, v37, v37);
            v40 = vabdq_u16(v33[-2], v31[-2]);
            v41 = vmlal_u16(v24, *v40.i8, *v40.i8);
            v42 = vmlal_high_u16(v25, v40, v40);
            v43 = vabdq_u16(v33[-1], v31[-1]);
            v44 = vmlal_u16(v27, *v43.i8, *v43.i8);
            v45 = vmlal_high_u16(v28, v43, v43);
            v46 = vabdq_u16(*v33, *v31);
            v18 = vmlal_u16(v35, *v46.i8, *v46.i8);
            v19 = vmlal_high_u16(v36, v46, v46);
            v47 = vabdq_u16(v33[1], v31[1]);
            v21 = vmlal_u16(v38, *v47.i8, *v47.i8);
            v22 = vmlal_high_u16(v39, v47, v47);
            v48 = vabdq_u16(v33[2], v31[2]);
            v24 = vmlal_u16(v41, *v48.i8, *v48.i8);
            v25 = vmlal_high_u16(v42, v48, v48);
            v49 = vabdq_u16(v33[3], v31[3]);
            v27 = vmlal_u16(v44, *v49.i8, *v49.i8);
            v28 = vmlal_high_u16(v45, v49, v49);
            v31 = (v31 + v30);
            v33 = (v33 + v32);
            --v29;
          }

          while (v29);
        }

        return vaddvq_s64(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpadalq_u32(vpaddlq_u32(v19), v18), v21), v22), v24), v25), v27), v28));
      default:
LABEL_27:
        result = 0;
        v103 = xmmword_273B92350;
        v102 = vcgtq_u16(vdupq_n_s16(a5 & 7), xmmword_273B92350);
        v103.i32[0] = 8;
        do
        {
          v104 = 0;
          v105 = a5;
          do
          {
            a11.i32[0] = v105;
            a11 = vdupq_lane_s32(*&vcgtq_s32(v103, a11), 0);
            v106 = vabdq_u16(vbslq_s8(a11, vandq_s8(*v11[v104].i8, v102), *v11[v104].i8), vbslq_s8(a11, vandq_s8(*v12[v104].i8, v102), *v12[v104].i8));
            result += vaddlvq_u32(vmlal_high_u16(vmull_u16(*v106.i8, *v106.i8), v106, v106));
            v104 += 2;
            v107 = __OFSUB__(v105, 8);
            v105 -= 8;
          }

          while (!((v105 < 0) ^ v107 | (v105 == 0)));
          v12 = (v12 + 2 * a4);
          v11 = (v11 + 2 * a2);
          --a6;
        }

        while (a6);
        return result;
    }
  }
}

uint64_t vpx_highbd_8_sub_pixel_variance4x4_neon(uint64_t a1, int a2, unsigned int a3, unsigned int a4, uint64_t a5, int a6, _DWORD *a7)
{
  v7 = vdup_n_s16(8 - a3);
  v8 = vdup_n_s16(a3);
  v9 = vrshr_n_u16(vmla_s16(vmul_s16(*(2 * a1), v7), *(2 * a1 + 2), v8), 3uLL);
  v10 = 2 * a2;
  v11 = 2 * a1 + v10;
  v12 = vrshr_n_u16(vmla_s16(vmul_s16(*v11, v7), *(v11 + 2), v8), 3uLL);
  v13 = v11 + v10;
  v14 = vrshr_n_u16(vmla_s16(vmul_s16(*v13, v7), *(v13 + 2), v8), 3uLL);
  v15 = v13 + v10;
  v16 = vrshr_n_u16(vmla_s16(vmul_s16(*v15, v7), *(v15 + 2), v8), 3uLL);
  v17 = vrshr_n_u16(vmla_s16(vmul_s16(*(v15 + v10), v7), *(v15 + v10 + 2), v8), 3uLL);
  v18 = vdup_n_s16(8 - a4);
  v19 = vdup_n_s16(a4);
  *v20.i8 = vrshr_n_u16(vmla_s16(vmul_s16(v9, v18), v12, v19), 3uLL);
  v21 = vrshr_n_u16(vmla_s16(vmul_s16(v12, v18), v14, v19), 3uLL);
  *v22.i8 = vrshr_n_u16(vmla_s16(vmul_s16(v14, v18), v16, v19), 3uLL);
  v23 = vmla_s16(vmul_s16(v16, v18), v17, v19);
  v24 = 2 * a6;
  v20.u64[1] = v21;
  v25.i64[0] = *(2 * a5);
  v25.i64[1] = *(2 * a5 + v24);
  v26 = vsubq_s16(v20, v25);
  v22.u64[1] = vrshr_n_u16(v23, 3uLL);
  v27 = (2 * a5 + 4 * a6);
  v28.i64[0] = *v27;
  v28.i64[1] = *(v27 + v24);
  v29 = vsubq_s16(v22, v28);
  LODWORD(v27) = vaddlvq_s16(vaddq_s16(v29, v26));
  v26.i32[0] = vaddvq_s32(vmlal_high_s16(vmlal_s16(vmlal_high_s16(vmull_s16(*v26.i8, *v26.i8), v26, v26), *v29.i8, *v29.i8), v29, v29));
  *a7 = v26.i32[0];
  return v26.i32[0] - ((v27 * v27) >> 4);
}

uint64_t vpx_highbd_8_sub_pixel_variance4x8_neon(uint64_t a1, int a2, unsigned int a3, unsigned int a4, uint64_t a5, int a6, _DWORD *a7)
{
  v29[8] = *MEMORY[0x277D85DE8];
  v7 = vdup_n_s16(8 - a3);
  v8 = vdup_n_s16(a3);
  v9 = vrshr_n_u16(vmla_s16(vmul_s16(*(2 * a1), v7), *(2 * a1 + 2), v8), 3uLL);
  v10 = 2 * a2;
  v11 = 2 * a1 + v10;
  v12 = vrshr_n_u16(vmla_s16(vmul_s16(*v11, v7), *(v11 + 2), v8), 3uLL);
  v13 = v11 + v10;
  v14 = vrshr_n_u16(vmla_s16(vmul_s16(*v13, v7), *(v13 + 2), v8), 3uLL);
  v15 = v13 + v10;
  v16 = vrshr_n_u16(vmla_s16(vmul_s16(*v15, v7), *(v15 + 2), v8), 3uLL);
  v17 = v15 + v10;
  v18 = vrshr_n_u16(vmla_s16(vmul_s16(*v17, v7), *(v17 + 2), v8), 3uLL);
  v19 = v17 + v10;
  v20 = vrshr_n_u16(vmla_s16(vmul_s16(*v19, v7), *(v19 + 2), v8), 3uLL);
  v21 = v19 + v10;
  v22 = vrshr_n_u16(vmla_s16(vmul_s16(*v21, v7), *(v21 + 2), v8), 3uLL);
  v23 = v21 + v10;
  v24 = vrshr_n_u16(vmla_s16(vmul_s16(*v23, v7), *(v23 + 2), v8), 3uLL);
  v25 = vrshr_n_u16(vmla_s16(vmul_s16(*(v23 + v10), v7), *(v23 + v10 + 2), v8), 3uLL);
  v26 = vdup_n_s16(8 - a4);
  v27 = vdup_n_s16(a4);
  v29[0] = vrshr_n_u16(vmla_s16(vmul_s16(v9, v26), v12, v27), 3uLL);
  v29[1] = vrshr_n_u16(vmla_s16(vmul_s16(v12, v26), v14, v27), 3uLL);
  v29[2] = vrshr_n_u16(vmla_s16(vmul_s16(v14, v26), v16, v27), 3uLL);
  v29[3] = vrshr_n_u16(vmla_s16(vmul_s16(v16, v26), v18, v27), 3uLL);
  v29[4] = vrshr_n_u16(vmla_s16(vmul_s16(v18, v26), v20, v27), 3uLL);
  v29[5] = vrshr_n_u16(vmla_s16(vmul_s16(v20, v26), v22, v27), 3uLL);
  v29[6] = vrshr_n_u16(vmla_s16(vmul_s16(v22, v26), v24, v27), 3uLL);
  v29[7] = vrshr_n_u16(vmla_s16(vmul_s16(v24, v26), v25, v27), 3uLL);
  return vpx_highbd_8_variance4x8_neon(v29 >> 1, 4, a5, a6, a7);
}

uint64_t vpx_highbd_8_sub_pixel_variance8x4_neon(uint64_t a1, int a2, unsigned int a3, unsigned int a4, uint64_t a5, int a6, _DWORD *a7)
{
  v7 = vdupq_n_s16(8 - a3);
  v8 = vdupq_n_s16(a3);
  v9 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(2 * a1), v7), *(2 * a1 + 2), v8), 3uLL);
  v10 = 2 * a2;
  v11 = 2 * a1 + v10;
  v12 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v11, v7), *(v11 + 2), v8), 3uLL);
  v13 = v11 + v10;
  v14 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v13, v7), *(v13 + 2), v8), 3uLL);
  v15 = v13 + v10;
  v16 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v15, v7), *(v15 + 2), v8), 3uLL);
  v17 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v15 + v10), v7), *(v15 + v10 + 2), v8), 3uLL);
  v18 = vdupq_n_s16(8 - a4);
  v19 = vdupq_n_s16(a4);
  v20 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v9, v18), v12, v19), 3uLL);
  v21 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v12, v18), v14, v19), 3uLL);
  v22 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v14, v18), v16, v19), 3uLL);
  v23 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v16, v18), v17, v19), 3uLL);
  v24 = vsubq_s16(v20, *(2 * a5));
  v25 = 2 * a6;
  v26 = (2 * a5 + v25);
  v27 = vsubq_s16(v21, *v26);
  v28 = (v26 + v25);
  v29 = vsubq_s16(v22, *v28);
  v30 = vsubq_s16(v23, *(v28 + v25));
  v16.i32[0] = vaddvq_s32(vpadalq_s16(vpadalq_s16(vpadalq_s16(vpaddlq_s16(v27), v24), v29), v30));
  v30.i32[0] = vaddlvq_u32(vmlal_high_s16(vmlal_s16(vmlal_high_s16(vmlal_s16(vmlal_high_s16(vmlal_s16(vmlal_high_s16(vmull_s16(*v24.i8, *v24.i8), v24, v24), *v27.i8, *v27.i8), v27, v27), *v29.i8, *v29.i8), v29, v29), *v30.i8, *v30.i8), v30, v30));
  *a7 = v30.i32[0];
  return v30.i32[0] - ((v16.i32[0] * v16.i32[0]) >> 5);
}

uint64_t vpx_highbd_8_sub_pixel_variance8x8_neon(uint64_t a1, int a2, unsigned int a3, unsigned int a4, uint64_t a5, int a6, _DWORD *a7)
{
  v30 = *MEMORY[0x277D85DE8];
  v7 = vdupq_n_s16(8 - a3);
  v8 = vdupq_n_s16(a3);
  v9 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(2 * a1), v7), *(2 * a1 + 2), v8), 3uLL);
  v10 = 2 * a2;
  v11 = 2 * a1 + v10;
  v12 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v11, v7), *(v11 + 2), v8), 3uLL);
  v13 = v11 + v10;
  v14 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v13, v7), *(v13 + 2), v8), 3uLL);
  v15 = v13 + v10;
  v16 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v15, v7), *(v15 + 2), v8), 3uLL);
  v17 = v15 + v10;
  v18 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v17, v7), *(v17 + 2), v8), 3uLL);
  v19 = v17 + v10;
  v20 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v19, v7), *(v19 + 2), v8), 3uLL);
  v21 = v19 + v10;
  v22 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v21, v7), *(v21 + 2), v8), 3uLL);
  v23 = v21 + v10;
  v24 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v23, v7), *(v23 + 2), v8), 3uLL);
  v25 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v23 + v10), v7), *(v23 + v10 + 2), v8), 3uLL);
  v26 = vdupq_n_s16(8 - a4);
  v27 = vdupq_n_s16(a4);
  v29[0] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v9, v26), v12, v27), 3uLL);
  v29[1] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v12, v26), v14, v27), 3uLL);
  v29[2] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v14, v26), v16, v27), 3uLL);
  v29[3] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v16, v26), v18, v27), 3uLL);
  v29[4] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v18, v26), v20, v27), 3uLL);
  v29[5] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v20, v26), v22, v27), 3uLL);
  v29[6] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v22, v26), v24, v27), 3uLL);
  v29[7] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v24, v26), v25, v27), 3uLL);
  return vpx_highbd_8_variance8x8_neon(v29 >> 1, 8, a5, a6, a7);
}

uint64_t vpx_highbd_8_sub_pixel_variance8x16_neon(uint64_t a1, int a2, unsigned int a3, unsigned int a4, uint64_t a5, int a6, _DWORD *a7)
{
  v46 = *MEMORY[0x277D85DE8];
  v7 = vdupq_n_s16(8 - a3);
  v8 = vdupq_n_s16(a3);
  v9 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(2 * a1), v7), *(2 * a1 + 2), v8), 3uLL);
  v10 = 2 * a2;
  v11 = 2 * a1 + v10;
  v12 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v11, v7), *(v11 + 2), v8), 3uLL);
  v13 = v11 + v10;
  v14 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v13, v7), *(v13 + 2), v8), 3uLL);
  v15 = v13 + v10;
  v16 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v15, v7), *(v15 + 2), v8), 3uLL);
  v17 = v15 + v10;
  v18 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v17, v7), *(v17 + 2), v8), 3uLL);
  v19 = v17 + v10;
  v20 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v19, v7), *(v19 + 2), v8), 3uLL);
  v21 = v19 + v10;
  v22 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v21, v7), *(v21 + 2), v8), 3uLL);
  v23 = v21 + v10;
  v24 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v23, v7), *(v23 + 2), v8), 3uLL);
  v25 = v23 + v10;
  v26 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v25, v7), *(v25 + 2), v8), 3uLL);
  v27 = v25 + v10;
  v28 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v27, v7), *(v27 + 2), v8), 3uLL);
  v29 = v27 + v10;
  v30 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v29, v7), *(v29 + 2), v8), 3uLL);
  v31 = v29 + v10;
  v32 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v31, v7), *(v31 + 2), v8), 3uLL);
  v33 = v31 + v10;
  v34 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v33, v7), *(v33 + 2), v8), 3uLL);
  v35 = v33 + v10;
  v36 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v35, v7), *(v35 + 2), v8), 3uLL);
  v37 = v35 + v10;
  v38 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v37, v7), *(v37 + 2), v8), 3uLL);
  v39 = v37 + v10;
  v40 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v39, v7), *(v39 + 2), v8), 3uLL);
  v41 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v39 + v10), v7), *(v39 + v10 + 2), v8), 3uLL);
  v42 = vdupq_n_s16(8 - a4);
  v43 = vdupq_n_s16(a4);
  v45[0] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v9, v42), v12, v43), 3uLL);
  v45[1] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v12, v42), v14, v43), 3uLL);
  v45[2] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v14, v42), v16, v43), 3uLL);
  v45[3] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v16, v42), v18, v43), 3uLL);
  v45[4] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v18, v42), v20, v43), 3uLL);
  v45[5] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v20, v42), v22, v43), 3uLL);
  v45[6] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v22, v42), v24, v43), 3uLL);
  v45[7] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v24, v42), v26, v43), 3uLL);
  v45[8] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v26, v42), v28, v43), 3uLL);
  v45[9] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v28, v42), v30, v43), 3uLL);
  v45[10] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v30, v42), v32, v43), 3uLL);
  v45[11] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v32, v42), v34, v43), 3uLL);
  v45[12] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v34, v42), v36, v43), 3uLL);
  v45[13] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v36, v42), v38, v43), 3uLL);
  v45[14] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v38, v42), v40, v43), 3uLL);
  v45[15] = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v40, v42), v41, v43), 3uLL);
  return vpx_highbd_8_variance8x16_neon(v45 >> 1, 8, a5, a6, a7);
}

uint64_t vpx_highbd_8_sub_pixel_variance16x8_neon(uint64_t a1, int a2, unsigned int a3, unsigned int a4, uint64_t a5, int a6, _DWORD *a7)
{
  v164 = *MEMORY[0x277D85DE8];
  v7 = (2 * a1);
  if (a3 == 4)
  {
    if (a4 == 4)
    {
      v54 = vrhaddq_u16(*(2 * a1 + 0x10), *(2 * a1 + 0x12));
      v146 = vrhaddq_u16(*(2 * a1), *(2 * a1 + 2));
      v147 = v54;
      v55 = 2 * a2;
      v56 = &v7->i8[v55];
      v148 = vrhaddq_u16(*v56, *(v56 + 2));
      v149 = vrhaddq_u16(*(v56 + 1), *(v56 + 18));
      v57 = &v56[v55];
      v150 = vrhaddq_u16(*v57, *(v57 + 2));
      v151 = vrhaddq_u16(*(v57 + 1), *(v57 + 18));
      v58 = &v57[v55];
      v152 = vrhaddq_u16(*v58, *(v58 + 2));
      v153 = vrhaddq_u16(*(v58 + 1), *(v58 + 18));
      v59 = &v58[v55];
      v154 = vrhaddq_u16(*v59, *(v59 + 2));
      v155 = vrhaddq_u16(*(v59 + 1), *(v59 + 18));
      v60 = &v59[v55];
      v156 = vrhaddq_u16(*v60, *(v60 + 2));
      v157 = vrhaddq_u16(*(v60 + 1), *(v60 + 18));
      v61 = &v60[v55];
      v158 = vrhaddq_u16(*v61, *(v61 + 2));
      v159 = vrhaddq_u16(*(v61 + 1), *(v61 + 18));
      v62 = &v61[v55];
      v160 = vrhaddq_u16(*v62, *(v62 + 2));
      v161 = vrhaddq_u16(*(v62 + 1), *(v62 + 18));
      v63 = &v62[v55];
      v162 = vrhaddq_u16(*v63, *(v63 + 2));
      v163 = vrhaddq_u16(*(v63 + 1), *(v63 + 18));
      v130 = vrhaddq_u16(v146, v148);
      v131 = vrhaddq_u16(v54, v149);
      v132 = vrhaddq_u16(v148, v150);
      v133 = vrhaddq_u16(v149, v151);
      v134 = vrhaddq_u16(v150, v152);
      v135 = vrhaddq_u16(v151, v153);
      v136 = vrhaddq_u16(v152, v154);
      v137 = vrhaddq_u16(v153, v155);
      v138 = vrhaddq_u16(v154, v156);
      v139 = vrhaddq_u16(v155, v157);
      v140 = vrhaddq_u16(v156, v158);
      v141 = vrhaddq_u16(v157, v159);
      v142 = vrhaddq_u16(v158, v160);
      v143 = vrhaddq_u16(v159, v161);
      v144 = vrhaddq_u16(v160, v162);
      v145 = vrhaddq_u16(v161, v163);
LABEL_23:
      v88 = &v130;
      return vpx_highbd_8_variance16x8_neon(v88 >> 1, 16, a5, a6, a7);
    }

    if (!a4)
    {
      v10 = vrhaddq_u16(*(2 * a1 + 0x10), *(2 * a1 + 0x12));
      v146 = vrhaddq_u16(*(2 * a1), *(2 * a1 + 2));
      v147 = v10;
      v11 = 2 * a2;
      v12 = &v7->i8[v11];
      v148 = vrhaddq_u16(*v12, *(v12 + 2));
      v149 = vrhaddq_u16(*(v12 + 1), *(v12 + 18));
      v13 = &v12[v11];
      v150 = vrhaddq_u16(*v13, *(v13 + 2));
      v151 = vrhaddq_u16(*(v13 + 1), *(v13 + 18));
      v14 = &v13[v11];
      v152 = vrhaddq_u16(*v14, *(v14 + 2));
      v153 = vrhaddq_u16(*(v14 + 1), *(v14 + 18));
      v15 = &v14[v11];
      v154 = vrhaddq_u16(*v15, *(v15 + 2));
      v155 = vrhaddq_u16(*(v15 + 1), *(v15 + 18));
      v16 = &v15[v11];
      v156 = vrhaddq_u16(*v16, *(v16 + 2));
      v157 = vrhaddq_u16(*(v16 + 1), *(v16 + 18));
      v17 = &v16[v11];
      v158 = vrhaddq_u16(*v17, *(v17 + 2));
      v159 = vrhaddq_u16(*(v17 + 1), *(v17 + 18));
      v18 = &v17[v11];
      v160 = vrhaddq_u16(*v18, *(v18 + 2));
      v19 = vrhaddq_u16(*(v18 + 1), *(v18 + 18));
LABEL_18:
      v161 = v19;
      v88 = &v146;
      return vpx_highbd_8_variance16x8_neon(v88 >> 1, 16, a5, a6, a7);
    }

    v89 = vrhaddq_u16(*(2 * a1 + 0x10), *(2 * a1 + 0x12));
    v146 = vrhaddq_u16(*(2 * a1), *(2 * a1 + 2));
    v147 = v89;
    v90 = 2 * a2;
    v91 = &v7->i8[v90];
    v148 = vrhaddq_u16(*v91, *(v91 + 2));
    v149 = vrhaddq_u16(*(v91 + 1), *(v91 + 18));
    v92 = &v91[v90];
    v150 = vrhaddq_u16(*v92, *(v92 + 2));
    v151 = vrhaddq_u16(*(v92 + 1), *(v92 + 18));
    v93 = &v92[v90];
    v152 = vrhaddq_u16(*v93, *(v93 + 2));
    v153 = vrhaddq_u16(*(v93 + 1), *(v93 + 18));
    v94 = &v93[v90];
    v154 = vrhaddq_u16(*v94, *(v94 + 2));
    v155 = vrhaddq_u16(*(v94 + 1), *(v94 + 18));
    v95 = &v94[v90];
    v156 = vrhaddq_u16(*v95, *(v95 + 2));
    v157 = vrhaddq_u16(*(v95 + 1), *(v95 + 18));
    v96 = &v95[v90];
    v158 = vrhaddq_u16(*v96, *(v96 + 2));
    v159 = vrhaddq_u16(*(v96 + 1), *(v96 + 18));
    v97 = &v96[v90];
    v160 = vrhaddq_u16(*v97, *(v97 + 2));
    v161 = vrhaddq_u16(*(v97 + 1), *(v97 + 18));
    v98 = &v97[v90];
    v162 = vrhaddq_u16(*v98, *(v98 + 2));
    v99 = vdupq_n_s16(8 - a4);
    v100 = vdupq_n_s16(a4);
    v163 = vrhaddq_u16(*(v98 + 1), *(v98 + 18));
    v130 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v146, v99), v148, v100), 3uLL);
    v131 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v89, v99), v149, v100), 3uLL);
    v132 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v148, v99), v150, v100), 3uLL);
    v133 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v149, v99), v151, v100), 3uLL);
    v134 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v150, v99), v152, v100), 3uLL);
    v135 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v151, v99), v153, v100), 3uLL);
    v136 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v152, v99), v154, v100), 3uLL);
    v137 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v153, v99), v155, v100), 3uLL);
    v138 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v154, v99), v156, v100), 3uLL);
    v139 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v155, v99), v157, v100), 3uLL);
    v140 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v156, v99), v158, v100), 3uLL);
    v141 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v157, v99), v159, v100), 3uLL);
    v142 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v158, v99), v160, v100), 3uLL);
    v143 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v159, v99), v161, v100), 3uLL);
    v101 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v160, v99), v162, v100), 3uLL);
    v102 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v161, v99), v163, v100), 3uLL);
LABEL_22:
    v144 = v101;
    v145 = v102;
    goto LABEL_23;
  }

  if (a3)
  {
    if (a4 == 4)
    {
      v103 = vdupq_n_s16(8 - a3);
      v104 = vdupq_n_s16(a3);
      v105 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(2 * a1 + 0x10), v103), *(2 * a1 + 0x12), v104), 3uLL);
      v146 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v7, v103), *(2 * a1 + 2), v104), 3uLL);
      v147 = v105;
      v106 = 2 * a2;
      v107 = &v7->i8[v106];
      v148 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v107, v103), *(v107 + 2), v104), 3uLL);
      v149 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v107 + 1), v103), *(v107 + 18), v104), 3uLL);
      v108 = &v107[v106];
      v150 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v108, v103), *(v108 + 2), v104), 3uLL);
      v151 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v108 + 1), v103), *(v108 + 18), v104), 3uLL);
      v109 = &v108[v106];
      v152 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v109, v103), *(v109 + 2), v104), 3uLL);
      v153 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v109 + 1), v103), *(v109 + 18), v104), 3uLL);
      v110 = &v109[v106];
      v154 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v110, v103), *(v110 + 2), v104), 3uLL);
      v155 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v110 + 1), v103), *(v110 + 18), v104), 3uLL);
      v111 = &v110[v106];
      v156 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v111, v103), *(v111 + 2), v104), 3uLL);
      v157 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v111 + 1), v103), *(v111 + 18), v104), 3uLL);
      v112 = &v111[v106];
      v158 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v112, v103), *(v112 + 2), v104), 3uLL);
      v159 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v112 + 1), v103), *(v112 + 18), v104), 3uLL);
      v113 = &v112[v106];
      v160 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v113, v103), *(v113 + 2), v104), 3uLL);
      v161 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v113 + 1), v103), *(v113 + 18), v104), 3uLL);
      v114 = &v113[v106];
      v162 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v114, v103), *(v114 + 2), v104), 3uLL);
      v163 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v114 + 1), v103), *(v114 + 18), v104), 3uLL);
      v130 = vrhaddq_u16(v146, v148);
      v131 = vrhaddq_u16(v105, v149);
      v132 = vrhaddq_u16(v148, v150);
      v133 = vrhaddq_u16(v149, v151);
      v134 = vrhaddq_u16(v150, v152);
      v135 = vrhaddq_u16(v151, v153);
      v136 = vrhaddq_u16(v152, v154);
      v137 = vrhaddq_u16(v153, v155);
      v138 = vrhaddq_u16(v154, v156);
      v139 = vrhaddq_u16(v155, v157);
      v140 = vrhaddq_u16(v156, v158);
      v141 = vrhaddq_u16(v157, v159);
      v142 = vrhaddq_u16(v158, v160);
      v143 = vrhaddq_u16(v159, v161);
      v101 = vrhaddq_u16(v160, v162);
      v102 = vrhaddq_u16(v161, v163);
    }

    else
    {
      if (!a4)
      {
        v20 = vdupq_n_s16(8 - a3);
        v21 = vdupq_n_s16(a3);
        v22 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(2 * a1 + 0x10), v20), *(2 * a1 + 0x12), v21), 3uLL);
        v146 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v7, v20), *(2 * a1 + 2), v21), 3uLL);
        v147 = v22;
        v23 = 2 * a2;
        v24 = &v7->i8[v23];
        v148 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v24, v20), *(v24 + 2), v21), 3uLL);
        v149 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v24 + 1), v20), *(v24 + 18), v21), 3uLL);
        v25 = &v24[v23];
        v150 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v25, v20), *(v25 + 2), v21), 3uLL);
        v151 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v25 + 1), v20), *(v25 + 18), v21), 3uLL);
        v26 = &v25[v23];
        v152 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v26, v20), *(v26 + 2), v21), 3uLL);
        v153 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v26 + 1), v20), *(v26 + 18), v21), 3uLL);
        v27 = &v26[v23];
        v154 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v27, v20), *(v27 + 2), v21), 3uLL);
        v155 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v27 + 1), v20), *(v27 + 18), v21), 3uLL);
        v28 = &v27[v23];
        v156 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v28, v20), *(v28 + 2), v21), 3uLL);
        v157 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v28 + 1), v20), *(v28 + 18), v21), 3uLL);
        v29 = &v28[v23];
        v158 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v29, v20), *(v29 + 2), v21), 3uLL);
        v159 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v29 + 1), v20), *(v29 + 18), v21), 3uLL);
        v30 = &v29[v23];
        v160 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v30, v20), *(v30 + 2), v21), 3uLL);
        v31 = vmlaq_s16(vmulq_s16(*(v30 + 1), v20), *(v30 + 18), v21);
LABEL_17:
        v19 = vrshrq_n_u16(v31, 3uLL);
        goto LABEL_18;
      }

      v115 = vdupq_n_s16(8 - a3);
      v116 = vdupq_n_s16(a3);
      v117 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(2 * a1 + 0x10), v115), *(2 * a1 + 0x12), v116), 3uLL);
      v146 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v7, v115), *(2 * a1 + 2), v116), 3uLL);
      v147 = v117;
      v118 = 2 * a2;
      v119 = &v7->i8[v118];
      v148 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v119, v115), *(v119 + 2), v116), 3uLL);
      v149 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v119 + 1), v115), *(v119 + 18), v116), 3uLL);
      v120 = &v119[v118];
      v150 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v120, v115), *(v120 + 2), v116), 3uLL);
      v151 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v120 + 1), v115), *(v120 + 18), v116), 3uLL);
      v121 = &v120[v118];
      v152 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v121, v115), *(v121 + 2), v116), 3uLL);
      v153 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v121 + 1), v115), *(v121 + 18), v116), 3uLL);
      v122 = &v121[v118];
      v154 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v122, v115), *(v122 + 2), v116), 3uLL);
      v155 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v122 + 1), v115), *(v122 + 18), v116), 3uLL);
      v123 = &v122[v118];
      v156 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v123, v115), *(v123 + 2), v116), 3uLL);
      v157 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v123 + 1), v115), *(v123 + 18), v116), 3uLL);
      v124 = &v123[v118];
      v158 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v124, v115), *(v124 + 2), v116), 3uLL);
      v159 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v124 + 1), v115), *(v124 + 18), v116), 3uLL);
      v125 = &v124[v118];
      v160 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v125, v115), *(v125 + 2), v116), 3uLL);
      v161 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v125 + 1), v115), *(v125 + 18), v116), 3uLL);
      v126 = &v125[v118];
      v162 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v126, v115), *(v126 + 2), v116), 3uLL);
      v127 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v126 + 1), v115), *(v126 + 18), v116), 3uLL);
      v128 = vdupq_n_s16(8 - a4);
      v129 = vdupq_n_s16(a4);
      v163 = v127;
      v130 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v146, v128), v148, v129), 3uLL);
      v131 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v117, v128), v149, v129), 3uLL);
      v132 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v148, v128), v150, v129), 3uLL);
      v133 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v149, v128), v151, v129), 3uLL);
      v134 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v150, v128), v152, v129), 3uLL);
      v135 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v151, v128), v153, v129), 3uLL);
      v136 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v152, v128), v154, v129), 3uLL);
      v137 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v153, v128), v155, v129), 3uLL);
      v138 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v154, v128), v156, v129), 3uLL);
      v139 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v155, v128), v157, v129), 3uLL);
      v140 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v156, v128), v158, v129), 3uLL);
      v141 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v157, v128), v159, v129), 3uLL);
      v142 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v158, v128), v160, v129), 3uLL);
      v143 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v159, v128), v161, v129), 3uLL);
      v101 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v160, v128), v162, v129), 3uLL);
      v102 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v161, v128), v127, v129), 3uLL);
    }

    goto LABEL_22;
  }

  if (a4 == 4)
  {
    v32 = 2 * a2;
    v33 = *(v7 + v32);
    v146 = vrhaddq_u16(*v7, v33);
    v34 = *(&v7[1] + v32);
    v147 = vrhaddq_u16(*(2 * a1 + 0x10), v34);
    v35 = &v7->i8[v32 + v32];
    v36 = *v35;
    v148 = vrhaddq_u16(v33, *v35);
    v37 = v35[1];
    v149 = vrhaddq_u16(v34, v37);
    v38 = (v35 + v32);
    v39 = *v38;
    v150 = vrhaddq_u16(v36, *v38);
    v40 = v38[1];
    v151 = vrhaddq_u16(v37, v40);
    v41 = (v38 + v32);
    v42 = *v41;
    v152 = vrhaddq_u16(v39, *v41);
    v43 = v41[1];
    v153 = vrhaddq_u16(v40, v43);
    v44 = (v41 + v32);
    v45 = *v44;
    v154 = vrhaddq_u16(v42, *v44);
    v46 = v44[1];
    v155 = vrhaddq_u16(v43, v46);
    v47 = (v44 + v32);
    v48 = *v47;
    v156 = vrhaddq_u16(v45, *v47);
    v49 = v47[1];
    v157 = vrhaddq_u16(v46, v49);
    v50 = (v47 + v32);
    v51 = *v50;
    v158 = vrhaddq_u16(v48, *v50);
    v52 = v50[1];
    v159 = vrhaddq_u16(v49, v52);
    v53 = (v50 + v32);
    v160 = vrhaddq_u16(v51, *v53);
    v19 = vrhaddq_u16(v52, v53[1]);
    goto LABEL_18;
  }

  if (a4)
  {
    v64 = vdupq_n_s16(8 - a4);
    v65 = vdupq_n_s16(a4);
    v66 = 2 * a2;
    v67 = *(v7 + v66);
    v146 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v7, v64), v67, v65), 3uLL);
    v68 = *(v7 + v66 + 16);
    v147 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(2 * a1 + 0x10), v64), v68, v65), 3uLL);
    v69 = (v7 + v66 + v66);
    v70 = *v69;
    v148 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v67, v64), *v69, v65), 3uLL);
    v71 = v69[1];
    v149 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v68, v64), v71, v65), 3uLL);
    v72 = (v69 + v66);
    v73 = *v72;
    v150 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v70, v64), *v72, v65), 3uLL);
    v74 = v72[1];
    v151 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v71, v64), v74, v65), 3uLL);
    v75 = (v72 + v66);
    v76 = *v75;
    v152 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v73, v64), *v75, v65), 3uLL);
    v77 = v75[1];
    v153 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v74, v64), v77, v65), 3uLL);
    v78 = (v75 + v66);
    v79 = *v78;
    v154 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v76, v64), *v78, v65), 3uLL);
    v80 = v78[1];
    v155 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v77, v64), v80, v65), 3uLL);
    v81 = (v78 + v66);
    v82 = *v81;
    v156 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v79, v64), *v81, v65), 3uLL);
    v83 = v81[1];
    v157 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v80, v64), v83, v65), 3uLL);
    v84 = (v81 + v66);
    v85 = *v84;
    v158 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v82, v64), *v84, v65), 3uLL);
    v86 = v84[1];
    v159 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v83, v64), v86, v65), 3uLL);
    v87 = (v84 + v66);
    v160 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v85, v64), *v87, v65), 3uLL);
    v31 = vmlaq_s16(vmulq_s16(v86, v64), v87[1], v65);
    goto LABEL_17;
  }

  v8 = a1 & 0x7FFFFFFFFFFFFFFFLL;

  return vpx_highbd_8_variance16x8_neon(v8, a2, a5, a6, a7);
}

uint64_t vpx_highbd_8_sub_pixel_variance16x16_neon(uint64_t a1, int a2, unsigned int a3, unsigned int a4, uint64_t a5, int a6, _DWORD *a7)
{
  v351 = *MEMORY[0x277D85DE8];
  v7 = (2 * a1);
  if (a3 == 4)
  {
    if (a4 == 4)
    {
      v121 = 0;
      v282 = vrhaddq_u16(*(2 * a1 + 0x10), *(2 * a1 + 0x12));
      v317 = vrhaddq_u16(*(2 * a1), *(2 * a1 + 2));
      v318 = v282;
      v122 = 2 * a2;
      v319 = vrhaddq_u16(*(v7 + v122), *(v7 + v122 + 2));
      v320 = vrhaddq_u16(*(&v7[1] + v122), *(&v7[1] + v122 + 2));
      v123 = &v7->i8[v122 + v122];
      v321 = vrhaddq_u16(*v123, *(v123 + 2));
      v322 = vrhaddq_u16(*(v123 + 1), *(v123 + 18));
      v124 = &v123[v122];
      v323 = vrhaddq_u16(*v124, *(v124 + 2));
      v324 = vrhaddq_u16(*(v124 + 1), *(v124 + 18));
      v125 = &v124[v122];
      v325 = vrhaddq_u16(*v125, *(v125 + 2));
      v326 = vrhaddq_u16(*(v125 + 1), *(v125 + 18));
      v126 = &v125[v122];
      v327 = vrhaddq_u16(*v126, *(v126 + 2));
      v328 = vrhaddq_u16(*(v126 + 1), *(v126 + 18));
      v127 = &v126[v122];
      v329 = vrhaddq_u16(*v127, *(v127 + 2));
      v330 = vrhaddq_u16(*(v127 + 1), *(v127 + 18));
      v128 = &v127[v122];
      v331 = vrhaddq_u16(*v128, *(v128 + 2));
      v332 = vrhaddq_u16(*(v128 + 1), *(v128 + 18));
      v129 = &v128[v122];
      v333 = vrhaddq_u16(*v129, *(v129 + 2));
      v334 = vrhaddq_u16(*(v129 + 1), *(v129 + 18));
      v130 = &v129[v122];
      v335 = vrhaddq_u16(*v130, *(v130 + 2));
      v336 = vrhaddq_u16(*(v130 + 1), *(v130 + 18));
      v131 = &v130[v122];
      v337 = vrhaddq_u16(*v131, *(v131 + 2));
      v338 = vrhaddq_u16(*(v131 + 16), *(v131 + 18));
      v132 = v131 + v122;
      v339 = vrhaddq_u16(*v132, *(v132 + 2));
      v340 = vrhaddq_u16(*(v132 + 16), *(v132 + 18));
      v133 = v132 + v122;
      v341 = vrhaddq_u16(*v133, *(v133 + 2));
      v342 = vrhaddq_u16(*(v133 + 16), *(v133 + 18));
      v134 = v133 + v122;
      v343 = vrhaddq_u16(*v134, *(v134 + 2));
      v344 = vrhaddq_u16(*(v134 + 16), *(v134 + 18));
      v135 = v134 + v122;
      v345 = vrhaddq_u16(*v135, *(v135 + 2));
      v346 = vrhaddq_u16(*(v135 + 16), *(v135 + 18));
      v136 = v135 + v122;
      v347 = vrhaddq_u16(*v136, *(v136 + 2));
      v137 = v136 + v122;
      v348 = vrhaddq_u16(*(v136 + 16), *(v136 + 18));
      v349 = vrhaddq_u16(*v137, *(v137 + 2));
      v138 = vrhaddq_u16(*(v137 + 16), *(v137 + 18));
      v285 = vrhaddq_u16(v317, v319);
      v286 = vrhaddq_u16(v282, v320);
      v287 = vrhaddq_u16(v319, v321);
      v288 = vrhaddq_u16(v320, v322);
      v289 = vrhaddq_u16(v321, v323);
      v290 = vrhaddq_u16(v322, v324);
      v291 = vrhaddq_u16(v323, v325);
      v292 = vrhaddq_u16(v324, v326);
      v293 = vrhaddq_u16(v325, v327);
      v294 = vrhaddq_u16(v326, v328);
      v295 = vrhaddq_u16(v327, v329);
      v296 = vrhaddq_u16(v328, v330);
      v297 = vrhaddq_u16(v329, v331);
      v298 = vrhaddq_u16(v330, v332);
      v299 = vrhaddq_u16(v331, v333);
      v300 = vrhaddq_u16(v332, v334);
      v301 = vrhaddq_u16(v333, v335);
      v302 = vrhaddq_u16(v334, v336);
      v303 = vrhaddq_u16(v335, v337);
      v304 = vrhaddq_u16(v336, v338);
      v305 = vrhaddq_u16(v337, v339);
      v306 = vrhaddq_u16(v338, v340);
      v307 = vrhaddq_u16(v339, v341);
      v308 = vrhaddq_u16(v340, v342);
      v309 = vrhaddq_u16(v341, v343);
      v310 = vrhaddq_u16(v342, v344);
      v311 = vrhaddq_u16(v343, v345);
      v312 = vrhaddq_u16(v344, v346);
      v313 = vrhaddq_u16(v345, v347);
      v314 = vrhaddq_u16(v346, v348);
      v350 = v138;
      v315 = vrhaddq_u16(v347, v349);
      v316 = vrhaddq_u16(v348, v138);
      v139 = (2 * a5 + 16);
      v10 = 0uLL;
      v12 = 0uLL;
      v13 = 0uLL;
      do
      {
        v140 = vsubq_s16(*(&v285 + v121), v139[-1]);
        v141 = vpadalq_s16(v13, v140);
        v142 = vmlal_s16(v10, *v140.i8, *v140.i8);
        v143 = vmlal_high_s16(v12, v140, v140);
        v144 = vsubq_s16(*(&v285 + v121 + 16), *v139);
        v13 = vpadalq_s16(v141, v144);
        v10 = vmlal_s16(v142, *v144.i8, *v144.i8);
        v12 = vmlal_high_s16(v143, v144, v144);
        v121 += 32;
        v139 = (v139 + 2 * a6);
      }

      while (v121 != 512);
    }

    else
    {
      v20 = 0;
      if (a4)
      {
        v283 = vrhaddq_u16(*(2 * a1 + 0x10), *(2 * a1 + 0x12));
        v317 = vrhaddq_u16(*(2 * a1), *(2 * a1 + 2));
        v318 = v283;
        v201 = 2 * a2;
        v319 = vrhaddq_u16(*(v7 + v201), *(v7 + v201 + 2));
        v320 = vrhaddq_u16(*(&v7[1] + v201), *(&v7[1] + v201 + 2));
        v202 = &v7->i8[v201 + v201];
        v321 = vrhaddq_u16(*v202, *(v202 + 2));
        v322 = vrhaddq_u16(*(v202 + 1), *(v202 + 18));
        v203 = &v202[v201];
        v323 = vrhaddq_u16(*v203, *(v203 + 2));
        v324 = vrhaddq_u16(*(v203 + 1), *(v203 + 18));
        v204 = &v203[v201];
        v325 = vrhaddq_u16(*v204, *(v204 + 2));
        v326 = vrhaddq_u16(*(v204 + 1), *(v204 + 18));
        v205 = &v204[v201];
        v327 = vrhaddq_u16(*v205, *(v205 + 2));
        v328 = vrhaddq_u16(*(v205 + 1), *(v205 + 18));
        v206 = &v205[v201];
        v329 = vrhaddq_u16(*v206, *(v206 + 2));
        v330 = vrhaddq_u16(*(v206 + 1), *(v206 + 18));
        v207 = &v206[v201];
        v331 = vrhaddq_u16(*v207, *(v207 + 2));
        v332 = vrhaddq_u16(*(v207 + 1), *(v207 + 18));
        v208 = &v207[v201];
        v333 = vrhaddq_u16(*v208, *(v208 + 2));
        v334 = vrhaddq_u16(*(v208 + 1), *(v208 + 18));
        v209 = &v208[v201];
        v335 = vrhaddq_u16(*v209, *(v209 + 2));
        v336 = vrhaddq_u16(*(v209 + 1), *(v209 + 18));
        v210 = &v209[v201];
        v337 = vrhaddq_u16(*v210, *(v210 + 2));
        v338 = vrhaddq_u16(*(v210 + 16), *(v210 + 18));
        v211 = v210 + v201;
        v339 = vrhaddq_u16(*v211, *(v211 + 2));
        v340 = vrhaddq_u16(*(v211 + 16), *(v211 + 18));
        v212 = v211 + v201;
        v341 = vrhaddq_u16(*v212, *(v212 + 2));
        v342 = vrhaddq_u16(*(v212 + 16), *(v212 + 18));
        v213 = v212 + v201;
        v343 = vrhaddq_u16(*v213, *(v213 + 2));
        v344 = vrhaddq_u16(*(v213 + 16), *(v213 + 18));
        v214 = v213 + v201;
        v345 = vrhaddq_u16(*v214, *(v214 + 2));
        v346 = vrhaddq_u16(*(v214 + 16), *(v214 + 18));
        v215 = v214 + v201;
        v347 = vrhaddq_u16(*v215, *(v215 + 2));
        v216 = v215 + v201;
        v348 = vrhaddq_u16(*(v215 + 16), *(v215 + 18));
        v349 = vrhaddq_u16(*v216, *(v216 + 2));
        v217 = vdupq_n_s16(8 - a4);
        v218 = vdupq_n_s16(a4);
        v350 = vrhaddq_u16(*(v216 + 16), *(v216 + 18));
        v285 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v317, v217), v319, v218), 3uLL);
        v286 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v283, v217), v320, v218), 3uLL);
        v287 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v319, v217), v321, v218), 3uLL);
        v288 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v320, v217), v322, v218), 3uLL);
        v289 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v321, v217), v323, v218), 3uLL);
        v290 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v322, v217), v324, v218), 3uLL);
        v291 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v323, v217), v325, v218), 3uLL);
        v292 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v324, v217), v326, v218), 3uLL);
        v293 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v325, v217), v327, v218), 3uLL);
        v294 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v326, v217), v328, v218), 3uLL);
        v295 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v327, v217), v329, v218), 3uLL);
        v296 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v328, v217), v330, v218), 3uLL);
        v297 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v329, v217), v331, v218), 3uLL);
        v298 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v330, v217), v332, v218), 3uLL);
        v299 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v331, v217), v333, v218), 3uLL);
        v300 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v332, v217), v334, v218), 3uLL);
        v301 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v333, v217), v335, v218), 3uLL);
        v302 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v334, v217), v336, v218), 3uLL);
        v303 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v335, v217), v337, v218), 3uLL);
        v304 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v336, v217), v338, v218), 3uLL);
        v305 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v337, v217), v339, v218), 3uLL);
        v306 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v338, v217), v340, v218), 3uLL);
        v307 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v339, v217), v341, v218), 3uLL);
        v308 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v340, v217), v342, v218), 3uLL);
        v309 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v341, v217), v343, v218), 3uLL);
        v310 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v342, v217), v344, v218), 3uLL);
        v311 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v343, v217), v345, v218), 3uLL);
        v312 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v344, v217), v346, v218), 3uLL);
        v313 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v345, v217), v347, v218), 3uLL);
        v314 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v346, v217), v348, v218), 3uLL);
        v315 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v347, v217), v349, v218), 3uLL);
        v316 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v348, v217), v350, v218), 3uLL);
        v219 = (2 * a5 + 16);
        v10 = 0uLL;
        v12 = 0uLL;
        v13 = 0uLL;
        do
        {
          v220 = vsubq_s16(*(&v285 + v20), v219[-1]);
          v221 = vpadalq_s16(v13, v220);
          v222 = vmlal_s16(v10, *v220.i8, *v220.i8);
          v223 = vmlal_high_s16(v12, v220, v220);
          v224 = vsubq_s16(*(&v285 + v20 + 16), *v219);
          v13 = vpadalq_s16(v221, v224);
          v10 = vmlal_s16(v222, *v224.i8, *v224.i8);
          v12 = vmlal_high_s16(v223, v224, v224);
          v20 += 32;
          v219 = (v219 + 2 * a6);
        }

        while (v20 != 512);
      }

      else
      {
        v21 = vrhaddq_u16(*(2 * a1 + 0x10), *(2 * a1 + 0x12));
        v317 = vrhaddq_u16(*(2 * a1), *(2 * a1 + 2));
        v318 = v21;
        v22 = 2 * a2;
        v319 = vrhaddq_u16(*(v7 + v22), *(v7 + v22 + 2));
        v320 = vrhaddq_u16(*(&v7[1] + v22), *(&v7[1] + v22 + 2));
        v23 = &v7->i8[v22 + v22];
        v321 = vrhaddq_u16(*v23, *(v23 + 2));
        v322 = vrhaddq_u16(*(v23 + 1), *(v23 + 18));
        v24 = &v23[v22];
        v323 = vrhaddq_u16(*v24, *(v24 + 2));
        v324 = vrhaddq_u16(*(v24 + 1), *(v24 + 18));
        v25 = &v24[v22];
        v325 = vrhaddq_u16(*v25, *(v25 + 2));
        v326 = vrhaddq_u16(*(v25 + 1), *(v25 + 18));
        v26 = &v25[v22];
        v327 = vrhaddq_u16(*v26, *(v26 + 2));
        v328 = vrhaddq_u16(*(v26 + 1), *(v26 + 18));
        v27 = &v26[v22];
        v329 = vrhaddq_u16(*v27, *(v27 + 2));
        v330 = vrhaddq_u16(*(v27 + 1), *(v27 + 18));
        v28 = &v27[v22];
        v331 = vrhaddq_u16(*v28, *(v28 + 2));
        v332 = vrhaddq_u16(*(v28 + 1), *(v28 + 18));
        v29 = &v28[v22];
        v333 = vrhaddq_u16(*v29, *(v29 + 2));
        v334 = vrhaddq_u16(*(v29 + 1), *(v29 + 18));
        v30 = &v29[v22];
        v335 = vrhaddq_u16(*v30, *(v30 + 2));
        v336 = vrhaddq_u16(*(v30 + 1), *(v30 + 18));
        v31 = &v30[v22];
        v337 = vrhaddq_u16(*v31, *(v31 + 2));
        v338 = vrhaddq_u16(*(v31 + 16), *(v31 + 18));
        v32 = v31 + v22;
        v339 = vrhaddq_u16(*v32, *(v32 + 2));
        v340 = vrhaddq_u16(*(v32 + 16), *(v32 + 18));
        v33 = v32 + v22;
        v341 = vrhaddq_u16(*v33, *(v33 + 2));
        v342 = vrhaddq_u16(*(v33 + 16), *(v33 + 18));
        v34 = v33 + v22;
        v343 = vrhaddq_u16(*v34, *(v34 + 2));
        v344 = vrhaddq_u16(*(v34 + 16), *(v34 + 18));
        v35 = v34 + v22;
        v345 = vrhaddq_u16(*v35, *(v35 + 2));
        v36 = v35 + v22;
        v346 = vrhaddq_u16(*(v35 + 16), *(v35 + 18));
        v347 = vrhaddq_u16(*v36, *(v36 + 2));
        v348 = vrhaddq_u16(*(v36 + 16), *(v36 + 18));
        v37 = (2 * a5 + 16);
        v10 = 0uLL;
        v12 = 0uLL;
        v13 = 0uLL;
        do
        {
          v38 = vsubq_s16(*(&v317 + v20), v37[-1]);
          v39 = vpadalq_s16(v13, v38);
          v40 = vmlal_s16(v10, *v38.i8, *v38.i8);
          v41 = vmlal_high_s16(v12, v38, v38);
          v42 = vsubq_s16(*(&v317 + v20 + 16), *v37);
          v13 = vpadalq_s16(v39, v42);
          v10 = vmlal_s16(v40, *v42.i8, *v42.i8);
          v12 = vmlal_high_s16(v41, v42, v42);
          v20 += 32;
          v37 = (v37 + 2 * a6);
        }

        while (v20 != 512);
      }
    }
  }

  else if (a3)
  {
    v43 = 0;
    if (a4 == 4)
    {
      v225 = vdupq_n_s16(8 - a3);
      v226 = vdupq_n_s16(a3);
      v284 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(2 * a1 + 0x10), v225), *(2 * a1 + 0x12), v226), 3uLL);
      v317 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v7, v225), *(2 * a1 + 2), v226), 3uLL);
      v318 = v284;
      v227 = 2 * a2;
      v319 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v7 + v227), v225), *(v7 + v227 + 2), v226), 3uLL);
      v320 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(&v7[1] + v227), v225), *(&v7[1] + v227 + 2), v226), 3uLL);
      v228 = &v7->i8[v227 + v227];
      v321 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v228, v225), *(v228 + 2), v226), 3uLL);
      v322 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v228 + 1), v225), *(v228 + 18), v226), 3uLL);
      v229 = &v228[v227];
      v323 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v229, v225), *(v229 + 2), v226), 3uLL);
      v324 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v229 + 1), v225), *(v229 + 18), v226), 3uLL);
      v230 = &v229[v227];
      v325 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v230, v225), *(v230 + 2), v226), 3uLL);
      v326 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v230 + 1), v225), *(v230 + 18), v226), 3uLL);
      v231 = &v230[v227];
      v327 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v231, v225), *(v231 + 2), v226), 3uLL);
      v328 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v231 + 1), v225), *(v231 + 18), v226), 3uLL);
      v232 = &v231[v227];
      v329 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v232, v225), *(v232 + 2), v226), 3uLL);
      v330 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v232 + 1), v225), *(v232 + 18), v226), 3uLL);
      v233 = &v232[v227];
      v331 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v233, v225), *(v233 + 2), v226), 3uLL);
      v332 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v233 + 1), v225), *(v233 + 18), v226), 3uLL);
      v234 = &v233[v227];
      v333 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v234, v225), *(v234 + 2), v226), 3uLL);
      v334 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v234 + 1), v225), *(v234 + 18), v226), 3uLL);
      v235 = &v234[v227];
      v335 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v235, v225), *(v235 + 2), v226), 3uLL);
      v336 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v235 + 1), v225), *(v235 + 18), v226), 3uLL);
      v236 = &v235[v227];
      v337 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v236, v225), *(v236 + 2), v226), 3uLL);
      v338 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v236 + 16), v225), *(v236 + 18), v226), 3uLL);
      v237 = v236 + v227;
      v339 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v237, v225), *(v237 + 2), v226), 3uLL);
      v340 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v237 + 16), v225), *(v237 + 18), v226), 3uLL);
      v238 = v237 + v227;
      v341 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v238, v225), *(v238 + 2), v226), 3uLL);
      v342 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v238 + 16), v225), *(v238 + 18), v226), 3uLL);
      v239 = v238 + v227;
      v343 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v239, v225), *(v239 + 2), v226), 3uLL);
      v344 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v239 + 16), v225), *(v239 + 18), v226), 3uLL);
      v240 = v239 + v227;
      v345 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v240, v225), *(v240 + 2), v226), 3uLL);
      v346 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v240 + 16), v225), *(v240 + 18), v226), 3uLL);
      v241 = v240 + v227;
      v347 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v241, v225), *(v241 + 2), v226), 3uLL);
      v348 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v241 + 16), v225), *(v241 + 18), v226), 3uLL);
      v242 = v241 + v227;
      v349 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v242, v225), *(v242 + 2), v226), 3uLL);
      v243 = vmlaq_s16(vmulq_s16(*(v242 + 16), v225), *(v242 + 18), v226);
      v285 = vrhaddq_u16(v317, v319);
      v286 = vrhaddq_u16(v284, v320);
      v287 = vrhaddq_u16(v319, v321);
      v288 = vrhaddq_u16(v320, v322);
      v289 = vrhaddq_u16(v321, v323);
      v290 = vrhaddq_u16(v322, v324);
      v291 = vrhaddq_u16(v323, v325);
      v292 = vrhaddq_u16(v324, v326);
      v293 = vrhaddq_u16(v325, v327);
      v294 = vrhaddq_u16(v326, v328);
      v295 = vrhaddq_u16(v327, v329);
      v296 = vrhaddq_u16(v328, v330);
      v297 = vrhaddq_u16(v329, v331);
      v298 = vrhaddq_u16(v330, v332);
      v299 = vrhaddq_u16(v331, v333);
      v300 = vrhaddq_u16(v332, v334);
      v301 = vrhaddq_u16(v333, v335);
      v302 = vrhaddq_u16(v334, v336);
      v303 = vrhaddq_u16(v335, v337);
      v304 = vrhaddq_u16(v336, v338);
      v305 = vrhaddq_u16(v337, v339);
      v306 = vrhaddq_u16(v338, v340);
      v307 = vrhaddq_u16(v339, v341);
      v308 = vrhaddq_u16(v340, v342);
      v309 = vrhaddq_u16(v341, v343);
      v310 = vrhaddq_u16(v342, v344);
      v311 = vrhaddq_u16(v343, v345);
      v312 = vrhaddq_u16(v344, v346);
      v313 = vrhaddq_u16(v345, v347);
      v314 = vrhaddq_u16(v346, v348);
      v350 = vrshrq_n_u16(v243, 3uLL);
      v315 = vrhaddq_u16(v347, v349);
      v316 = vrhaddq_u16(v348, v350);
      v244 = (2 * a5 + 16);
      v10 = 0uLL;
      v12 = 0uLL;
      v13 = 0uLL;
      do
      {
        v245 = vsubq_s16(*(&v285 + v43), v244[-1]);
        v246 = vpadalq_s16(v13, v245);
        v247 = vmlal_s16(v10, *v245.i8, *v245.i8);
        v248 = vmlal_high_s16(v12, v245, v245);
        v249 = vsubq_s16(*(&v285 + v43 + 16), *v244);
        v13 = vpadalq_s16(v246, v249);
        v10 = vmlal_s16(v247, *v249.i8, *v249.i8);
        v12 = vmlal_high_s16(v248, v249, v249);
        v43 += 32;
        v244 = (v244 + 2 * a6);
      }

      while (v43 != 512);
    }

    else if (a4)
    {
      v250 = vdupq_n_s16(8 - a3);
      v251 = vdupq_n_s16(a3);
      v252 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(2 * a1 + 0x10), v250), *(2 * a1 + 0x12), v251), 3uLL);
      v317 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v7, v250), *(2 * a1 + 2), v251), 3uLL);
      v318 = v252;
      v253 = 2 * a2;
      v254 = &v7->i8[v253];
      v319 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v254, v250), *(v254 + 2), v251), 3uLL);
      v320 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v254 + 1), v250), *(v254 + 18), v251), 3uLL);
      v255 = &v254[v253];
      v321 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v255, v250), *(v255 + 2), v251), 3uLL);
      v322 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v255 + 1), v250), *(v255 + 18), v251), 3uLL);
      v256 = &v255[v253];
      v323 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v256, v250), *(v256 + 2), v251), 3uLL);
      v324 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v256 + 1), v250), *(v256 + 18), v251), 3uLL);
      v257 = &v256[v253];
      v325 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v257, v250), *(v257 + 2), v251), 3uLL);
      v326 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v257 + 1), v250), *(v257 + 18), v251), 3uLL);
      v258 = &v257[v253];
      v327 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v258, v250), *(v258 + 2), v251), 3uLL);
      v328 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v258 + 1), v250), *(v258 + 18), v251), 3uLL);
      v259 = &v258[v253];
      v329 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v259, v250), *(v259 + 2), v251), 3uLL);
      v330 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v259 + 1), v250), *(v259 + 18), v251), 3uLL);
      v260 = &v259[v253];
      v331 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v260, v250), *(v260 + 2), v251), 3uLL);
      v332 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v260 + 1), v250), *(v260 + 18), v251), 3uLL);
      v261 = &v260[v253];
      v333 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v261, v250), *(v261 + 2), v251), 3uLL);
      v334 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v261 + 1), v250), *(v261 + 18), v251), 3uLL);
      v262 = &v261[v253];
      v335 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v262, v250), *(v262 + 2), v251), 3uLL);
      v336 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v262 + 16), v250), *(v262 + 18), v251), 3uLL);
      v263 = v262 + v253;
      v337 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v263, v250), *(v263 + 2), v251), 3uLL);
      v338 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v263 + 16), v250), *(v263 + 18), v251), 3uLL);
      v264 = v263 + v253;
      v339 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v264, v250), *(v264 + 2), v251), 3uLL);
      v340 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v264 + 16), v250), *(v264 + 18), v251), 3uLL);
      v265 = v264 + v253;
      v341 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v265, v250), *(v265 + 2), v251), 3uLL);
      v342 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v265 + 16), v250), *(v265 + 18), v251), 3uLL);
      v266 = v265 + v253;
      v343 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v266, v250), *(v266 + 2), v251), 3uLL);
      v344 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v266 + 16), v250), *(v266 + 18), v251), 3uLL);
      v267 = v266 + v253;
      v345 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v267, v250), *(v267 + 2), v251), 3uLL);
      v346 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v267 + 16), v250), *(v267 + 18), v251), 3uLL);
      v268 = v267 + v253;
      v347 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v268, v250), *(v268 + 2), v251), 3uLL);
      v348 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v268 + 16), v250), *(v268 + 18), v251), 3uLL);
      v269 = v268 + v253;
      v349 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v269, v250), *(v269 + 2), v251), 3uLL);
      v270 = vmlaq_s16(vmulq_s16(*(v269 + 16), v250), *(v269 + 18), v251);
      v271 = vdupq_n_s16(8 - a4);
      v272 = vdupq_n_s16(a4);
      v350 = vrshrq_n_u16(v270, 3uLL);
      v285 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v317, v271), v319, v272), 3uLL);
      v286 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v252, v271), v320, v272), 3uLL);
      v287 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v319, v271), v321, v272), 3uLL);
      v288 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v320, v271), v322, v272), 3uLL);
      v289 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v321, v271), v323, v272), 3uLL);
      v290 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v322, v271), v324, v272), 3uLL);
      v291 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v323, v271), v325, v272), 3uLL);
      v292 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v324, v271), v326, v272), 3uLL);
      v293 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v325, v271), v327, v272), 3uLL);
      v294 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v326, v271), v328, v272), 3uLL);
      v295 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v327, v271), v329, v272), 3uLL);
      v296 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v328, v271), v330, v272), 3uLL);
      v297 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v329, v271), v331, v272), 3uLL);
      v298 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v330, v271), v332, v272), 3uLL);
      v299 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v331, v271), v333, v272), 3uLL);
      v300 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v332, v271), v334, v272), 3uLL);
      v301 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v333, v271), v335, v272), 3uLL);
      v302 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v334, v271), v336, v272), 3uLL);
      v303 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v335, v271), v337, v272), 3uLL);
      v304 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v336, v271), v338, v272), 3uLL);
      v305 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v337, v271), v339, v272), 3uLL);
      v306 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v338, v271), v340, v272), 3uLL);
      v307 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v339, v271), v341, v272), 3uLL);
      v308 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v340, v271), v342, v272), 3uLL);
      v309 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v341, v271), v343, v272), 3uLL);
      v310 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v342, v271), v344, v272), 3uLL);
      v311 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v343, v271), v345, v272), 3uLL);
      v312 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v344, v271), v346, v272), 3uLL);
      v313 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v345, v271), v347, v272), 3uLL);
      v314 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v346, v271), v348, v272), 3uLL);
      v315 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v347, v271), v349, v272), 3uLL);
      v316 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v348, v271), v350, v272), 3uLL);
      v273 = (2 * a5 + 16);
      v10 = 0uLL;
      v12 = 0uLL;
      v13 = 0uLL;
      do
      {
        v274 = vsubq_s16(*(&v285 + v43), v273[-1]);
        v275 = vpadalq_s16(v13, v274);
        v276 = vmlal_s16(v10, *v274.i8, *v274.i8);
        v277 = vmlal_high_s16(v12, v274, v274);
        v278 = vsubq_s16(*(&v285 + v43 + 16), *v273);
        v13 = vpadalq_s16(v275, v278);
        v10 = vmlal_s16(v276, *v278.i8, *v278.i8);
        v12 = vmlal_high_s16(v277, v278, v278);
        v43 += 32;
        v273 = (v273 + 2 * a6);
      }

      while (v43 != 512);
    }

    else
    {
      v44 = vdupq_n_s16(8 - a3);
      v45 = vdupq_n_s16(a3);
      v46 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(2 * a1 + 0x10), v44), *(2 * a1 + 0x12), v45), 3uLL);
      v317 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v7, v44), *(2 * a1 + 2), v45), 3uLL);
      v318 = v46;
      v47 = 2 * a2;
      v319 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v7 + v47), v44), *(v7 + v47 + 2), v45), 3uLL);
      v320 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(&v7[1] + v47), v44), *(&v7[1] + v47 + 2), v45), 3uLL);
      v48 = &v7->i8[v47 + v47];
      v321 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v48, v44), *(v48 + 2), v45), 3uLL);
      v322 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v48 + 1), v44), *(v48 + 18), v45), 3uLL);
      v49 = &v48[v47];
      v323 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v49, v44), *(v49 + 2), v45), 3uLL);
      v324 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v49 + 1), v44), *(v49 + 18), v45), 3uLL);
      v50 = &v49[v47];
      v325 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v50, v44), *(v50 + 2), v45), 3uLL);
      v326 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v50 + 1), v44), *(v50 + 18), v45), 3uLL);
      v51 = &v50[v47];
      v327 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v51, v44), *(v51 + 2), v45), 3uLL);
      v328 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v51 + 1), v44), *(v51 + 18), v45), 3uLL);
      v52 = &v51[v47];
      v329 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v52, v44), *(v52 + 2), v45), 3uLL);
      v330 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v52 + 1), v44), *(v52 + 18), v45), 3uLL);
      v53 = &v52[v47];
      v331 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v53, v44), *(v53 + 2), v45), 3uLL);
      v332 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v53 + 1), v44), *(v53 + 18), v45), 3uLL);
      v54 = &v53[v47];
      v333 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v54, v44), *(v54 + 2), v45), 3uLL);
      v334 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v54 + 1), v44), *(v54 + 18), v45), 3uLL);
      v55 = &v54[v47];
      v335 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v55, v44), *(v55 + 2), v45), 3uLL);
      v336 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v55 + 1), v44), *(v55 + 18), v45), 3uLL);
      v56 = &v55[v47];
      v337 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v56, v44), *(v56 + 2), v45), 3uLL);
      v338 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v56 + 16), v44), *(v56 + 18), v45), 3uLL);
      v57 = v56 + v47;
      v339 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v57, v44), *(v57 + 2), v45), 3uLL);
      v340 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v57 + 16), v44), *(v57 + 18), v45), 3uLL);
      v58 = v57 + v47;
      v341 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v58, v44), *(v58 + 2), v45), 3uLL);
      v342 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v58 + 16), v44), *(v58 + 18), v45), 3uLL);
      v59 = v58 + v47;
      v343 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v59, v44), *(v59 + 2), v45), 3uLL);
      v344 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v59 + 16), v44), *(v59 + 18), v45), 3uLL);
      v60 = v59 + v47;
      v345 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v60, v44), *(v60 + 2), v45), 3uLL);
      v346 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v60 + 16), v44), *(v60 + 18), v45), 3uLL);
      v61 = v60 + v47;
      v347 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v61, v44), *(v61 + 2), v45), 3uLL);
      v348 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*(v61 + 16), v44), *(v61 + 18), v45), 3uLL);
      v62 = (2 * a5 + 16);
      v10 = 0uLL;
      v12 = 0uLL;
      v13 = 0uLL;
      do
      {
        v63 = vsubq_s16(*(&v317 + v43), v62[-1]);
        v64 = vpadalq_s16(v13, v63);
        v65 = vmlal_s16(v10, *v63.i8, *v63.i8);
        v66 = vmlal_high_s16(v12, v63, v63);
        v67 = vsubq_s16(*(&v317 + v43 + 16), *v62);
        v13 = vpadalq_s16(v64, v67);
        v10 = vmlal_s16(v65, *v67.i8, *v67.i8);
        v12 = vmlal_high_s16(v66, v67, v67);
        v43 += 32;
        v62 = (v62 + 2 * a6);
      }

      while (v43 != 512);
    }
  }

  else if (a4 == 4)
  {
    v68 = 0;
    v69 = 2 * a2;
    v70 = *(v7 + v69);
    v317 = vrhaddq_u16(*v7, v70);
    v71 = *(&v7[1] + v69);
    v318 = vrhaddq_u16(*(2 * a1 + 0x10), v71);
    v72 = &v7->i8[v69 + v69];
    v73 = *v72;
    v319 = vrhaddq_u16(v70, *v72);
    v74 = v72[1];
    v320 = vrhaddq_u16(v71, v74);
    v75 = (v72 + v69);
    v76 = *v75;
    v321 = vrhaddq_u16(v73, *v75);
    v77 = v75[1];
    v322 = vrhaddq_u16(v74, v77);
    v78 = (v75 + v69);
    v79 = *v78;
    v323 = vrhaddq_u16(v76, *v78);
    v80 = v78[1];
    v324 = vrhaddq_u16(v77, v80);
    v81 = (v78 + v69);
    v82 = *v81;
    v325 = vrhaddq_u16(v79, *v81);
    v83 = v81[1];
    v326 = vrhaddq_u16(v80, v83);
    v84 = (v81 + v69);
    v85 = *v84;
    v327 = vrhaddq_u16(v82, *v84);
    v86 = v84[1];
    v328 = vrhaddq_u16(v83, v86);
    v87 = (v84 + v69);
    v88 = *v87;
    v329 = vrhaddq_u16(v85, *v87);
    v89 = v87[1];
    v330 = vrhaddq_u16(v86, v89);
    v90 = (v87 + v69);
    v91 = *v90;
    v331 = vrhaddq_u16(v88, *v90);
    v92 = v90[1];
    v332 = vrhaddq_u16(v89, v92);
    v93 = (v90 + v69);
    v94 = *v93;
    v333 = vrhaddq_u16(v91, *v93);
    v95 = v93[1];
    v334 = vrhaddq_u16(v92, v95);
    v96 = (v93 + v69);
    v97 = *v96;
    v335 = vrhaddq_u16(v94, *v96);
    v98 = v96[1];
    v336 = vrhaddq_u16(v95, v98);
    v99 = (v96 + v69);
    v100 = *v99;
    v337 = vrhaddq_u16(v97, *v99);
    v101 = v99[1];
    v338 = vrhaddq_u16(v98, v101);
    v102 = (v99 + v69);
    v103 = *v102;
    v339 = vrhaddq_u16(v100, *v102);
    v104 = v102[1];
    v340 = vrhaddq_u16(v101, v104);
    v105 = (v102 + v69);
    v106 = *v105;
    v341 = vrhaddq_u16(v103, *v105);
    v107 = v105[1];
    v342 = vrhaddq_u16(v104, v107);
    v108 = (v105 + v69);
    v109 = *v108;
    v343 = vrhaddq_u16(v106, *v108);
    v110 = v108[1];
    v344 = vrhaddq_u16(v107, v110);
    v111 = (v108 + v69);
    v112 = *v111;
    v345 = vrhaddq_u16(v109, *v111);
    v113 = v111[1];
    v346 = vrhaddq_u16(v110, v113);
    v114 = (v111 + v69);
    v347 = vrhaddq_u16(v112, *v114);
    v348 = vrhaddq_u16(v113, v114[1]);
    v115 = (2 * a5 + 16);
    v10 = 0uLL;
    v12 = 0uLL;
    v13 = 0uLL;
    do
    {
      v116 = vsubq_s16(*(&v317 + v68), v115[-1]);
      v117 = vpadalq_s16(v13, v116);
      v118 = vmlal_s16(v10, *v116.i8, *v116.i8);
      v119 = vmlal_high_s16(v12, v116, v116);
      v120 = vsubq_s16(*(&v317 + v68 + 16), *v115);
      v13 = vpadalq_s16(v117, v120);
      v10 = vmlal_s16(v118, *v120.i8, *v120.i8);
      v12 = vmlal_high_s16(v119, v120, v120);
      v68 += 32;
      v115 = (v115 + 2 * a6);
    }

    while (v68 != 512);
  }

  else if (a4)
  {
    v145 = 0;
    v146 = vdupq_n_s16(8 - a4);
    v147 = vdupq_n_s16(a4);
    v148 = 2 * a2;
    v149 = *(v7 + v148);
    v150 = *(2 * a1 + 0x10);
    v317 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(*v7, v146), v149, v147), 3uLL);
    v151 = *(v7 + v148 + 16);
    v318 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v150, v146), v151, v147), 3uLL);
    v152 = (v7 + v148 + v148);
    v153 = *v152;
    v319 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v149, v146), *v152, v147), 3uLL);
    v154 = v152[1];
    v320 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v151, v146), v154, v147), 3uLL);
    v155 = (v152 + v148);
    v156 = *v155;
    v321 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v153, v146), *v155, v147), 3uLL);
    v157 = v155[1];
    v322 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v154, v146), v157, v147), 3uLL);
    v158 = (v155 + v148);
    v159 = *v158;
    v323 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v156, v146), *v158, v147), 3uLL);
    v160 = v158[1];
    v324 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v157, v146), v160, v147), 3uLL);
    v161 = (v158 + v148);
    v162 = *v161;
    v325 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v159, v146), *v161, v147), 3uLL);
    v163 = v161[1];
    v326 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v160, v146), v163, v147), 3uLL);
    v164 = (v161 + v148);
    v165 = *v164;
    v327 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v162, v146), *v164, v147), 3uLL);
    v166 = v164[1];
    v328 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v163, v146), v166, v147), 3uLL);
    v167 = (v164 + v148);
    v168 = *v167;
    v329 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v165, v146), *v167, v147), 3uLL);
    v169 = v167[1];
    v330 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v166, v146), v169, v147), 3uLL);
    v170 = (v167 + v148);
    v171 = *v170;
    v331 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v168, v146), *v170, v147), 3uLL);
    v172 = v170[1];
    v332 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v169, v146), v172, v147), 3uLL);
    v173 = (v170 + v148);
    v174 = *v173;
    v333 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v171, v146), *v173, v147), 3uLL);
    v175 = v173[1];
    v334 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v172, v146), v175, v147), 3uLL);
    v176 = (v173 + v148);
    v177 = *v176;
    v335 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v174, v146), *v176, v147), 3uLL);
    v178 = v176[1];
    v336 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v175, v146), v178, v147), 3uLL);
    v179 = (v176 + v148);
    v180 = *v179;
    v337 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v177, v146), *v179, v147), 3uLL);
    v181 = v179[1];
    v338 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v178, v146), v181, v147), 3uLL);
    v182 = (v179 + v148);
    v183 = *v182;
    v339 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v180, v146), *v182, v147), 3uLL);
    v184 = v182[1];
    v340 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v181, v146), v184, v147), 3uLL);
    v185 = (v182 + v148);
    v186 = *v185;
    v341 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v183, v146), *v185, v147), 3uLL);
    v187 = v185[1];
    v342 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v184, v146), v187, v147), 3uLL);
    v188 = (v185 + v148);
    v189 = *v188;
    v343 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v186, v146), *v188, v147), 3uLL);
    v190 = v188[1];
    v344 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v187, v146), v190, v147), 3uLL);
    v191 = (v188 + v148);
    v192 = *v191;
    v345 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v189, v146), *v191, v147), 3uLL);
    v193 = v191[1];
    v346 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v190, v146), v193, v147), 3uLL);
    v194 = (v191 + v148);
    v347 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v192, v146), *v194, v147), 3uLL);
    v348 = vrshrq_n_u16(vmlaq_s16(vmulq_s16(v193, v146), v194[1], v147), 3uLL);
    v195 = (2 * a5 + 16);
    v10 = 0uLL;
    v12 = 0uLL;
    v13 = 0uLL;
    do
    {
      v196 = vsubq_s16(*(&v317 + v145), v195[-1]);
      v197 = vpadalq_s16(v13, v196);
      v198 = vmlal_s16(v10, *v196.i8, *v196.i8);
      v199 = vmlal_high_s16(v12, v196, v196);
      v200 = vsubq_s16(*(&v317 + v145 + 16), *v195);
      v13 = vpadalq_s16(v197, v200);
      v10 = vmlal_s16(v198, *v200.i8, *v200.i8);
      v12 = vmlal_high_s16(v199, v200, v200);
      v145 += 32;
      v195 = (v195 + 2 * a6);
    }

    while (v145 != 512);
  }

  else
  {
    v8 = (2 * a5 + 16);
    v9 = v7 + 1;
    v10 = 0uLL;
    v11 = -16;
    v12 = 0uLL;
    v13 = 0uLL;
    do
    {
      v14 = vsubq_s16(v9[-1], v8[-1]);
      v15 = vpadalq_s16(v13, v14);
      v16 = vmlal_s16(v10, *v14.i8, *v14.i8);
      v17 = vmlal_high_s16(v12, v14, v14);
      v18 = vsubq_s16(*v9, *v8);
      v13 = vpadalq_s16(v15, v18);
      v10 = vmlal_s16(v16, *v18.i8, *v18.i8);
      v12 = vmlal_high_s16(v17, v18, v18);
      v8 = (v8 + 2 * a6);
      v9 = (v9 + 2 * a2);
    }

    while (!__CFADD__(v11++, 1));
  }

  v279 = vaddvq_s32(v13);
  v280 = vaddlvq_u32(vaddq_s32(v12, v10));
  *a7 = v280;
  return v280 - ((v279 * v279) >> 8);
}